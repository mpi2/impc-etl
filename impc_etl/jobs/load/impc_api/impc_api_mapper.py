import luigi
from luigi.contrib.spark import PySparkTask
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import DoubleType, IntegerType, BooleanType, ArrayType
from pyspark.sql.functions import (
    col,
    first,
    explode,
    zip_with,
    struct,
    when,
    sum,
    collect_set,
    lit,
    concat,
    count,
    max,
    avg,
    regexp_replace,
    split,
)

from impc_etl.jobs.extract import ProductReportExtractor
from impc_etl.jobs.load import ExperimentToObservationMapper
from impc_etl.jobs.load.solr.gene_mapper import GeneLoader
from impc_etl.jobs.load.solr.genotype_phenotype_mapper import GenotypePhenotypeLoader
from impc_etl.jobs.load.solr.impc_images_mapper import ImpcImagesLoader
from impc_etl.jobs.load.solr.stats_results_mapper import StatsResultsMapper
from impc_etl.workflow.config import ImpcConfig

GENE_SUMMARY_MAPPINGS = {
    "mgi_accession_id": "mgiGeneAccessionId",
    "marker_symbol": "geneSymbol",
    "marker_name": "geneName",
    "marker_synonym": "synonyms",
    "significant_top_level_mp_terms": "significantTopLevelPhenotypes",
    "not_significant_top_level_mp_terms": "notSignificantTopLevelPhenotypes",
    "embryo_data_available": "hasEmbryoImagingData",
    "human_gene_symbol": "human_gene_symbols",
    "human_symbol_synonym": "human_symbol_synonyms",
    "production_centre": "production_centres",
    "phenotyping_centre": "phenotyping_centres",
    "allele_name": "allele_names",
    "ensembl_gene_id": "ensembl_gene_ids",
}


class ImpcGeneSummaryMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "Impc_Gene_Summary_Mapper"

    #: Path to the CSV gene disease association report
    gene_disease_association_csv_path = luigi.Parameter()

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            GeneLoader(),
            GenotypePhenotypeLoader(),
            ExperimentToObservationMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_summary_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.input()[1].path,
            self.input()[2].path,
            self.gene_disease_association_csv_path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        gene_parquet_path = args[0]
        genotype_phenotype_parquet_path = args[1]
        observations_parquet_path = args[2]
        gene_disease_association_csv_path = args[3]
        output_path = args[4]

        gene_df = spark.read.parquet(gene_parquet_path)
        genotype_phenotype_df = spark.read.parquet(genotype_phenotype_parquet_path)
        observations_df = spark.read.parquet(observations_parquet_path)
        gene_disease_association_df = spark.read.csv(
            gene_disease_association_csv_path, header=True
        )

        gene_df = gene_df.withColumn("id", col("mgi_accession_id"))
        for col_name in GENE_SUMMARY_MAPPINGS.keys():
            gene_df = gene_df.withColumnRenamed(
                col_name, GENE_SUMMARY_MAPPINGS[col_name]
            )
        gene_df = gene_df.drop("ccds_ids")
        gene_df = gene_df.withColumnRenamed("ccds_id", "ccds_ids")
        genotype_phenotype_df = genotype_phenotype_df.withColumnRenamed(
            "marker_accession_id", "id"
        )
        gp_call_by_gene = genotype_phenotype_df.groupBy("id").count()
        gp_call_by_gene = gp_call_by_gene.withColumnRenamed(
            "count", "significantPhenotypesCount"
        )
        gene_df = gene_df.join(gp_call_by_gene, "id", "left_outer")

        adult_lacz_observations_by_gene = get_lacz_expression_count(
            observations_df, "adult"
        )
        gene_df = gene_df.join(adult_lacz_observations_by_gene, "id", "left_outer")

        embryo_lacz_observations_by_gene = get_lacz_expression_count(
            observations_df, "embryo"
        )
        gene_df = gene_df.join(embryo_lacz_observations_by_gene, "id", "left_outer")

        gene_disease_association_df = gene_disease_association_df.where(
            col("type") == "disease_gene_summary"
        )

        gene_disease_association_df = gene_disease_association_df.groupBy(
            "marker_id"
        ).agg(sum(when(col("disease_id").isNotNull(), 1).otherwise(0)).alias("count"))
        gene_disease_association_df = gene_disease_association_df.withColumnRenamed(
            "marker_id", "id"
        )
        gene_disease_association_df = gene_disease_association_df.withColumnRenamed(
            "count", "associatedDiseasesCount"
        )
        gene_df = gene_df.join(gene_disease_association_df, "id", "left_outer")

        gene_df = gene_df.withColumn(
            "hasLacZData",
            (col("adultExpressionObservationsCount") > 0)
            | (col("embryoExpressionObservationsCount") > 0),
        )

        gene_images_flag = observations_df.where(
            col("observation_type") == "image_record"
        )
        gene_images_flag = gene_images_flag.groupBy("gene_accession_id").agg(
            first("observation_id").alias("obs_id")
        )
        gene_images_flag = gene_images_flag.withColumn(
            "hasImagingData", col("obs_id").isNotNull()
        )
        gene_images_flag = gene_images_flag.withColumnRenamed("gene_accession_id", "id")
        gene_images_flag = gene_images_flag.drop("obs_id")
        gene_df = gene_df.join(gene_images_flag, "id", "left_outer")

        gene_hist_flag = observations_df.where(
            col("procedure_stable_id").contains("HIS")
        )
        gene_hist_flag = gene_hist_flag.groupBy("gene_accession_id").agg(
            first("observation_id").alias("obs_id")
        )
        gene_hist_flag = gene_hist_flag.withColumn(
            "hasHistopathologyData", col("obs_id").isNotNull()
        )
        gene_hist_flag = gene_hist_flag.withColumnRenamed("gene_accession_id", "id")
        gene_hist_flag = gene_hist_flag.drop("obs_id")

        gene_df = gene_df.join(gene_hist_flag, "id", "left_outer")

        gene_via_flag = observations_df.where(
            col("parameter_stable_id").isin(
                [
                    "IMPC_VIA_001_001",
                    "IMPC_VIA_002_001",
                    "IMPC_EVL_001_001",
                    "IMPC_EVM_001_001",
                    "IMPC_EVP_001_001",
                    "IMPC_EVO_001_001",
                    "IMPC_VIA_063_001",
                    "IMPC_VIA_064_001",
                    "IMPC_VIA_065_001",
                    "IMPC_VIA_066_001",
                    "IMPC_VIA_067_001",
                    "IMPC_VIA_056_001",
                ]
            )
        )
        gene_via_flag = gene_via_flag.groupBy("gene_accession_id").agg(
            first("observation_id").alias("obs_id")
        )
        gene_via_flag = gene_via_flag.withColumn(
            "hasViabilityData", col("obs_id").isNotNull()
        )
        gene_via_flag = gene_via_flag.withColumnRenamed("gene_accession_id", "id")
        gene_via_flag = gene_via_flag.drop("obs_id")
        gene_df = gene_df.join(gene_via_flag, "id", "left_outer")

        gene_bw_flag = observations_df.where(
            col("parameter_stable_id") == "IMPC_BWT_008_001"
        )
        gene_bw_flag = gene_bw_flag.groupBy("gene_accession_id").agg(
            first("observation_id").alias("obs_id")
        )
        gene_bw_flag = gene_bw_flag.withColumn(
            "hasBodyWeightData", col("obs_id").isNotNull()
        )
        gene_bw_flag = gene_bw_flag.withColumnRenamed("gene_accession_id", "id")
        gene_bw_flag = gene_bw_flag.drop("obs_id")
        gene_df = gene_df.join(gene_bw_flag, "id", "left_outer")

        gene_df = gene_df.drop("datasets_raw_data")

        for col_name in gene_df.columns:
            gene_df = gene_df.withColumnRenamed(col_name, to_camel_case(col_name))

        gene_avg_df = gene_df.where(col("phenotypingDataAvailable") == True).select(
            avg("significantPhenotypesCount").alias("significantPhenotypesAverage"),
            avg("associatedDiseasesCount").alias("associatedDiseasesAverage"),
            avg("adultExpressionObservationsCount").alias(
                "adultExpressionObservationsAverage"
            ),
            avg("embryoExpressionObservationsCount").alias(
                "embryoExpressionObservationsAverage"
            ),
        )
        gene_avg_df.repartition(1).write.option("ignoreNullFields", "false").json(
            output_path + "_avgs"
        )
        gene_df.repartition(100).write.option("ignoreNullFields", "false").json(
            output_path
        )


def get_lacz_expression_count(observations_df, lacz_lifestage):
    procedure_name = "Adult LacZ" if lacz_lifestage == "adult" else "Embryo LacZ"
    lacz_observations_by_gene = observations_df.where(
        (col("procedure_name") == procedure_name)
        & (col("observation_type") == "categorical")
        & (col("parameter_name") != "LacZ Images Section")
        & (col("parameter_name") != "LacZ Images Wholemount")
    )
    lacz_observations_by_gene = lacz_observations_by_gene.select(
        "gene_accession_id", "zygosity", "parameter_name"
    ).distinct()
    lacz_observations_by_gene = lacz_observations_by_gene.groupBy(
        "gene_accession_id"
    ).agg(sum(when(col("parameter_name").isNotNull(), 1).otherwise(0)).alias("count"))
    lacz_observations_by_gene = lacz_observations_by_gene.withColumnRenamed(
        "count", f"{lacz_lifestage}ExpressionObservationsCount"
    )
    lacz_observations_by_gene = lacz_observations_by_gene.withColumnRenamed(
        "gene_accession_id", "id"
    )

    return lacz_observations_by_gene


def get_lacz_expression_data(observations_df, lacz_lifestage):
    procedure_name = "Adult LacZ" if lacz_lifestage == "adult" else "Embryo LacZ"

    lacz_observations = observations_df.where(
        (col("procedure_name") == procedure_name)
        & (col("observation_type") == "categorical")
        & (col("parameter_name") != "LacZ Images Section")
        & (col("parameter_name") != "LacZ Images Wholemount")
    )
    categories = [
        "expression",
        "tissue not available",
        "no expression",
        "imageOnly",
        "ambiguous",
    ]
    lacz_observations_by_gene = lacz_observations.groupBy(
        "strain_accession_id",
        "gene_accession_id",
        "zygosity",
        "parameter_stable_id",
        "parameter_name",
    ).agg(
        *[
            sum(when(col("category") == category, 1).otherwise(0)).alias(
                to_camel_case(category.replace(" ", "_"))
            )
            for category in categories
        ]
    )
    lacz_observations_by_gene = lacz_observations_by_gene.withColumn(
        "mutantCounts",
        struct(*[to_camel_case(category.replace(" ", "_")) for category in categories]),
    )

    lacz_observations_by_gene = lacz_observations_by_gene.select(
        "gene_accession_id",
        "zygosity",
        "parameter_stable_id",
        "parameter_name",
        "mutantCounts",
    )

    wt_lacz_observations_by_strain = lacz_observations.where(
        col("biological_sample_group") == "control"
    )

    wt_lacz_observations_by_strain = wt_lacz_observations_by_strain.groupBy(
        "parameter_stable_id", "parameter_name"
    ).agg(
        *[
            sum(when(col("category") == category, 1).otherwise(0)).alias(
                to_camel_case(category.replace(" ", "_"))
            )
            for category in categories
        ]
    )

    wt_lacz_observations_by_strain = wt_lacz_observations_by_strain.withColumn(
        "controlCounts",
        struct(*[to_camel_case(category.replace(" ", "_")) for category in categories]),
    )

    wt_lacz_observations_by_strain = wt_lacz_observations_by_strain.select(
        "parameter_stable_id", "parameter_name", "controlCounts"
    )

    lacz_observations_by_gene = lacz_observations_by_gene.join(
        wt_lacz_observations_by_strain,
        ["parameter_stable_id", "parameter_name"],
        "left_outer",
    )

    lacz_images_by_gene = observations_df.where(
        (col("procedure_name") == procedure_name)
        & (col("observation_type") == "image_record")
        & (
            (col("parameter_name") != "LacZ Images Section")
            | (col("parameter_name") != "LacZ Images Wholemount")
        )
    )

    lacz_images_by_gene = lacz_images_by_gene.select(
        struct(
            "parameter_stable_id",
            "parameter_name",
        ).alias("expression_image_parameter"),
        "gene_accession_id",
        "zygosity",
        explode("parameter_association_name").alias("parameter_association_name"),
    ).distinct()
    lacz_images_by_gene = lacz_images_by_gene.groupBy(
        "gene_accession_id", "zygosity", "parameter_association_name"
    ).agg(
        collect_set("expression_image_parameter").alias("expression_image_parameters")
    )
    lacz_images_by_gene = lacz_images_by_gene.withColumnRenamed(
        "parameter_association_name", "parameter_name"
    )
    lacz_observations_by_gene = lacz_observations_by_gene.join(
        lacz_images_by_gene,
        ["gene_accession_id", "zygosity", "parameter_name"],
        "left_outer",
    )
    lacz_observations_by_gene = lacz_observations_by_gene.withColumn(
        "lacZLifestage", lit(lacz_lifestage)
    )
    return lacz_observations_by_gene


def to_camel_case(snake_str):
    components = snake_str.split("_")
    # We capitalize the first letter of each component except the first one
    # with the 'title' method and join them together.
    return components[0] + "".join(x.title() for x in components[1:])


class ImpcGeneStatsResultsMapper(PySparkTask):

    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGeneStatsResultsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            StatsResultsMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_statistical_results_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        stats_results_parquet_path = args[0]
        output_path = args[1]

        stats_results_df = spark.read.parquet(stats_results_parquet_path)
        explode_cols = [
            "procedure_stable_id",
            "procedure_name",
            "project_name",
            "life_stage_name",
        ]

        for col_name in explode_cols:
            stats_results_df = stats_results_df.withColumn(col_name, explode(col_name))
        stats_results_df = stats_results_df.select(
            "doc_id",
            "data_type",
            "marker_accession_id",
            "pipeline_stable_id",
            "procedure_stable_id",
            "procedure_name",
            "parameter_stable_id",
            "parameter_name",
            "allele_accession_id",
            "allele_name",
            "allele_symbol",
            "metadata_group",
            "zygosity",
            "phenotyping_center",
            "phenotype_sex",
            "project_name",
            "male_mutant_count",
            "female_mutant_count",
            "life_stage_name",
            "p_value",
            "effect_size",
            "significant",
            "mp_term_id",
            "mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
        )

        stats_results_df = stats_results_df.withColumn(
            "phenotype",
            struct(col("mp_term_id").alias("id"), col("mp_term_name").alias("name")),
        )

        stats_results_df = stats_results_df.withColumn(
            "topLevelPhenotypes",
            zip_with(
                "top_level_mp_term_id",
                "top_level_mp_term_name",
                lambda x, y: struct(x.alias("id"), y.alias("name")),
            ),
        )

        stats_results_df = stats_results_df.drop(
            "mp_term_id",
            "mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
        )

        stats_results_df = stats_results_df.withColumnRenamed(
            "marker_accession_id", "mgiGeneAccessionId"
        )
        stats_results_df = stats_results_df.withColumnRenamed(
            "doc_id", "statistical_result_id"
        )
        stats_results_df = stats_results_df.withColumn("id", col("mgiGeneAccessionId"))

        for col_name in stats_results_df.columns:
            stats_results_df = stats_results_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        stats_results_df = stats_results_df.withColumnRenamed(
            "phenotypingCenter", "phenotypingCentre"
        )
        stats_results_df = stats_results_df.withColumnRenamed(
            "phenotypeSex", "phenotypeSexes"
        )

        stats_results_df = stats_results_df.withColumn(
            "maleMutantCount", col("maleMutantCount").astype(IntegerType())
        )
        stats_results_df = stats_results_df.withColumn(
            "femaleMutantCount", col("femaleMutantCount").astype(IntegerType())
        )
        stats_results_df.repartition(1000).write.option(
            "ignoreNullFields", "false"
        ).json(output_path)


class ImpcGenePhenotypeHitsMapper(PySparkTask):

    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGenePhenotypeHitsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            GenotypePhenotypeLoader(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/gene_significant_phenotypes_json)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_phenotype_hits_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        gp_parquet_path = args[0]
        output_path = args[1]

        gp_df = spark.read.parquet(gp_parquet_path)
        explode_cols = ["procedure_stable_id", "procedure_name", "project_name"]

        for col_name in explode_cols:
            gp_df = gp_df.withColumn(col_name, explode(col_name))
        gp_df = gp_df.select(
            "statistical_result_id",
            "marker_accession_id",
            "pipeline_stable_id",
            "procedure_stable_id",
            "procedure_name",
            "parameter_stable_id",
            "parameter_name",
            "allele_accession_id",
            "allele_name",
            "allele_symbol",
            "zygosity",
            "phenotyping_center",
            "sex",
            "project_name",
            "p_value",
            "life_stage_name",
            "effect_size",
            "mp_term_id",
            "mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
        )

        gp_df = gp_df.withColumn(
            "phenotype",
            struct(col("mp_term_id").alias("id"), col("mp_term_name").alias("name")),
        )

        gp_df = gp_df.withColumn(
            "topLevelPhenotype",
            zip_with(
                "top_level_mp_term_id",
                "top_level_mp_term_name",
                lambda x, y: struct(x.alias("id"), y.alias("name")),
            ),
        )

        gp_df = gp_df.drop(
            "mp_term_id",
            "mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
        )

        gp_df = gp_df.withColumnRenamed("marker_accession_id", "mgiGeneAccessionId")
        gp_df = gp_df.withColumn("id", col("mgiGeneAccessionId"))

        for col_name in gp_df.columns:
            gp_df = gp_df.withColumnRenamed(col_name, to_camel_case(col_name))

        gp_df = gp_df.withColumn("lifeStageName", explode("lifeStageName"))
        gp_df = gp_df.withColumnRenamed("topLevelPhenotype", "topLevelPhenotypes")
        gp_df = gp_df.withColumnRenamed("phenotypingCenter", "phenotypingCentre")
        gp_df = gp_df.withColumn("pValue", col("pValue").astype(DoubleType()))
        gp_df = gp_df.withColumn("effectSize", col("effectSize").astype(DoubleType()))
        gp_df.repartition(100).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcLacZExpressionMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGeneStatsResultsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            ExperimentToObservationMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_expression_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        observations_parquet_path = args[0]
        output_path = args[1]

        observations_df = spark.read.parquet(observations_parquet_path)

        adult_lacz_expression_data = get_lacz_expression_data(observations_df, "adult")
        embryo_lacz_expression_data = get_lacz_expression_data(
            observations_df, "embryo"
        )

        lacz_expression_data = adult_lacz_expression_data.union(
            embryo_lacz_expression_data
        )
        lacz_expression_data = lacz_expression_data.withColumn(
            "id", col("gene_accession_id")
        )
        for col_name in lacz_expression_data.columns:
            lacz_expression_data = lacz_expression_data.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )
        lacz_expression_data = lacz_expression_data.withColumnRenamed(
            "geneAccessionId", "mgiGeneAccessionId"
        )
        lacz_expression_data.repartition(100).write.option(
            "ignoreNullFields", "false"
        ).json(output_path)


class ImpcPublicationsMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcPublicationsMapper"

    #: Path to the CSV gene publications association report
    gene_publications_json_path = luigi.Parameter()

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_publications_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.gene_publications_json_path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        gene_publications_json_path = args[0]
        output_path = args[1]

        publications_df = spark.read.json(gene_publications_json_path)
        publications_df = publications_df.withColumn("allele", explode("alleles"))
        publications_df = publications_df.select(
            "allele.*",
            "journalInfo.*",
            *[col_name for col_name in publications_df.columns if col_name != "allele"],
        )
        publications_df = publications_df.withColumn(
            "journalTitle", col("journal.title")
        )
        publications_df = publications_df.drop("journal")
        publications_df = publications_df.withColumnRenamed(
            "gacc", "mgiGeneAccessionId"
        )
        publications_df.repartition(100).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcProductsMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcProductsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [ProductReportExtractor()]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_order_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        products_parquet_path = args[0]
        output_path = args[1]

        products_df = spark.read.parquet(products_parquet_path)
        products_df = products_df.select(
            "mgi_accession_id",
            "marker_symbol",
            "allele_name",
            "allele_description",
            "type",
        )
        products_df = products_df.withColumn(
            "allele_symbol",
            concat(col("marker_symbol"), lit("<"), col("allele_name"), lit(">")),
        )
        products_df = products_df.groupBy(
            "mgi_accession_id", "allele_symbol", "allele_description"
        ).agg(collect_set("type").alias("product_types"))

        for col_name in products_df.columns:
            products_df = products_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        products_df.repartition(100).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcGeneImagesMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGeneImagesMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [ImpcImagesLoader()]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_images_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        impc_images_parquet_path = args[0]
        output_path = args[1]

        impc_images_df = spark.read.parquet(impc_images_parquet_path)

        impc_images_df = impc_images_df.groupBy(
            "gene_accession_id",
            "strain_accession_id",
            "procedure_stable_id",
            "procedure_name",
            "parameter_stable_id",
            "parameter_name",
        ).agg(
            count("observation_id").alias("count"),
            first("thumbnail_url").alias("thumbnail_url"),
            first("file_type").alias("file_type"),
        )

        impc_images_df = impc_images_df.withColumnRenamed(
            "gene_accession_id", "mgiGeneAccessionId"
        )

        for col_name in impc_images_df.columns:
            impc_images_df = impc_images_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        impc_images_df.repartition(500).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcGeneDiseasesMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGeneDiseasesMapper"

    #: Path to the CSV gene disease association report
    disease_model_summary_csv_path = luigi.Parameter()

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_diseases_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.disease_model_summary_csv_path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        disease_model_summary_csv_path = args[0]
        output_path = args[1]

        disease_df = spark.read.csv(disease_model_summary_csv_path, header=True)

        disease_df = disease_df.withColumn(
            "phenodigm_score",
            (col("disease_model_avg_norm") + col("disease_model_max_norm")) / 2,
        )

        max_values = disease_df.groupBy("disease_id", "marker_id").agg(
            max("phenodigm_score").alias("phenodigm_score")
        )
        max_disease_df = disease_df.join(
            max_values, ["disease_id", "marker_id", "phenodigm_score"]
        )

        max_disease_df = max_disease_df.withColumnRenamed("marker_id", "id")

        for col_name in max_disease_df.columns:
            max_disease_df = max_disease_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        double_cols = [
            "diseaseModelAvgNorm",
            "diseaseModelAvgRaw",
            "diseaseModelMaxRaw",
            "diseaseModelMaxNorm",
        ]

        for col_name in double_cols:
            max_disease_df = max_disease_df.withColumn(
                col_name, col(col_name).astype(DoubleType())
            )

        max_disease_df = max_disease_df.withColumn(
            "markerNumModels", col("markerNumModels").astype(IntegerType())
        )
        max_disease_df = max_disease_df.withColumn(
            "associationCurated", col("associationCurated").astype(BooleanType())
        )

        max_disease_df.repartition(500).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcGeneHistopathologyMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGeneHistopathologyMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            GenotypePhenotypeLoader(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/gene_histopath_json)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_histopathology_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        gp_parquet_path = args[0]
        output_path = args[1]

        gp_df = spark.read.parquet(gp_parquet_path)
        explode_cols = [
            "procedure_stable_id",
            "procedure_name",
            "project_name",
            "life_stage_name",
        ]

        for col_name in explode_cols:
            gp_df = gp_df.withColumn(col_name, explode(col_name))

        gp_df = gp_df.where(col("parameter_stable_id").contains("_HIS_"))

        gp_df = gp_df.select(
            "marker_accession_id",
            "pipeline_stable_id",
            "procedure_stable_id",
            "procedure_name",
            "parameter_stable_id",
            "parameter_name",
            "allele_accession_id",
            "allele_name",
            "allele_symbol",
            "zygosity",
            "phenotyping_center",
            "sex",
            "project_name",
            "life_stage_name",
            "mpath_term_id",
            "mpath_term_name",
        )

        for col_name in gp_df.columns:
            gp_df = gp_df.withColumnRenamed(col_name, to_camel_case(col_name))
        gp_df = gp_df.withColumnRenamed("markerAccessionId", "mgiGeneAccessionId")

        gp_df.repartition(500).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcSupportingDataMapper(PySparkTask):

    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcSupportingDataMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            StatsResultsMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/supporting_data_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        stats_results_parquet_path = args[0]
        output_path = args[1]

        stats_results_df = spark.read.parquet(stats_results_parquet_path)
        explode_cols = [
            "procedure_stable_key",
            "procedure_stable_id",
            "procedure_name",
            "parameter_stable_key",
            "project_name",
            "life_stage_name",
            "life_stage_acc",
        ]
        col_name_map = {
            "doc_id": "statisticalResultId",
            "marker_accession_id": "mgiGeneAccessionId",
            "marker_symbol": "geneSymbol",
            "metadata": "metadataValues",
            "production_center": "productionCentre",
            "phenotyping_center": "phenotypingCentre",
            "resource_fullname": "resourceFullName",
            "statistical_method": "name",
            "soft_windowing_bandwidth": "bandwidth",
            "soft_windowing_shape": "shape",
            "soft_windowing_peaks": "peaks",
            "soft_windowing_min_obs_required": "minObsRequired",
            "soft_windowing_total_obs_or_weight": "totalObsOrWeight",
            "soft_windowing_threshold": "threshold",
            "soft_windowing_number_of_doe": "numberOfDoe",
            "soft_windowing_doe_note": "doeNote",
            "effect_size": "reportedEffectSize",
            "p_value": "reportedPValue",
        }

        new_structs_dict = {
            "summaryStatistics": [
                "female_control_count",
                "female_control_mean",
                "female_control_sd",
                "male_control_count",
                "male_control_mean",
                "male_control_sd",
                "female_mutant_count",
                "female_mutant_mean",
                "female_mutant_sd",
                "male_mutant_count",
                "male_mutant_mean",
                "male_mutant_sd",
                "both_mutant_count",
                "both_mutant_mean",
                "both_mutant_sd",
            ],
            "statisticalMethod": [
                "statistical_method",
                {
                    "attributes": [
                        "female_ko_effect_p_value",
                        "female_ko_effect_stderr_estimate",
                        "female_ko_parameter_estimate",
                        "female_percentage_change",
                        "genotype_effect_p_value",
                        "genotype_effect_stderr_estimate",
                        "group_1_genotype",
                        "group_1_residuals_normality_test",
                        "group_2_genotype",
                        "group_2_residuals_normality_test",
                        "interaction_effect_p_value",
                        "interaction_significant",
                        "intercept_estimate",
                        "intercept_estimate_stderr_estimate",
                        "sex_effect_p_value",
                        "sex_effect_parameter_estimate",
                        "sex_effect_stderr_estimate",
                        "male_effect_size",
                        "female_effect_size",
                    ]
                },
            ],
            "softWindowing": [
                "soft_windowing_bandwidth",
                "soft_windowing_shape",
                "soft_windowing_peaks",
                "soft_windowing_min_obs_required",
                "soft_windowing_total_obs_or_weight",
                "soft_windowing_threshold",
                "soft_windowing_number_of_doe",
                "soft_windowing_doe_note",
            ],
        }

        int_columns = [
            "female_control_count",
            "male_control_count",
            "both_control_count",
            "female_mutant_count",
            "male_mutant_count",
            "both_mutant_count",
        ]
        double_columns = [
            "p_value",
            "effect_size",
            "female_ko_effect_p_value",
            "female_ko_parameter_estimate",
            "female_percentage_change",
            "genotype_effect_p_value",
            "male_ko_effect_p_value",
            "male_ko_parameter_estimate",
            "male_percentage_change",
        ]

        for col_name in int_columns:
            stats_results_df = stats_results_df.withColumn(
                col_name, col(col_name).astype(IntegerType())
            )
        for col_name in double_columns:
            stats_results_df = stats_results_df.withColumn(
                col_name, col(col_name).astype(DoubleType())
            )

        for col_name in explode_cols:
            stats_results_df = stats_results_df.withColumn(col_name, explode(col_name))
        stats_results_df = stats_results_df.select(
            "doc_id",  # statisticalResultId
            "strain_accession_id",
            "strain_name",
            "genetic_background",
            "colony_id",
            "marker_accession_id",  # mgiGeneAccessionId
            "marker_symbol",  # geneSymbol
            "allele_accession_id",
            "allele_name",
            "allele_symbol",
            "metadata_group",
            "metadata",  # metadataValues
            "life_stage_acc",  # explode
            "life_stage_name",  # explode
            "data_type",
            "production_center",  # productionCentre
            "phenotyping_center",  # phenotypingCentre
            "project_name",
            "resource_name",
            "resource_fullname",  # resourceFullName
            "pipeline_stable_key",
            "pipeline_stable_id",
            "pipeline_name",
            "procedure_stable_key",  # explode
            "procedure_stable_id",  # explode
            "procedure_name",  # explode
            "procedure_group",
            "parameter_stable_key",  # explode
            "parameter_stable_id",
            "parameter_name",
            "female_control_count",  # group under summaryStatistics
            "female_control_mean",  # group under summaryStatistics
            "female_control_sd",  # group under summaryStatistics
            "male_control_count",  # group under summaryStatistics
            "male_control_mean",  # group under summaryStatistics
            "male_control_sd",  # group under summaryStatistics
            "female_mutant_count",  # group under summaryStatistics
            "female_mutant_mean",  # group under summaryStatistics
            "female_mutant_sd",  # group under summaryStatistics
            "male_mutant_count",  # group under summaryStatistics
            "male_mutant_mean",  # group under summaryStatistics
            "male_mutant_sd",  # group under summaryStatistics
            "both_mutant_count",  # group under summaryStatistics
            "both_mutant_mean",  # group under summaryStatistics
            "both_mutant_sd",  # group under summaryStatistics
            "status",
            "sex",  # phenotypeSex
            "phenotype_sex",  # testedSexes
            "significant",
            "classification_tag",
            "p_value",  # reportedPValue
            "effect_size",  # reportedEffectSize
            "statistical_method",  # name, group under statisticalMethod
            "female_ko_effect_p_value",  # group under statisticalMethod.attributes
            "female_ko_effect_stderr_estimate",  # group under statisticalMethod.attributes
            "female_ko_parameter_estimate",  # group under statisticalMethod.attributes
            "female_percentage_change",  # group under statisticalMethod.attributes
            "genotype_effect_p_value",  # group under statisticalMethod.attributes
            "genotype_effect_stderr_estimate",  # group under statisticalMethod.attributes
            "group_1_genotype",  # group under statisticalMethod.attributes
            "group_1_residuals_normality_test",  # group under statisticalMethod.attributes
            "group_2_genotype",  # group under statisticalMethod.attributes
            "group_2_residuals_normality_test",  # group under statisticalMethod.attributes
            "interaction_effect_p_value",  # group under statisticalMethod.attributes
            "interaction_significant",  # group under statisticalMethod.attributes
            "intercept_estimate",  # group under statisticalMethod.attributes
            "intercept_estimate_stderr_estimate",  # group under statisticalMethod.attributes
            "sex_effect_p_value",  # group under statisticalMethod.attributes
            "sex_effect_parameter_estimate",  # group under statisticalMethod.attributes
            "sex_effect_stderr_estimate",  # group under statisticalMethod.attributes
            "male_effect_size",  # group under statisticalMethod.attributes
            "female_effect_size",  # group under statisticalMethod.attributes
            "soft_windowing_bandwidth",  # bandwidth, group under softWindowing
            "soft_windowing_shape",  # shape, group under softWindowing
            "soft_windowing_peaks",  # peaks, group under softWindowing
            "soft_windowing_min_obs_required",  # minObsRequired, group under softWindowing
            "soft_windowing_total_obs_or_weight",  # totalObsOrWeight, group under softWindowing
            "soft_windowing_threshold",  # threshold, group under softWindowing
            "soft_windowing_number_of_doe",  # numberOfDoe, group under softWindowing
            "soft_windowing_doe_note",  # doeNote, group under softWindowing
            "mp_term_id",
            "mp_term_name",
            "intermediate_mp_term_id",
            "intermediate_mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
            "mp_term_id_options",
            "mp_term_name_options",
        )

        stats_results_df = stats_results_df.withColumn(
            "soft_windowing_peaks",
            split(regexp_replace("soft_windowing_peaks", "[|]", ""), ",").cast(
                "array<int>"
            ),
        )
        stats_results_df = stats_results_df.withColumn(
            "significantPhenotype",
            struct(col("mp_term_id").alias("id"), col("mp_term_name").alias("name")),
        )
        stats_results_df = stats_results_df.withColumn(
            "intermediatePhenotypes",
            zip_with(
                "intermediate_mp_term_id",
                "intermediate_mp_term_name",
                lambda x, y: struct(x.alias("id"), y.alias("name")),
            ),
        )

        stats_results_df = stats_results_df.withColumn(
            "topLevelPhenotypes",
            zip_with(
                "top_level_mp_term_id",
                "top_level_mp_term_name",
                lambda x, y: struct(x.alias("id"), y.alias("name")),
            ),
        )

        stats_results_df = stats_results_df.withColumn(
            "potentialPhenotypes",
            zip_with(
                "mp_term_id_options",
                "mp_term_name_options",
                lambda x, y: struct(x.alias("id"), y.alias("name")),
            ),
        )

        stats_results_df = stats_results_df.drop(
            "mp_term_id",
            "mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
            "intermediate_mp_term_id",
            "intermediate_mp_term_name",
        )

        stats_results_df = stats_results_df.withColumnRenamed(
            "marker_accession_id", "mgiGeneAccessionId"
        )
        stats_results_df = stats_results_df.withColumnRenamed(
            "doc_id", "statistical_result_id"
        )
        stats_results_df = stats_results_df.withColumn("id", col("mgiGeneAccessionId"))

        drop_list = []
        for struct_col_name in new_structs_dict.keys():
            columns = new_structs_dict[struct_col_name]
            fields = []
            for column in columns:
                if type(column) == str:
                    fields.append(
                        col(column).alias(col_name_map[column])
                        if column in col_name_map
                        else col(column).alias(to_camel_case(column))
                    )
                    drop_list.append(column)
                else:
                    for sub_field_name in column.keys():
                        fields.append(
                            struct(
                                *[
                                    col(sub_column).alias(col_name_map[sub_column])
                                    if sub_column in col_name_map
                                    else col(sub_column).alias(
                                        to_camel_case(sub_column)
                                    )
                                    for sub_column in column[sub_field_name]
                                ]
                            ).alias(sub_field_name)
                        )
                        for sub_column in column[sub_field_name]:
                            drop_list.append(sub_column)

            stats_results_df = stats_results_df.withColumn(
                struct_col_name,
                struct(*fields),
            )

        stats_results_df = stats_results_df.drop(*drop_list)

        for col_name in stats_results_df.columns:
            stats_results_df = stats_results_df.withColumnRenamed(
                col_name,
                col_name_map[col_name]
                if col_name in col_name_map
                else to_camel_case(col_name),
            )

        stats_results_df = stats_results_df.withColumn(
            "maleMutantCount", col("maleMutantCount").astype(IntegerType())
        )
        stats_results_df = stats_results_df.withColumn(
            "femaleMutantCount", col("femaleMutantCount").astype(IntegerType())
        )
        stats_results_df.limit(100).repartition(1000).write.option(
            "ignoreNullFields", "false"
        ).json(output_path)


# class ImpcWebApiMapper(luigi.Task):
#     name = "IMPC_Web_Api_Mapper"
#
#     def requires(self):
#         return [
#             ImpcGeneSummaryMapper(),
#             ImpcGenePhenotypeHitsMapper(),
#             ImpcGeneStatsResultsMapper(),
#             ImpcLacZExpressionMapper(),
#             ImpcGeneImagesMapper(),
#             ImpcGeneHistopathologyMapper(),
#             ImpcGeneDiseasesMapper(),
#             ImpcPublicationsMapper(),
#             ImpcProductsMapper(),
#         ]
