import luigi
from luigi.contrib.spark import PySparkTask
from pyspark import SparkContext
from pyspark.sql import SparkSession, Window
import json
import os
from pyspark.sql.types import (
    DoubleType,
    IntegerType,
    BooleanType,
    ArrayType,
    StringType,
    StructType,
    StructField,
)
from pyspark.sql.functions import (
    col,
    first,
    explode,
    zip_with,
    struct,
    when,
    sum,
    collect_set,
    lit,
    concat,
    count,
    max,
    avg,
    regexp_replace,
    split,
    arrays_zip,
    expr,
    concat_ws,
    countDistinct,
    array_contains,
    array_union,
    array,
    udf,
    row_number,
    avg,
    stddev,
    count,
    quarter,
)

from impc_etl.jobs.extract import ProductReportExtractor
from impc_etl.jobs.load import ExperimentToObservationMapper
from impc_etl.jobs.load.solr.gene_mapper import GeneLoader
from impc_etl.jobs.load.solr.genotype_phenotype_mapper import GenotypePhenotypeLoader
from impc_etl.jobs.load.solr.impc_images_mapper import ImpcImagesLoader
from impc_etl.jobs.load.solr.mp_mapper import MpLoader
from impc_etl.jobs.load.solr.pipeline_mapper import ImpressToParameterMapper
from impc_etl.jobs.load.solr.stats_results_mapper import StatsResultsMapper
from impc_etl.workflow import SmallPySparkTask
from impc_etl.workflow.config import ImpcConfig

GENE_SUMMARY_MAPPINGS = {
    "mgi_accession_id": "mgiGeneAccessionId",
    "marker_symbol": "geneSymbol",
    "marker_name": "geneName",
    "marker_synonym": "synonyms",
    "significant_top_level_mp_terms": "significantTopLevelPhenotypes",
    "not_significant_top_level_mp_terms": "notSignificantTopLevelPhenotypes",
    "embryo_data_available": "hasEmbryoImagingData",
    "human_gene_symbol": "human_gene_symbols",
    "human_symbol_synonym": "human_symbol_synonyms",
    "production_centre": "production_centres",
    "phenotyping_centre": "phenotyping_centres",
    "allele_name": "allele_names",
    "ensembl_gene_id": "ensembl_gene_ids",
}

# stats_df.select("doc_id", "procedure_stable_id", "parameter_stable_id", "marker_accession_id").withColumn("procedure_stable_id", explode("procedure_stable_id")).where(col("parameter_stable_id") == "IMPC_BWT_008_001").join(raw_data, "doc_id", "left_outer").withColumn("observation_id", explode("observation_id")).drop("window_weight").join(exp_df.drop("procedure_stable_id", "parameter_stable_id"), "observation_id", "left_outer").select("doc_id", "procedure_stable_id", "parameter_stable_id", "marker_accession_id", "biological_sample_group", "sex", "zygosity", "discrete_point", "data_point").groupBy("doc_id", "procedure_stable_id", "parameter_stable_id", "marker_accession_id", "biological_sample_group", "sex", "zygosity", "discrete_point").agg(avg("data_point").alias("mean"), stddev("data_point").alias("std"), count("data_point").alias("count")).groupBy(col("doc_id").alias("datasetId"), col("procedure_stable_id").alias("procedureStableId"), col("parameter_stable_id").alias("parameterStableId"), col("marker_accession_id").alias("mgiGeneAccessionId")).agg(collect_set(struct( col("biological_sample_group").alias("sampleGroup"), "sex", "zygosity", col("discrete_point").alias("ageInWeeks"), "mean", "std", "count")).alias("dataPoints")).write.json("/nfs/production/tudor/komp/data-releases/latest-input/dr19.2/output/impc_web_api/bwt_curve_service_json")


class ImpcGeneSummaryMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "Impc_Gene_Summary_Mapper"

    #: Path to the CSV gene disease association report
    gene_disease_association_csv_path = luigi.Parameter()

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            GeneLoader(),
            GenotypePhenotypeLoader(),
            ExperimentToObservationMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_summary_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.input()[1].path,
            self.input()[2].path,
            self.gene_disease_association_csv_path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        gene_parquet_path = args[0]
        genotype_phenotype_parquet_path = args[1]
        observations_parquet_path = args[2]
        gene_disease_association_csv_path = args[3]
        output_path = args[4]

        gene_df = spark.read.parquet(gene_parquet_path)
        genotype_phenotype_df = spark.read.parquet(genotype_phenotype_parquet_path)
        observations_df = spark.read.parquet(observations_parquet_path)
        gene_disease_association_df = spark.read.csv(
            gene_disease_association_csv_path, header=True
        )

        gene_df = gene_df.withColumn("id", col("mgi_accession_id"))
        for col_name in GENE_SUMMARY_MAPPINGS.keys():
            gene_df = gene_df.withColumnRenamed(
                col_name, GENE_SUMMARY_MAPPINGS[col_name]
            )
        gene_df = gene_df.drop("ccds_ids")
        gene_df = gene_df.withColumnRenamed("ccds_id", "ccds_ids")
        genotype_phenotype_df = genotype_phenotype_df.withColumnRenamed(
            "marker_accession_id", "id"
        )
        gp_call_by_gene = genotype_phenotype_df.groupBy("id").count()
        gp_call_by_gene = gp_call_by_gene.withColumnRenamed(
            "count", "significantPhenotypesCount"
        )
        gene_df = gene_df.join(gp_call_by_gene, "id", "left_outer")

        adult_lacz_observations_by_gene = get_lacz_expression_count(
            observations_df, "adult"
        )
        gene_df = gene_df.join(adult_lacz_observations_by_gene, "id", "left_outer")

        embryo_lacz_observations_by_gene = get_lacz_expression_count(
            observations_df, "embryo"
        )
        gene_df = gene_df.join(embryo_lacz_observations_by_gene, "id", "left_outer")

        gene_disease_association_df = gene_disease_association_df.where(
            col("type") == "disease_gene_summary"
        )

        gene_disease_association_df = gene_disease_association_df.groupBy(
            "marker_id"
        ).agg(sum(when(col("disease_id").isNotNull(), 1).otherwise(0)).alias("count"))
        gene_disease_association_df = gene_disease_association_df.withColumnRenamed(
            "marker_id", "id"
        )
        gene_disease_association_df = gene_disease_association_df.withColumnRenamed(
            "count", "associatedDiseasesCount"
        )
        gene_df = gene_df.join(gene_disease_association_df, "id", "left_outer")

        gene_df = gene_df.withColumn(
            "hasLacZData",
            (col("adultExpressionObservationsCount") > 0)
            | (col("embryoExpressionObservationsCount") > 0),
        )

        gene_images_flag = observations_df.where(
            col("observation_type") == "image_record"
        )
        gene_images_flag = gene_images_flag.groupBy("gene_accession_id").agg(
            first("observation_id").alias("obs_id")
        )
        gene_images_flag = gene_images_flag.withColumn(
            "hasImagingData", col("obs_id").isNotNull()
        )
        gene_images_flag = gene_images_flag.withColumnRenamed("gene_accession_id", "id")
        gene_images_flag = gene_images_flag.drop("obs_id")
        gene_df = gene_df.join(gene_images_flag, "id", "left_outer")

        gene_hist_flag = observations_df.where(
            col("procedure_stable_id").contains("HIS")
        )
        gene_hist_flag = gene_hist_flag.groupBy("gene_accession_id").agg(
            first("observation_id").alias("obs_id")
        )
        gene_hist_flag = gene_hist_flag.withColumn(
            "hasHistopathologyData", col("obs_id").isNotNull()
        )
        gene_hist_flag = gene_hist_flag.withColumnRenamed("gene_accession_id", "id")
        gene_hist_flag = gene_hist_flag.drop("obs_id")

        gene_df = gene_df.join(gene_hist_flag, "id", "left_outer")

        gene_via_flag = observations_df.where(
            col("parameter_stable_id").isin(
                [
                    "IMPC_VIA_001_001",
                    "IMPC_VIA_002_001",
                    "IMPC_EVL_001_001",
                    "IMPC_EVM_001_001",
                    "IMPC_EVP_001_001",
                    "IMPC_EVO_001_001",
                    "IMPC_VIA_063_001",
                    "IMPC_VIA_064_001",
                    "IMPC_VIA_065_001",
                    "IMPC_VIA_066_001",
                    "IMPC_VIA_067_001",
                    "IMPC_VIA_056_001",
                ]
            )
        )
        gene_via_flag = gene_via_flag.groupBy("gene_accession_id").agg(
            first("observation_id").alias("obs_id")
        )
        gene_via_flag = gene_via_flag.withColumn(
            "hasViabilityData", col("obs_id").isNotNull()
        )
        gene_via_flag = gene_via_flag.withColumnRenamed("gene_accession_id", "id")
        gene_via_flag = gene_via_flag.drop("obs_id")
        gene_df = gene_df.join(gene_via_flag, "id", "left_outer")

        gene_bw_flag = observations_df.where(
            col("parameter_stable_id") == "IMPC_BWT_008_001"
        )
        gene_bw_flag = gene_bw_flag.groupBy("gene_accession_id").agg(
            first("observation_id").alias("obs_id")
        )
        gene_bw_flag = gene_bw_flag.withColumn(
            "hasBodyWeightData", col("obs_id").isNotNull()
        )
        gene_bw_flag = gene_bw_flag.withColumnRenamed("gene_accession_id", "id")
        gene_bw_flag = gene_bw_flag.drop("obs_id")
        gene_df = gene_df.join(gene_bw_flag, "id", "left_outer")

        gene_df = gene_df.drop("datasets_raw_data")

        for col_name in gene_df.columns:
            gene_df = gene_df.withColumnRenamed(col_name, to_camel_case(col_name))

        gene_avg_df = gene_df.where(col("phenotypingDataAvailable") == True).select(
            avg("significantPhenotypesCount").alias("significantPhenotypesAverage"),
            avg("associatedDiseasesCount").alias("associatedDiseasesAverage"),
            avg("adultExpressionObservationsCount").alias(
                "adultExpressionObservationsAverage"
            ),
            avg("embryoExpressionObservationsCount").alias(
                "embryoExpressionObservationsAverage"
            ),
        )
        gene_avg_df.repartition(1).write.option("ignoreNullFields", "false").json(
            output_path + "_avgs"
        )
        gene_df.repartition(100).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcGeneSearchMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "Impc_Gene_Search_Mapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return GeneLoader()

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_search_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input().path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        gene_parquet_path = args[0]
        gene_df = spark.read.parquet(gene_parquet_path)
        output_path = args[1]

        for col_name in GENE_SUMMARY_MAPPINGS.keys():
            gene_df = gene_df.withColumnRenamed(
                col_name, GENE_SUMMARY_MAPPINGS[col_name]
            )

        for col_name in gene_df.columns:
            gene_df = gene_df.withColumnRenamed(col_name, to_camel_case(col_name))

        gene_search_df = gene_df.select(
            "mgiGeneAccessionId",
            "geneName",
            "geneSymbol",
            "synonyms",
            "humanGeneSymbols",
            "humanSymbolSynonyms",
            "esCellProductionStatus",
            "mouseProductionStatus",
            "phenotypeStatus",
            "phenotypingDataAvailable",
        )
        gene_search_df.repartition(1).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcPhenotypeSummaryMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "Impc_Phenotype_Summary_Mapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            MpLoader(),
            ImpressToParameterMapper(),
            StatsResultsMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/phenotype_summary_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.input()[1].path,
            self.input()[2].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        mp_parquet_path = args[0]
        impress_parameter_parquet_path = args[1]
        stats_results_parquet_path = args[2]
        output_path = args[3]

        mp_df = spark.read.parquet(mp_parquet_path)
        impress_df = spark.read.parquet(impress_parameter_parquet_path)
        stats_results_df = spark.read.parquet(stats_results_parquet_path)

        stats_results_df = (
            stats_results_df.select(
                "marker_accession_id",
                "status",
                "top_level_mp_term_id",
                "intermediate_mp_term_id",
                when(col("mp_term_id").isNotNull(), array("mp_term_id"))
                .otherwise(col("mp_term_id_options"))
                .alias("mp_term_id_options"),
                "significant",
            )
            .where(col("status") != "NotProcessed")
            .distinct()
        )

        stats_results_df = stats_results_df.groupBy(
            "marker_accession_id",
            "status",
            "top_level_mp_term_id",
            "intermediate_mp_term_id",
            "mp_term_id_options",
        ).agg(max("significant").alias("significant"))

        stats_results_df = stats_results_df.withColumn(
            "mp_ids",
            array_union(
                "top_level_mp_term_id",
                array_union(
                    "intermediate_mp_term_id",
                    "mp_term_id_options",
                ),
            ),
        ).drop(
            "status",
            "top_level_mp_term_id",
            "intermediate_mp_term_id",
            "mp_term_id_options",
        )
        stats_results_df = (
            stats_results_df.withColumn("mp_id", explode("mp_ids"))
            .drop("mp_ids")
            .distinct()
        )
        stats_results_df = stats_results_df.groupBy("mp_id").agg(
            sum(when(col("significant") == True, 1).otherwise(0)).alias(
                "significant_genes"
            ),
            sum(when(col("significant") == False, 1).otherwise(0)).alias(
                "not_significant_genes"
            ),
        )

        phenotype_summary_df = mp_df.join(stats_results_df, "mp_id", "left")

        impress_df = (
            impress_df.select(
                "pipeline_stable_id",
                "pipeline_name",
                "procedure_stable_id",
                "procedure_name",
                "procedure_stable_key",
                "procedure.description",
                array_union(
                    "mp_id",
                    array_union(
                        "top_level_mp_id",
                        array_union(
                            "intermediate_mp_id",
                            array_union(
                                "abnormal_mp_id",
                                array_union("increased_mp_id", "decreased_mp_id"),
                            ),
                        ),
                    ),
                ).alias("phenotype_ids"),
            )
            .where(col("phenotype_ids").isNotNull())
            .distinct()
        )

        for col_name in impress_df.columns:
            impress_df = impress_df.withColumnRenamed(col_name, to_camel_case(col_name))
        impress_df = impress_df.withColumn("mp_id", explode("phenotypeIds"))
        impress_df = impress_df.drop("phenotypeIds")
        impress_df = impress_df.groupBy("mp_id").agg(
            collect_set(
                struct(
                    *[
                        col_name
                        for col_name in impress_df.columns
                        if col_name != "mp_id"
                    ]
                )
            ).alias("procedures")
        )
        phenotype_summary_df = phenotype_summary_df.join(impress_df, "mp_id", "left")

        phenotype_summary_mappings = {
            "mp_id": "phenotypeId",
            "mp_term": "phenotypeName",
            "mp_definition": "phenotypeDefinition",
            "mp_term_synonym": "phenotypeSynonyms",
        }

        for col_name in phenotype_summary_mappings.keys():
            phenotype_summary_df = phenotype_summary_df.withColumnRenamed(
                col_name, phenotype_summary_mappings[col_name]
            )

        for col_name in phenotype_summary_df.columns:
            phenotype_summary_df = phenotype_summary_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        phenotype_summary_df = phenotype_summary_df.withColumn(
            "topLevelPhenotypes",
            zip_with(
                "topLevelMpId",
                "topLevelMpTerm",
                lambda x, y: struct(x.alias("id"), y.alias("name")),
            ),
        )

        phenotype_summary_df.select(
            "phenotypeId",
            "phenotypeName",
            "phenotypeDefinition",
            "phenotypeSynonyms",
            "significantGenes",
            "notSignificantGenes",
            "procedures",
            "topLevelPhenotypes",
        ).distinct().repartition(100).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcPhenotypeGenotypeHitsMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "Impc_Phenotype_Genotype_Hits_Mapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [MpLoader(), GenotypePhenotypeLoader()]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/phenotype_genotype_hits_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.input()[1].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        mp_parquet_path = args[0]
        impress_parameter_parquet_path = args[1]
        stats_results_parquet_path = args[2]
        output_path = args[3]

        mp_df = spark.read.parquet(mp_parquet_path)
        impress_df = spark.read.parquet(impress_parameter_parquet_path)
        stats_results_df = spark.read.parquet(stats_results_parquet_path)

        stats_results_df = (
            stats_results_df.select(
                "marker_accession_id",
                "status",
                "top_level_mp_term_id",
                "intermediate_mp_term_id",
                when(col("mp_term_id").isNotNull(), array("mp_term_id"))
                .otherwise(col("mp_term_id_options"))
                .alias("mp_term_id_options"),
                "significant",
            )
            .where(col("status") != "NotProcessed")
            .distinct()
        )

        stats_results_df = stats_results_df.groupBy(
            "marker_accession_id",
            "status",
            "top_level_mp_term_id",
            "intermediate_mp_term_id",
            "mp_term_id_options",
        ).agg(max("significant").alias("significant"))

        stats_results_df = stats_results_df.withColumn(
            "mp_ids",
            array_union(
                "top_level_mp_term_id",
                array_union(
                    "intermediate_mp_term_id",
                    "mp_term_id_options",
                ),
            ),
        ).drop(
            "status",
            "top_level_mp_term_id",
            "intermediate_mp_term_id",
            "mp_term_id_options",
        )
        stats_results_df = (
            stats_results_df.withColumn("mp_id", explode("mp_ids"))
            .drop("mp_ids")
            .distinct()
        )
        stats_results_df = stats_results_df.groupBy("mp_id").agg(
            sum(when(col("significant") == True, 1).otherwise(0)).alias(
                "significant_genes"
            ),
            sum(when(col("significant") == False, 1).otherwise(0)).alias(
                "not_significant_genes"
            ),
        )

        phenotype_summary_df = mp_df.join(stats_results_df, "mp_id", "left")

        impress_df = (
            impress_df.select(
                "pipeline_stable_id",
                "pipeline_name",
                "procedure_stable_id",
                "procedure_name",
                "procedure_stable_key",
                "procedure.description",
                array_union(
                    "mp_id",
                    array_union(
                        "top_level_mp_id",
                        array_union(
                            "intermediate_mp_id",
                            array_union(
                                "abnormal_mp_id",
                                array_union("increased_mp_id", "decreased_mp_id"),
                            ),
                        ),
                    ),
                ).alias("phenotype_ids"),
            )
            .where(col("phenotype_ids").isNotNull())
            .distinct()
        )

        for col_name in impress_df.columns:
            impress_df = impress_df.withColumnRenamed(col_name, to_camel_case(col_name))
        impress_df = impress_df.withColumn("mp_id", explode("phenotypeIds"))
        impress_df = impress_df.drop("phenotypeIds")
        impress_df = impress_df.groupBy("mp_id").agg(
            collect_set(
                struct(
                    *[
                        col_name
                        for col_name in impress_df.columns
                        if col_name != "mp_id"
                    ]
                )
            ).alias("procedures")
        )
        phenotype_summary_df = phenotype_summary_df.join(impress_df, "mp_id", "left")

        phenotype_summary_mappings = {
            "mp_id": "phenotypeId",
            "mp_term": "phenotypeName",
            "mp_definition": "phenotypeDefinition",
            "mp_term_synonym": "phenotypeSynonyms",
        }

        for col_name in phenotype_summary_mappings.keys():
            phenotype_summary_df = phenotype_summary_df.withColumnRenamed(
                col_name, phenotype_summary_mappings[col_name]
            )

        for col_name in phenotype_summary_df.columns:
            phenotype_summary_df = phenotype_summary_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        phenotype_summary_df = phenotype_summary_df.withColumn(
            "topLevelPhenotypes",
            zip_with(
                "topLevelMpId",
                "topLevelMpTerm",
                lambda x, y: struct(x.alias("id"), y.alias("name")),
            ),
        )

        phenotype_summary_df.select(
            "mpId",
            "mpName",
            "mpDefinition",
            "mpSynonyms",
            "significantGenes",
            "notSignificantGenes",
            "procedures",
            "topLevelPhenotypes",
        ).distinct().repartition(100).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcPhenotypeSearchMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "Impc_Phenotype_Search_Mapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [MpLoader(), GenotypePhenotypeLoader()]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/phenotype_search_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.input()[1].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        mp_parquet_path = args[0]
        genotype_phenotype_parquet_path = args[1]
        output_path = args[2]

        mp_df = spark.read.parquet(mp_parquet_path)
        genotype_phenotype_df = spark.read.parquet(genotype_phenotype_parquet_path)
        genotype_phenotype_df = genotype_phenotype_df.select(
            "mp_term_id",
            "intermediate_mp_term_id",
            "top_level_mp_term_id",
            "marker_accession_id",
        ).distinct()

        phenotype_gene_count_df = (
            mp_df.join(
                genotype_phenotype_df,
                (col("mp_id") == col("mp_term_id"))
                | (array_contains(col("intermediate_mp_term_id"), col("mp_id")))
                | (
                    array_contains(
                        genotype_phenotype_df["top_level_mp_term_id"], col("mp_id")
                    )
                ),
            )
            .groupBy("mp_id")
            .agg(countDistinct("marker_accession_id").alias("geneCount"))
        )

        phenotype_search_df = mp_df.join(phenotype_gene_count_df, "mp_id", "left")
        phenotype_search_df = phenotype_search_df.withColumn(
            "topLevelParents",
            zip_with(
                "top_level_mp_term_id",
                "top_level_mp_term",
                lambda x, y: struct(x.alias("key"), y.alias("value")),
            ),
        )

        phenotype_search_df = phenotype_search_df.withColumn(
            "intermediateLevelParents",
            zip_with(
                "intermediate_mp_id",
                "intermediate_mp_term",
                lambda x, y: struct(x.alias("key"), y.alias("value")),
            ),
        )

        for col_name in phenotype_search_df.columns:
            phenotype_search_df = phenotype_search_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        phenotype_search_df = phenotype_search_df.select(
            "mpId",
            col("mpTerm").alias("phenotypeName"),
            col("mpTermSynonym").alias("synonyms"),
            col("mpDefinition").alias("definition"),
            "topLevelParents",
            "intermediateLevelParents",
            "geneCount",
        )

        phenotype_search_df.repartition(1).write.option(
            "ignoreNullFields", "false"
        ).json(output_path)


def get_lacz_expression_count(observations_df, lacz_lifestage):
    procedure_name = "Adult LacZ" if lacz_lifestage == "adult" else "Embryo LacZ"
    lacz_observations_by_gene = observations_df.where(
        (col("procedure_name") == procedure_name)
        & (col("observation_type") == "categorical")
        & (col("parameter_name") != "LacZ Images Section")
        & (col("parameter_name") != "LacZ Images Wholemount")
    )
    lacz_observations_by_gene = lacz_observations_by_gene.select(
        "gene_accession_id", "zygosity", "parameter_name"
    ).distinct()
    lacz_observations_by_gene = lacz_observations_by_gene.groupBy(
        "gene_accession_id"
    ).agg(sum(when(col("parameter_name").isNotNull(), 1).otherwise(0)).alias("count"))
    lacz_observations_by_gene = lacz_observations_by_gene.withColumnRenamed(
        "count", f"{lacz_lifestage}ExpressionObservationsCount"
    )
    lacz_observations_by_gene = lacz_observations_by_gene.withColumnRenamed(
        "gene_accession_id", "id"
    )

    return lacz_observations_by_gene


def get_lacz_expression_data(observations_df, lacz_lifestage):
    procedure_name = "Adult LacZ" if lacz_lifestage == "adult" else "Embryo LacZ"

    lacz_observations = observations_df.where(
        (col("procedure_name") == procedure_name)
        & (col("observation_type") == "categorical")
        & (col("parameter_name") != "LacZ Images Section")
        & (col("parameter_name") != "LacZ Images Wholemount")
    )
    categories = [
        "expression",
        "tissue not available",
        "no expression",
        "imageOnly",
        "ambiguous",
    ]
    lacz_observations_by_gene = lacz_observations.groupBy(
        "strain_accession_id",
        "gene_accession_id",
        "zygosity",
        "parameter_stable_id",
        "parameter_name",
    ).agg(
        *[
            sum(when(col("category") == category, 1).otherwise(0)).alias(
                to_camel_case(category.replace(" ", "_"))
            )
            for category in categories
        ]
    )
    lacz_observations_by_gene = lacz_observations_by_gene.withColumn(
        "mutantCounts",
        struct(*[to_camel_case(category.replace(" ", "_")) for category in categories]),
    )

    lacz_observations_by_gene = lacz_observations_by_gene.select(
        "gene_accession_id",
        "zygosity",
        "parameter_stable_id",
        "parameter_name",
        "mutantCounts",
    )

    wt_lacz_observations_by_strain = lacz_observations.where(
        col("biological_sample_group") == "control"
    )

    wt_lacz_observations_by_strain = wt_lacz_observations_by_strain.groupBy(
        "parameter_stable_id", "parameter_name"
    ).agg(
        *[
            sum(when(col("category") == category, 1).otherwise(0)).alias(
                to_camel_case(category.replace(" ", "_"))
            )
            for category in categories
        ]
    )

    wt_lacz_observations_by_strain = wt_lacz_observations_by_strain.withColumn(
        "controlCounts",
        struct(*[to_camel_case(category.replace(" ", "_")) for category in categories]),
    )

    wt_lacz_observations_by_strain = wt_lacz_observations_by_strain.select(
        "parameter_stable_id", "parameter_name", "controlCounts"
    )

    lacz_observations_by_gene = lacz_observations_by_gene.join(
        wt_lacz_observations_by_strain,
        ["parameter_stable_id", "parameter_name"],
        "left_outer",
    )

    lacz_images_by_gene = observations_df.where(
        (col("procedure_name") == procedure_name)
        & (col("observation_type") == "image_record")
        & (
            (col("parameter_name") != "LacZ Images Section")
            | (col("parameter_name") != "LacZ Images Wholemount")
        )
    )

    lacz_images_by_gene = lacz_images_by_gene.select(
        struct(
            "parameter_stable_id",
            "parameter_name",
        ).alias("expression_image_parameter"),
        "gene_accession_id",
        "zygosity",
        explode("parameter_association_name").alias("parameter_association_name"),
    ).distinct()
    lacz_images_by_gene = lacz_images_by_gene.groupBy(
        "gene_accession_id", "zygosity", "parameter_association_name"
    ).agg(
        collect_set("expression_image_parameter").alias("expression_image_parameters")
    )
    lacz_images_by_gene = lacz_images_by_gene.withColumnRenamed(
        "parameter_association_name", "parameter_name"
    )
    lacz_observations_by_gene = lacz_observations_by_gene.join(
        lacz_images_by_gene,
        ["gene_accession_id", "zygosity", "parameter_name"],
        "left_outer",
    )
    lacz_observations_by_gene = lacz_observations_by_gene.withColumn(
        "lacZLifestage", lit(lacz_lifestage)
    )
    return lacz_observations_by_gene


def to_camel_case(snake_str):
    components = snake_str.split("_")
    # We capitalize the first letter of each component except the first one
    # with the 'title' method and join them together.
    return components[0] + "".join(x.title() for x in components[1:])


def phenotype_term_zip_udf(x, y):
    return when(x.isNotNull(), struct(x.alias("id"), y.alias("name"))).otherwise(
        lit(None)
    )


class ImpcGeneStatsResultsMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGeneStatsResultsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            StatsResultsMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_statistical_results_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        stats_results_parquet_path = args[0]
        output_path = args[1]

        stats_results_df = spark.read.parquet(stats_results_parquet_path)
        explode_cols = [
            "procedure_stable_id",
            "procedure_name",
            "project_name",
            "life_stage_name",
        ]

        for col_name in explode_cols:
            stats_results_df = stats_results_df.withColumn(col_name, explode(col_name))
        stats_results_df = stats_results_df.select(
            "doc_id",
            "data_type",
            "marker_accession_id",
            "pipeline_stable_id",
            "procedure_stable_id",
            "procedure_name",
            "parameter_stable_id",
            "parameter_name",
            "allele_accession_id",
            "allele_name",
            "allele_symbol",
            "metadata_group",
            "zygosity",
            "phenotyping_center",
            "phenotype_sex",
            "project_name",
            "male_mutant_count",
            "female_mutant_count",
            "life_stage_name",
            "p_value",
            "effect_size",
            "significant",
            "mp_term_id",
            "mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
        )

        stats_results_df = stats_results_df.withColumn(
            "phenotype",
            struct(col("mp_term_id").alias("id"), col("mp_term_name").alias("name")),
        )

        stats_results_df = stats_results_df.withColumn(
            "topLevelPhenotypes",
            zip_with(
                "top_level_mp_term_id",
                "top_level_mp_term_name",
                lambda x, y: struct(x.alias("id"), y.alias("name")),
            ),
        )

        stats_results_df = stats_results_df.drop(
            "mp_term_id",
            "mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
        )

        stats_results_df = stats_results_df.withColumnRenamed(
            "marker_accession_id", "mgiGeneAccessionId"
        )
        stats_results_df = stats_results_df.withColumnRenamed(
            "doc_id", "statistical_result_id"
        )
        stats_results_df = stats_results_df.withColumn("id", col("mgiGeneAccessionId"))

        for col_name in stats_results_df.columns:
            stats_results_df = stats_results_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        stats_results_df = stats_results_df.withColumnRenamed(
            "phenotypingCenter", "phenotypingCentre"
        )
        stats_results_df = stats_results_df.withColumnRenamed(
            "phenotypeSex", "phenotypeSexes"
        )

        stats_results_df = stats_results_df.withColumn(
            "maleMutantCount", col("maleMutantCount").astype(IntegerType())
        )
        stats_results_df = stats_results_df.withColumn(
            "femaleMutantCount", col("femaleMutantCount").astype(IntegerType())
        )
        stats_results_df.repartition(1000).write.option(
            "ignoreNullFields", "false"
        ).json(output_path)


class ImpcGenePhenotypeHitsMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGenePhenotypeHitsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            GenotypePhenotypeLoader(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/gene_significant_phenotypes_json)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_phenotype_hits_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        gp_parquet_path = args[0]
        output_path = args[1]

        gp_df = spark.read.parquet(gp_parquet_path)
        explode_cols = ["procedure_stable_id", "procedure_name", "project_name"]

        for col_name in explode_cols:
            gp_df = gp_df.withColumn(col_name, explode(col_name))
        gp_df = gp_df.select(
            "statistical_result_id",
            "marker_accession_id",
            "pipeline_stable_id",
            "procedure_stable_id",
            "procedure_name",
            "parameter_stable_id",
            "parameter_name",
            "allele_accession_id",
            "allele_name",
            "allele_symbol",
            "zygosity",
            "phenotyping_center",
            "sex",
            "project_name",
            "p_value",
            "life_stage_name",
            "effect_size",
            "mp_term_id",
            "mp_term_name",
            "intermediate_mp_term_id",
            "intermediate_mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
        )

        gp_df = gp_df.withColumn(
            "phenotype",
            struct(col("mp_term_id").alias("id"), col("mp_term_name").alias("name")),
        )

        gp_df = gp_df.withColumn(
            "intermediatePhenotype",
            zip_with(
                "intermediate_mp_term_id",
                "intermediate_mp_term_name",
                lambda x, y: struct(x.alias("id"), y.alias("name")),
            ),
        )

        gp_df = gp_df.withColumn(
            "topLevelPhenotype",
            zip_with(
                "top_level_mp_term_id",
                "top_level_mp_term_name",
                lambda x, y: struct(x.alias("id"), y.alias("name")),
            ),
        )

        gp_df = gp_df.drop(
            "mp_term_id",
            "mp_term_name",
            "intermediate_mp_term_id",
            "intermediate_mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
        )

        gp_df = gp_df.withColumnRenamed("marker_accession_id", "mgiGeneAccessionId")
        gp_df = gp_df.withColumn("id", col("mgiGeneAccessionId"))

        for col_name in gp_df.columns:
            gp_df = gp_df.withColumnRenamed(col_name, to_camel_case(col_name))

        gp_df = gp_df.withColumn("lifeStageName", explode("lifeStageName"))
        gp_df = gp_df.withColumnRenamed("topLevelPhenotype", "topLevelPhenotypes")
        gp_df = gp_df.withColumnRenamed(
            "intermediatePhenotype", "intermediatePhenotypes"
        )
        gp_df = gp_df.withColumnRenamed("phenotypingCenter", "phenotypingCentre")
        gp_df = gp_df.withColumnRenamed("statisticalResultId", "datasetId")
        gp_df = gp_df.withColumn("pValue", col("pValue").astype(DoubleType()))
        gp_df = gp_df.withColumn("effectSize", col("effectSize").astype(DoubleType()))
        gp_df.repartition(100).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcLacZExpressionMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGeneStatsResultsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            ExperimentToObservationMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_expression_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        observations_parquet_path = args[0]
        output_path = args[1]

        observations_df = spark.read.parquet(observations_parquet_path)

        adult_lacz_expression_data = get_lacz_expression_data(observations_df, "adult")
        embryo_lacz_expression_data = get_lacz_expression_data(
            observations_df, "embryo"
        )

        lacz_expression_data = adult_lacz_expression_data.union(
            embryo_lacz_expression_data
        )
        lacz_expression_data = lacz_expression_data.withColumn(
            "id", col("gene_accession_id")
        )
        for col_name in lacz_expression_data.columns:
            lacz_expression_data = lacz_expression_data.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )
        lacz_expression_data = lacz_expression_data.withColumnRenamed(
            "geneAccessionId", "mgiGeneAccessionId"
        )
        lacz_expression_data.repartition(100).write.option(
            "ignoreNullFields", "false"
        ).json(output_path)


class ImpcPublicationsMapper(SmallPySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcPublicationsMapper"

    #: Path to the CSV gene publications association report
    gene_publications_json_path = luigi.Parameter()

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/publications_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        output_path = args[0]

        publications_df = spark.read.format("mongodb").load()

        publication_service_df = publications_df.where(
            col("status") == "reviewed"
        ).select(
            "title",
            "authorString",
            "consortiumPaper",
            "doi",
            col("firstPublicationDate").alias("publicationDate"),
            col("journalInfo.journal.title").alias("journalTitle"),
            col("alleles.acc").alias("mgiAlleleAccessionId"),
            col("alleles.gacc").alias("mgiGeneAccessionId"),
            col("alleles.geneSymbol"),
            col("alleles.alleleSymbol"),
            col("pmid").alias("pmId"),
            "abstractText",
            "meshHeadingList",
            "grantsList",
        )

        def zip_func(*args):
            return list(zip(*args))

        zip_udf = udf(
            zip_func,
            ArrayType(
                StructType(
                    [
                        StructField("mgiAlleleAccessionId", StringType(), True),
                        StructField("mgiGeneAccessionId", StringType(), True),
                        StructField("geneSymbol", StringType(), True),
                        StructField("alleleSymbol", StringType(), True),
                    ]
                )
            ),
        )

        publication_service_df = publication_service_df.withColumn(
            "alleles",
            zip_udf(
                col("mgiAlleleAccessionId"),
                col("mgiGeneAccessionId"),
                col("geneSymbol"),
                col("alleleSymbol"),
            ),
        )

        publication_service_df = publication_service_df.drop(
            "mgiAlleleAccessionId",
            "mgiGeneAccessionId",
            "geneSymbol",
            "alleleSymbol",
        )

        publication_service_df.repartition(1).write.json(output_path)

        # Incremental count by year
        incremental_counts_by_year = (
            publications_df.where(col("status") == "reviewed")
            .select("pmid", "pubYear")
            .withColumn(
                "count",
                count("pmid")
                .over(Window.partitionBy("pubYear").orderBy("pubYear"))
                .alias("count"),
            )
            .select(col("pubYear").cast("int"), col("count").cast("int"))
            .distinct()
            .sort("pubYear")
            .rdd.map(lambda row: row.asDict())
            .collect()
        )
        aggregations_path = output_path.replace(
            "publications", "publications_aggregation"
        )
        if not os.path.exists(aggregations_path):
            os.makedirs(aggregations_path)
        with open(
            output_path.replace("publications", "publications_aggregation")
            + "/incremental_counts_by_year.json",
            "w",
        ) as f:
            json.dump(incremental_counts_by_year, f, ensure_ascii=False)

        # Count by year and quarter
        publications_by_quarter = [
            json.loads(s)
            for s in publications_df.where(col("status") == "reviewed")
            .select("pmid", "pubYear", quarter("firstPublicationDate").alias("quarter"))
            .withColumn(
                "countQuarter",
                count("pmid").over(
                    Window.partitionBy("pubYear", "quarter").orderBy(
                        "pubYear", col("quarter").asc()
                    )
                ),
            )
            .withColumn(
                "countYear",
                count("pmid").over(Window.partitionBy("pubYear").orderBy("pubYear")),
            )
            .select(
                col("pubYear").cast("int"),
                "quarter",
                col("countYear"),
                col("countQuarter"),
            )
            .distinct()
            .sort("pubYear", "quarter")
            .groupBy("pubYear", col("countYear").alias("count"))
            .agg(
                collect_set(
                    struct("quarter", col("countQuarter").alias("count"))
                ).alias("byQuarter")
            )
            .sort("pubYear")
            .toJSON()
            .collect()
        ]

        with open(
            output_path.replace("publications", "publications_aggregation")
            + "/publications_by_quarter.json",
            "w",
        ) as f:
            json.dump(publications_by_quarter, f, ensure_ascii=False)

        # Count by Grant Agency
        publications_by_grant_agency = (
            publications_df.where(col("status") == "reviewed")
            .select("pmid", explode("grantsList").alias("grantInfo"))
            .select("pmid", "grantInfo.agency")
            .groupBy("agency")
            .agg(countDistinct("pmid").alias("count"))
            .sort(col("count").desc())
            .rdd.map(lambda row: row.asDict())
            .collect()
        )

        with open(
            output_path.replace("publications", "publications_aggregation")
            + "/publications_by_grant_agency.json",
            "w",
        ) as f:
            json.dump(publications_by_grant_agency, f, ensure_ascii=False)


class ImpcProductsMapper(SmallPySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcProductsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [ProductReportExtractor()]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_order_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        products_parquet_path = args[0]
        output_path = args[1]

        products_df = spark.read.parquet(products_parquet_path)
        products_df = products_df.select(
            "mgi_accession_id",
            "marker_symbol",
            "allele_name",
            "allele_description",
            "type",
        )
        products_df = products_df.withColumn(
            "allele_symbol",
            concat(col("marker_symbol"), lit("<"), col("allele_name"), lit(">")),
        )
        products_df = products_df.groupBy(
            "mgi_accession_id", "allele_symbol", "allele_description"
        ).agg(collect_set("type").alias("product_types"))

        products_df = products_df.withColumnRenamed(
            "mgi_accession_id", "mgiGeneAccessionId"
        )

        for col_name in products_df.columns:
            products_df = products_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        products_df.repartition(100).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcGeneImagesMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGeneImagesMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [ImpcImagesLoader()]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_images_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        impc_images_parquet_path = args[0]
        output_path = args[1]

        impc_images_df = spark.read.parquet(impc_images_parquet_path)

        impc_images_df = impc_images_df.groupBy(
            "gene_accession_id",
            "strain_accession_id",
            "procedure_stable_id",
            "procedure_name",
            "parameter_stable_id",
            "parameter_name",
        ).agg(
            count("observation_id").alias("count"),
            first("thumbnail_url").alias("thumbnail_url"),
            first("file_type").alias("file_type"),
        )

        impc_images_df = impc_images_df.withColumnRenamed(
            "gene_accession_id", "mgiGeneAccessionId"
        )

        for col_name in impc_images_df.columns:
            impc_images_df = impc_images_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        impc_images_df.repartition(500).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcImagesMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcImagesMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [ImpcImagesLoader()]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/images_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        impc_images_parquet_path = args[0]
        output_path = args[1]

        impc_images_df = spark.read.parquet(impc_images_parquet_path)

        impc_images_df = impc_images_df.withColumnRenamed(
            "parameter_association_stable_id", "stableId"
        )
        impc_images_df = impc_images_df.withColumnRenamed(
            "parameter_association_sequence_id", "associationSequenceId"
        )
        impc_images_df = impc_images_df.withColumnRenamed(
            "parameter_association_name", "name"
        )
        impc_images_df = impc_images_df.withColumnRenamed(
            "parameter_association_value", "value"
        )
        impc_images_df = impc_images_df.withColumnRenamed(
            "life_stage_name", "lifeStageName"
        )

        impc_images_df = impc_images_df.withColumnRenamed(
            "gene_accession_id", "mgiGeneAccessionId"
        )

        for col_name in impc_images_df.columns:
            impc_images_df = impc_images_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        impc_images_df = impc_images_df.withColumn(
            "associatedParameters",
            arrays_zip("stableId", "associationSequenceId", "name", "value"),
        ).drop("id", "associationSequenceId", "name", "value")

        impc_images_experimental_df = (
            impc_images_df.where(col("biologicalSampleGroup") == "experimental")
            .groupBy(
                "mgiGeneAccessionId",
                "geneSymbol",
                "strainAccessionId",
                "pipelineStableId",
                "procedureStableId",
                "procedureName",
                "parameterStableId",
                "parameterName",
                "biologicalSampleGroup",
            )
            .agg(
                collect_set(
                    struct(
                        "thumbnailUrl",
                        "downloadUrl",
                        "jpegUrl",
                        "fileType",
                        "observationId",
                        "specimenId",
                        "colonyId",
                        "sex",
                        "zygosity",
                        "ageInWeeks",
                        "alleleSymbol",
                        "associatedParameters",
                    )
                ).alias("images")
            )
        )

        window_controls = Window.partitionBy(
            "strainAccessionId",
            "procedureStableId",
            "procedureName",
            "parameterStableId",
            "parameterName",
        ).orderBy(col("observationId"))

        impc_images_control_df = impc_images_df.where(
            col("biologicalSampleGroup") == "control"
        )

        impc_images_control_df = impc_images_control_df.withColumn(
            "row", row_number().over(window_controls)
        )

        impc_images_control_df = impc_images_control_df.where(col("row") <= 50).drop(
            "row"
        )

        impc_images_control_df = impc_images_control_df.groupBy(
            "mgiGeneAccessionId",
            "geneSymbol",
            "strainAccessionId",
            "pipelineStableId",
            "procedureStableId",
            "procedureName",
            "parameterStableId",
            "parameterName",
            "biologicalSampleGroup",
        ).agg(
            collect_set(
                struct(
                    "thumbnailUrl",
                    "downloadUrl",
                    "jpegUrl",
                    "fileType",
                    "observationId",
                    "specimenId",
                    "colonyId",
                    "sex",
                    "zygosity",
                    "ageInWeeks",
                    "alleleSymbol",
                    "associatedParameters",
                )
            ).alias("images")
        )
        impc_images_df = impc_images_experimental_df.union(impc_images_control_df)

        impc_images_df.repartition(500).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcGeneDiseasesMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGeneDiseasesMapper"

    #: Path to the CSV gene disease association report
    disease_model_summary_csv_path = luigi.Parameter()

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_diseases_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.disease_model_summary_csv_path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        disease_model_summary_csv_path = args[0]
        output_path = args[1]

        disease_df = spark.read.csv(disease_model_summary_csv_path, header=True)

        disease_df = disease_df.withColumn(
            "phenodigm_score",
            (col("disease_model_avg_norm") + col("disease_model_max_norm")) / 2,
        )

        max_values = disease_df.groupBy("disease_id", "marker_id").agg(
            max("phenodigm_score").alias("phenodigm_score")
        )
        max_disease_df = disease_df.join(
            max_values, ["disease_id", "marker_id", "phenodigm_score"]
        )

        max_disease_df = max_disease_df.withColumnRenamed("marker_id", "id")

        for col_name in max_disease_df.columns:
            max_disease_df = max_disease_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        double_cols = [
            "diseaseModelAvgNorm",
            "diseaseModelAvgRaw",
            "diseaseModelMaxRaw",
            "diseaseModelMaxNorm",
        ]

        for col_name in double_cols:
            max_disease_df = max_disease_df.withColumn(
                col_name, col(col_name).astype(DoubleType())
            )

        max_disease_df = max_disease_df.withColumn(
            "markerNumModels", col("markerNumModels").astype(IntegerType())
        )
        max_disease_df = max_disease_df.withColumn(
            "associationCurated", col("associationCurated").astype(BooleanType())
        )

        max_disease_df.repartition(500).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcGeneHistopathologyMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcGeneHistopathologyMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            GenotypePhenotypeLoader(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/gene_histopath_json)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/gene_histopathology_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        gp_parquet_path = args[0]
        output_path = args[1]

        gp_df = spark.read.parquet(gp_parquet_path)
        explode_cols = [
            "procedure_stable_id",
            "procedure_name",
            "project_name",
            "life_stage_name",
        ]

        for col_name in explode_cols:
            gp_df = gp_df.withColumn(col_name, explode(col_name))

        gp_df = gp_df.where(col("parameter_stable_id").contains("_HIS_"))

        gp_df = gp_df.select(
            "marker_accession_id",
            "pipeline_stable_id",
            "procedure_stable_id",
            "procedure_name",
            "parameter_stable_id",
            "parameter_name",
            "allele_accession_id",
            "allele_name",
            "allele_symbol",
            "zygosity",
            "phenotyping_center",
            "sex",
            "project_name",
            "life_stage_name",
            "mpath_term_id",
            "mpath_term_name",
        )

        for col_name in gp_df.columns:
            gp_df = gp_df.withColumnRenamed(col_name, to_camel_case(col_name))
        gp_df = gp_df.withColumnRenamed("markerAccessionId", "mgiGeneAccessionId")

        gp_df.repartition(500).write.option("ignoreNullFields", "false").json(
            output_path
        )


class ImpcDatasetsMetadataMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcDatasetsMetadataMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            ImpressToParameterMapper(),
            StatsResultsMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/datasets_metadata_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.input()[1].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        impress_parameter_parquet_path = args[0]
        stats_results_parquet_path = args[1]
        output_path = args[2]

        stats_results_df = spark.read.parquet(stats_results_parquet_path)
        explode_cols = [
            "procedure_stable_key",
            "procedure_stable_id",
            "procedure_name",
            "parameter_stable_key",
            "project_name",
            "life_stage_name",
            "life_stage_acc",
        ]
        impress_parameter_df = spark.read.parquet(impress_parameter_parquet_path)
        unit_df = impress_parameter_df.select(
            "fully_qualified_name", col("unit_x").alias("x"), col("unit_y").alias("y")
        )
        unit_df = unit_df.withColumn("unit", struct("x", "y"))
        unit_df = unit_df.drop("x", "y")
        stats_results_df = stats_results_df.withColumn(
            "fully_qualified_name",
            concat_ws(
                "_", "pipeline_stable_id", "procedure_stable_id", "parameter_stable_id"
            ),
        )
        stats_results_df = stats_results_df.join(
            unit_df, "fully_qualified_name", "left_outer"
        ).drop("fully_qualified_name")
        col_name_map = {
            "doc_id": "datasetId",
            "marker_accession_id": "mgiGeneAccessionId",
            "marker_symbol": "geneSymbol",
            "metadata": "metadataValues",
            "production_center": "productionCentre",
            "phenotyping_center": "phenotypingCentre",
            "resource_fullname": "resourceFullName",
            "statistical_method": "name",
            "soft_windowing_bandwidth": "bandwidth",
            "soft_windowing_shape": "shape",
            "soft_windowing_peaks": "peaks",
            "soft_windowing_min_obs_required": "minObsRequired",
            "soft_windowing_total_obs_or_weight": "totalObsOrWeight",
            "soft_windowing_threshold": "threshold",
            "soft_windowing_number_of_doe": "numberOfDoe",
            "soft_windowing_doe_note": "doeNote",
            "effect_size": "reportedEffectSize",
            "p_value": "reportedPValue",
        }

        new_structs_dict = {
            "summaryStatistics": [
                "female_control_count",
                "female_control_mean",
                "female_control_sd",
                "male_control_count",
                "male_control_mean",
                "male_control_sd",
                "female_mutant_count",
                "female_mutant_mean",
                "female_mutant_sd",
                "male_mutant_count",
                "male_mutant_mean",
                "male_mutant_sd",
                "both_mutant_count",
                "both_mutant_mean",
                "both_mutant_sd",
            ],
            "statisticalMethod": [
                "statistical_method",
                {
                    "attributes": [
                        "female_ko_effect_p_value",
                        "female_ko_effect_stderr_estimate",
                        "female_ko_parameter_estimate",
                        "female_percentage_change",
                        "male_ko_effect_p_value",
                        "male_ko_effect_stderr_estimate",
                        "male_ko_parameter_estimate",
                        "male_percentage_change",
                        "genotype_effect_p_value",
                        "genotype_effect_stderr_estimate",
                        "group_1_genotype",
                        "group_1_residuals_normality_test",
                        "group_2_genotype",
                        "group_2_residuals_normality_test",
                        "interaction_effect_p_value",
                        "interaction_significant",
                        "intercept_estimate",
                        "intercept_estimate_stderr_estimate",
                        "sex_effect_p_value",
                        "sex_effect_parameter_estimate",
                        "sex_effect_stderr_estimate",
                        "male_effect_size",
                        "female_effect_size",
                        "batch_significant",
                        "variance_significant",
                    ]
                },
            ],
            "softWindowing": [
                "soft_windowing_bandwidth",
                "soft_windowing_shape",
                "soft_windowing_peaks",
                "soft_windowing_min_obs_required",
                "soft_windowing_total_obs_or_weight",
                "soft_windowing_threshold",
                "soft_windowing_number_of_doe",
                "soft_windowing_doe_note",
            ],
        }

        int_columns = [
            "female_control_count",
            "male_control_count",
            "female_mutant_count",
            "male_mutant_count",
            "both_mutant_count",
        ]
        double_columns = [
            "p_value",
            "effect_size",
            "female_ko_effect_p_value",
            "female_ko_parameter_estimate",
            "female_percentage_change",
            "genotype_effect_p_value",
            "male_ko_effect_p_value",
            "male_ko_parameter_estimate",
            "male_percentage_change",
        ]

        for col_name in int_columns:
            stats_results_df = stats_results_df.withColumn(
                col_name, col(col_name).astype(IntegerType())
            )
        for col_name in double_columns:
            stats_results_df = stats_results_df.withColumn(
                col_name, col(col_name).astype(DoubleType())
            )

        for col_name in explode_cols:
            stats_results_df = stats_results_df.withColumn(col_name, explode(col_name))
        stats_results_df = stats_results_df.select(
            "doc_id",  # statisticalResultId
            "strain_accession_id",
            "strain_name",
            "genetic_background",
            "colony_id",
            "marker_accession_id",  # mgiGeneAccessionId
            "marker_symbol",  # geneSymbol
            "allele_accession_id",
            "allele_name",
            "allele_symbol",
            "metadata_group",
            "metadata",  # metadataValues
            "life_stage_acc",  # explode
            "life_stage_name",  # explode
            "data_type",
            "production_center",  # productionCentre
            "phenotyping_center",  # phenotypingCentre
            "project_name",
            "resource_name",
            "resource_fullname",  # resourceFullName
            "pipeline_stable_key",
            "pipeline_stable_id",
            "pipeline_name",
            "procedure_stable_key",  # explode
            "procedure_stable_id",  # explode
            "procedure_name",  # explode
            "procedure_group",
            "parameter_stable_key",  # explode
            "parameter_stable_id",
            "parameter_name",
            "female_control_count",  # group under summaryStatistics
            "female_control_mean",  # group under summaryStatistics
            "female_control_sd",  # group under summaryStatistics
            "male_control_count",  # group under summaryStatistics
            "male_control_mean",  # group under summaryStatistics
            "male_control_sd",  # group under summaryStatistics
            "female_mutant_count",  # group under summaryStatistics
            "female_mutant_mean",  # group under summaryStatistics
            "female_mutant_sd",  # group under summaryStatistics
            "male_mutant_count",  # group under summaryStatistics
            "male_mutant_mean",  # group under summaryStatistics
            "male_mutant_sd",  # group under summaryStatistics
            "both_mutant_count",  # group under summaryStatistics
            "both_mutant_mean",  # group under summaryStatistics
            "both_mutant_sd",  # group under summaryStatistics
            "status",
            "sex",  # phenotypeSex
            "zygosity",
            "phenotype_sex",  # testedSexes
            "significant",
            "classification_tag",
            "p_value",  # reportedPValue
            "effect_size",  # reportedEffectSize
            "statistical_method",  # name, group under statisticalMethod
            "female_ko_effect_p_value",  # group under statisticalMethod.attributes
            "female_ko_effect_stderr_estimate",  # group under statisticalMethod.attributes
            "female_ko_parameter_estimate",  # group under statisticalMethod.attributes
            "female_percentage_change",  # group under statisticalMethod.attributes
            "male_ko_effect_p_value",  # group under statisticalMethod.attributes
            "male_ko_effect_stderr_estimate",  # group under statisticalMethod.attributes
            "male_ko_parameter_estimate",  # group under statisticalMethod.attributes
            "male_percentage_change",  # group under statisticalMethod.attributes
            "genotype_effect_p_value",  # group under statisticalMethod.attributes
            "genotype_effect_stderr_estimate",  # group under statisticalMethod.attributes
            "group_1_genotype",  # group under statisticalMethod.attributes
            "group_1_residuals_normality_test",  # group under statisticalMethod.attributes
            "group_2_genotype",  # group under statisticalMethod.attributes
            "group_2_residuals_normality_test",  # group under statisticalMethod.attributes
            "interaction_effect_p_value",  # group under statisticalMethod.attributes
            "interaction_significant",  # group under statisticalMethod.attributes
            "intercept_estimate",  # group under statisticalMethod.attributes
            "intercept_estimate_stderr_estimate",  # group under statisticalMethod.attributes
            "sex_effect_p_value",  # group under statisticalMethod.attributes
            "sex_effect_parameter_estimate",  # group under statisticalMethod.attributes
            "sex_effect_stderr_estimate",  # group under statisticalMethod.attributes
            "male_effect_size",  # group under statisticalMethod.attributes
            "female_effect_size",  # group under statisticalMethod.attributes
            "batch_significant",  # group under statisticalMethod.attributes
            "variance_significant",  # group under statisticalMethod.attributes
            "soft_windowing_bandwidth",  # bandwidth, group under softWindowing
            "soft_windowing_shape",  # shape, group under softWindowing
            "soft_windowing_peaks",  # peaks, group under softWindowing
            "soft_windowing_min_obs_required",  # minObsRequired, group under softWindowing
            "soft_windowing_total_obs_or_weight",  # totalObsOrWeight, group under softWindowing
            "soft_windowing_threshold",  # threshold, group under softWindowing
            "soft_windowing_number_of_doe",  # numberOfDoe, group under softWindowing
            "soft_windowing_doe_note",  # doeNote, group under softWindowing
            "mp_term_id",
            "mp_term_name",
            "intermediate_mp_term_id",
            "intermediate_mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
            "mp_term_id_options",
            "mp_term_name_options",
            "unit",
        )

        stats_results_df = stats_results_df.withColumn(
            "soft_windowing_peaks",
            split(regexp_replace("soft_windowing_peaks", "\[|\]", ""), ",").cast(
                "array<int>"
            ),
        )

        stats_results_df = stats_results_df.withColumn(
            "significantPhenotype",
            when(
                col("mp_term_id").isNotNull(),
                struct(
                    col("mp_term_id").alias("id"), col("mp_term_name").alias("name")
                ),
            ).otherwise(lit(None)),
        )

        stats_results_df = stats_results_df.withColumn(
            "intermediatePhenotypes",
            zip_with(
                "intermediate_mp_term_id",
                "intermediate_mp_term_name",
                phenotype_term_zip_udf,
            ),
        )

        stats_results_df = stats_results_df.withColumn(
            "topLevelPhenotypes",
            zip_with(
                "top_level_mp_term_id",
                "top_level_mp_term_name",
                phenotype_term_zip_udf,
            ),
        )

        stats_results_df = stats_results_df.withColumn(
            "potentialPhenotypes",
            zip_with(
                "mp_term_id_options",
                "mp_term_name_options",
                phenotype_term_zip_udf,
            ),
        )

        stats_results_df = stats_results_df.drop(
            "mp_term_id",
            "mp_term_name",
            "top_level_mp_term_id",
            "top_level_mp_term_name",
            "intermediate_mp_term_id",
            "intermediate_mp_term_name",
            "mp_term_id_options",
            "mp_term_name_options",
        )

        stats_results_df = stats_results_df.withColumnRenamed(
            "marker_accession_id", "mgiGeneAccessionId"
        )
        stats_results_df = stats_results_df.withColumnRenamed("doc_id", "datasetId")
        stats_results_df = stats_results_df.withColumn("id", col("datasetId"))

        drop_list = []
        for struct_col_name in new_structs_dict.keys():
            columns = new_structs_dict[struct_col_name]
            fields = []
            for column in columns:
                if type(column) == str:
                    fields.append(
                        col(column).alias(col_name_map[column])
                        if column in col_name_map
                        else col(column).alias(to_camel_case(column))
                    )
                    drop_list.append(column)
                else:
                    for sub_field_name in column.keys():
                        fields.append(
                            struct(
                                *[
                                    col(sub_column).alias(col_name_map[sub_column])
                                    if sub_column in col_name_map
                                    else col(sub_column).alias(
                                        to_camel_case(sub_column)
                                    )
                                    for sub_column in column[sub_field_name]
                                ]
                            ).alias(sub_field_name)
                        )
                        for sub_column in column[sub_field_name]:
                            drop_list.append(sub_column)

            stats_results_df = stats_results_df.withColumn(
                struct_col_name,
                struct(*fields),
            )

        stats_results_df = stats_results_df.drop(*drop_list)

        for col_name in stats_results_df.columns:
            stats_results_df = stats_results_df.withColumnRenamed(
                col_name,
                col_name_map[col_name]
                if col_name in col_name_map
                else to_camel_case(col_name),
            )
        stats_results_df.repartition(1000).write.option(
            "ignoreNullFields", "false"
        ).json(output_path)


class ImpcDatasetsMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcDatasetsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            StatsResultsMapper(raw_data_in_output="bundled"),
            ExperimentToObservationMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/datasets_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.input()[1].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        dataset_observation_index_parquet_path = args[0] + "_raw_data_ids"
        observations_parquet_path = args[1]
        output_path = args[2]

        dataset_observation_index_df = spark.read.parquet(
            dataset_observation_index_parquet_path
        )
        line_level_procedures = [
            "IMPC_VIA_002",
            "IMPC_VIA_001",
            "IMPC_EVM_001",
            "IMPC_FER_001",
            "IMPC_EVL_001",
            "IMPC_EVP_001",
            "IMPC_EVO_001",
        ]
        observations_df = spark.read.parquet(observations_parquet_path)
        line_observations_df = observations_df.where(
            col("procedure_stable_id").isin(line_level_procedures)
        )
        dataset_line_observation_index_df = dataset_observation_index_df.select(
            "doc_id", "observation_id"
        ).distinct()
        observations_df = observations_df.where(
            ~col("procedure_stable_id").isin(line_level_procedures)
        )
        dataset_observation_index_df = dataset_observation_index_df.withColumn(
            "window_weight",
            when(col("window_weight").isNotNull(), col("window_weight")).otherwise(
                expr("transform(observation_id, id -> NULL)")
            ),
        )
        dataset_observation_index_df = dataset_observation_index_df.withColumn(
            "obs_id_ww",
            arrays_zip(
                "observation_id",
                "window_weight",
            ),
        )
        dataset_observation_index_df = dataset_observation_index_df.drop(
            "observation_id", "window_weight"
        )
        dataset_observation_index_df = dataset_observation_index_df.withColumn(
            "obs_id_ww", explode("obs_id_ww")
        )
        dataset_observation_index_df = dataset_observation_index_df.select(
            "doc_id", "obs_id_ww.*"
        )
        observations_df = observations_df.select(
            "observation_id",
            "biological_sample_group",
            "date_of_experiment",
            "external_sample_id",
            "sex",
            "weight",
            "data_point",
            "category",
            "time_point",
            "discrete_point",
            "date_of_birth",
        ).distinct()

        datasets_df = dataset_observation_index_df.join(
            observations_df, "observation_id"
        )
        datasets_df = datasets_df.drop("observation_id")
        datasets_col_map = {
            "doc_id": "datasetId",
            "biological_sample_group": "sampleGroup",
            "sex": "specimenSex",
            "date_of_birth": "specimenDateOfBirth",
            "date_of_experiment": "dateOfExperiment",
            "external_sample_id": "specimenId",
            "weight": "bodyWeight",
            "window_weight": "windowWeight",
            "category": "category",
            "data_point": "dataPoint",
            "time_point": "timePoint",
            "discrete_point": "discretePoint",
        }
        double_type_cols = ["data_point", "discrete_point", "weight", "window_weight"]
        for double_col in double_type_cols:
            datasets_df = datasets_df.withColumn(
                double_col, col(double_col).astype(DoubleType())
            )
        for column_name, new_column_name in datasets_col_map.items():
            datasets_df = datasets_df.withColumnRenamed(column_name, new_column_name)

        datasets_df = datasets_df.groupBy(
            "datasetId", "sampleGroup", "specimenSex"
        ).agg(
            collect_set(
                struct(
                    "specimenDateOfBirth",
                    "dateOfExperiment",
                    "specimenId",
                    "bodyWeight",
                    "windowWeight",
                    "category",
                    "dataPoint",
                    "timePoint",
                    "discretePoint",
                )
            ).alias("observations")
        )

        datasets_df = datasets_df.groupBy("datasetId").agg(
            collect_set(struct("sampleGroup", "specimenSex", "observations")).alias(
                "series"
            )
        )
        datasets_df.repartition(10000).write.parquet(output_path)

        dataset_line_observation_index_df = (
            dataset_line_observation_index_df.withColumn(
                "observation_id", explode("observation_id")
            )
        )

        line_datasets_df = dataset_line_observation_index_df.join(
            line_observations_df, "observation_id"
        )
        line_datasets_df = line_datasets_df.drop("observation_id")
        line_datasets_col_map = {
            "doc_id": "datasetId",
            "parameter_stable_id": "parameterStableId",
            "parameter_name": "parameterName",
            "data_point": "dataPoint",
        }

        for column_name, new_column_name in line_datasets_col_map.items():
            line_datasets_df = line_datasets_df.withColumnRenamed(
                column_name, new_column_name
            )
        line_datasets_df = line_datasets_df.withColumn(
            "dataPoint", col("dataPoint").astype(DoubleType())
        )
        line_datasets_df = line_datasets_df.groupBy("datasetId").agg(
            collect_set(
                struct(
                    "parameterStableId",
                    "parameterName",
                    "dataPoint",
                )
            ).alias("counts")
        )

        line_datasets_df.write.parquet(output_path + "_line")


class ImpcPhenotypeStatisticalResultsMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcPhenotypeStatisticalResultsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [StatsResultsMapper(), GeneLoader()]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/phenotype_stats_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.input()[1].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        stats_results_parquet_path = args[0]
        gene_parquet_path = args[1]
        output_path = args[2]

        stats_results_df = spark.read.parquet(stats_results_parquet_path)
        gene_df = spark.read.parquet(gene_parquet_path)

        gene_df = gene_df.select(
            col("mgi_accession_id").alias("marker_accession_id"),
            "seq_region_start",
            "seq_region_end",
            "seq_region_id",
            "chr_name",
            "chr_strand",
            "marker_symbol",
            "marker_name",
        ).distinct()

        phenotype_stats_df = stats_results_df.select(
            "doc_id",
            "marker_accession_id",
            "p_value",
            "effect_size",
            "significant",
            "mp_term_id",
            "mp_term_id_options",
            "intermediate_mp_term_id",
            "top_level_mp_term_id",
            "resource_fullname",
            "mp_term_name",
            "intermediate_mp_term_name",
            "top_level_mp_term_name",
            "mp_term_name_options",
        )

        phenotype_stats_df = phenotype_stats_df.withColumn(
            "significantPhenotype",
            when(
                col("mp_term_id").isNotNull(),
                struct(
                    col("mp_term_id").alias("id"), col("mp_term_name").alias("name")
                ),
            ).otherwise(lit(None)),
        )

        phenotype_stats_df = phenotype_stats_df.withColumn(
            "intermediatePhenotypes",
            zip_with(
                "intermediate_mp_term_id",
                "intermediate_mp_term_name",
                phenotype_term_zip_udf,
            ),
        )

        phenotype_stats_df = phenotype_stats_df.withColumn(
            "topLevelPhenotypes",
            zip_with(
                "top_level_mp_term_id",
                "top_level_mp_term_name",
                phenotype_term_zip_udf,
            ),
        )

        phenotype_stats_df = phenotype_stats_df.withColumn(
            "potentialPhenotypes",
            zip_with(
                "mp_term_id_options",
                "mp_term_name_options",
                phenotype_term_zip_udf,
            ),
        )
        phenotype_stats_df = phenotype_stats_df.drop(
            "top_level_mp_term_id",
            "top_level_mp_term_name",
            "intermediate_mp_term_id",
            "intermediate_mp_term_name",
            "mp_term_id_options",
            "mp_term_name_options",
            "mp_term_id",
            "mp_term_name",
        )

        phenotype_stats_df = phenotype_stats_df.join(gene_df, "marker_accession_id")

        phenotype_stats_map = {
            "marker_accession_id": "mgiGeneAccessionId",
            "effect_size": "reportedEffectSize",
            "p_value": "reportedPValue",
            "resource_fullname": "resourceFullName",
            "doc_id": "datasetId",
        }

        for col_name in phenotype_stats_map.keys():
            phenotype_stats_df = phenotype_stats_df.withColumnRenamed(
                col_name, phenotype_stats_map[col_name]
            )

        for col_name in phenotype_stats_df.columns:
            phenotype_stats_df = phenotype_stats_df.withColumnRenamed(
                col_name, to_camel_case(col_name)
            )

        double_cols = [
            "reportedEffectSize",
            "reportedPValue",
        ]

        for col_name in double_cols:
            phenotype_stats_df = phenotype_stats_df.withColumn(
                col_name, col(col_name).astype(DoubleType())
            )

        int_cols = [
            "seqRegionStart",
            "seqRegionEnd",
        ]

        for col_name in int_cols:
            phenotype_stats_df = phenotype_stats_df.withColumn(
                col_name, col(col_name).astype(IntegerType())
            )
        phenotype_stats_df = (
            phenotype_stats_df.withColumn(
                "potentialPhenotypes",
                when(
                    col("significantPhenotype").isNotNull(),
                    array("significantPhenotype"),
                ).otherwise(col("potentialPhenotypes")),
            )
            .withColumn(
                "phenotypes",
                concat(
                    "potentialPhenotypes",
                    "intermediatePhenotypes",
                    "topLevelPhenotypes",
                ),
            )
            .drop(
                "potentialPhenotypes",
                "intermediatePhenotypes",
                "topLevelPhenotypes",
                "significantPhenotype",
            )
            .withColumn("phenotypeId", explode("phenotypes.id"))
            .drop("phenotypes")
            .groupBy("phenotypeId")
            .agg(
                collect_set(
                    struct(
                        "mgiGeneAccessionId",
                        "markerSymbol",
                        "markerName",
                        "reportedPValue",
                        "reportedEffectSize",
                        "chrName",
                        "chrStrand",
                        "seqRegionId",
                        "seqRegionStart",
                        "seqRegionEnd",
                        "significant",
                        "resourceFullName",
                    )
                ).alias("results")
            )
            .drop("markerName")
        )
        phenotype_stats_df.repartition(1000).write.option(
            "ignoreNullFields", "false"
        ).json(output_path)


class ImpcBWTDatasetsMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcBWTDatasetsMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            StatsResultsMapper(raw_data_in_output="bundled"),
            ExperimentToObservationMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/bwt_curve_service_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.input()[1].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        stats_results_parquet_path = args[0]
        raw_data_parquet_path = args[0] + "_raw_data_ids"
        observations_parquet_path = args[1]
        output_path = args[2]

        stats_results_df = spark.read.parquet(stats_results_parquet_path)
        raw_data_df = spark.read.parquet(raw_data_parquet_path)
        observations_df = spark.read.parquet(observations_parquet_path)
        stats_results_df.select(
            "doc_id",
            "procedure_stable_id",
            "parameter_stable_id",
            "marker_accession_id",
        ).withColumn("procedure_stable_id", explode("procedure_stable_id")).where(
            col("parameter_stable_id") == "IMPC_BWT_008_001"
        ).join(
            raw_data_df, "doc_id", "left_outer"
        ).withColumn(
            "observation_id", explode("observation_id")
        ).drop(
            "window_weight"
        ).join(
            observations_df.drop("procedure_stable_id", "parameter_stable_id"),
            "observation_id",
            "left_outer",
        ).select(
            "doc_id",
            "procedure_stable_id",
            "parameter_stable_id",
            "marker_accession_id",
            "biological_sample_group",
            "sex",
            "zygosity",
            "discrete_point",
            "data_point",
        ).groupBy(
            "doc_id",
            "procedure_stable_id",
            "parameter_stable_id",
            "marker_accession_id",
            "biological_sample_group",
            "sex",
            "zygosity",
            "discrete_point",
        ).agg(
            avg("data_point").alias("mean"),
            stddev("data_point").alias("std"),
            count("data_point").alias("count"),
        ).groupBy(
            col("doc_id").alias("datasetId"),
            col("procedure_stable_id").alias("procedureStableId"),
            col("parameter_stable_id").alias("parameterStableId"),
            col("marker_accession_id").alias("mgiGeneAccessionId"),
        ).agg(
            collect_set(
                struct(
                    col("biological_sample_group").alias("sampleGroup"),
                    "sex",
                    "zygosity",
                    col("discrete_point").alias("ageInWeeks"),
                    "mean",
                    "std",
                    "count",
                )
            ).alias("dataPoints")
        ).write.json(
            output_path
        )


class ImpcExternalLinksMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcExternalLinksMapper"

    mouse_human_ortholog_report_tsv_path: luigi.Parameter = luigi.Parameter()

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [GeneLoader()]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/impc_web_api/external_links_json"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.mouse_human_ortholog_report_tsv_path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        gene_parquet_path = args[0]
        mouse_human_ortholog_report_tsv_path = args[1]
        output_path = args[2]

        gene_df = spark.read.parquet(gene_parquet_path)
        mouse_human_ortholog_report_df = spark.read.csv(
            mouse_human_ortholog_report_tsv_path, sep="\t", header=True
        )

        for col_name in mouse_human_ortholog_report_df.columns:
            mouse_human_ortholog_report_df = (
                mouse_human_ortholog_report_df.withColumnRenamed(
                    col_name, col_name.replace(" ", "_").lower()
                )
            )

        mouse_human_ortholog_report_df = mouse_human_ortholog_report_df.select(
            "human_gene_symbol", "mgi_gene_acc_id"
        )

        mouse_human_ortholog_report_df = (
            mouse_human_ortholog_report_df.withColumnRenamed(
                "mgi_gene_acc_id", "mgi_gene_accession_id"
            )
        )

        gene_mgi_accession_df = (
            gene_df.select("mgi_accession_id")
            .withColumnRenamed("mgi_accession_id", "mgi_gene_accession_id")
            .dropDuplicates()
        )

        external_links_df = gene_mgi_accession_df.join(
            mouse_human_ortholog_report_df, "mgi_gene_accession_id"
        )

        external_links_df = external_links_df.withColumnRenamed(
            "mgi_gene_accession_id", "mgiGeneAccessionId"
        )

        external_links_df = external_links_df.withColumnRenamed(
            "human_gene_symbol", "label"
        )

        external_links_df = external_links_df.withColumn(
            "href",
            concat(lit("https://www.ebi.ac.uk/gwas/genes/"), external_links_df.label),
        )
        external_links_df = external_links_df.withColumn("providerName", lit("GWAS"))
        external_links_df.write.json(output_path, mode="overwrite")
