import gzip

import luigi
from luigi.contrib.spark import PySparkTask
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    when,
    expr,
    explode,
    collect_set,
    struct,
    to_json,
    regexp_replace,
    base64,
    arrays_zip,
    udf,
)
from pyspark.sql.types import DoubleType, StringType

from impc_etl.jobs.load import ExperimentToObservationMapper
from impc_etl.jobs.load.solr.stats_results_mapper import StatsResultsMapper
from impc_etl.workflow.config import ImpcConfig


def _compress_and_encode(json_text):
    if json_text is None:
        return None
    else:
        return str(base64.b64encode(gzip.compress(bytes(json_text, "utf-8"))), "utf-8")


class ImpcStatisticalRawDataMapper(PySparkTask):
    """
    PySpark Task class to extract GenTar Product report data.
    """

    #: Name of the Spark task
    name: str = "ImpcStatisticalRawDataMapper"

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        return [
            StatsResultsMapper(raw_data_in_output="bundled"),
            ExperimentToObservationMapper(),
        ]

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/product_report_parquet)
        """
        return ImpcConfig().get_target(
            f"{self.output_path}/statistical_raw_data_parquet"
        )

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.input()[0].path,
            self.input()[1].path,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        spark = SparkSession(sc)

        # Parsing app options
        dataset_observation_index_parquet_path = args[0] + "_raw_data_ids"
        observations_parquet_path = args[1]
        output_path = args[2]

        dataset_observation_index_df = spark.read.parquet(
            dataset_observation_index_parquet_path
        )

        line_level_procedures = [
            "IMPC_VIA_002",
            "IMPC_VIA_001",
            "IMPC_EVM_001",
            "IMPC_FER_001",
            "IMPC_EVL_001",
            "IMPC_EVP_001",
            "IMPC_EVO_001",
        ]
        observations_df = spark.read.parquet(observations_parquet_path)
        observations_df = observations_df.where(
            ~col("procedure_stable_id").isin(line_level_procedures)
        )
        dataset_observation_index_df = dataset_observation_index_df.withColumn(
            "window_weight",
            when(col("window_weight").isNotNull(), col("window_weight")).otherwise(
                expr("transform(observation_id, id -> NULL)")
            ),
        )
        dataset_observation_index_df = dataset_observation_index_df.withColumn(
            "obs_id_ww",
            arrays_zip(
                "observation_id",
                "window_weight",
            ),
        )
        dataset_observation_index_df = dataset_observation_index_df.drop(
            "observation_id", "window_weight"
        )
        dataset_observation_index_df = dataset_observation_index_df.withColumn(
            "obs_id_ww", explode("obs_id_ww")
        )
        dataset_observation_index_df = dataset_observation_index_df.select(
            "doc_id", "obs_id_ww.*"
        )
        observations_df = observations_df.select(
            "observation_id",
            "biological_sample_group",
            "date_of_experiment",
            "external_sample_id",
            "sex",
            "weight",
            "data_point",
            "category",
            "time_point",
            "discrete_point",
            "date_of_birth",
            "sub_term_id",
            "sub_term_name",
        ).distinct()

        datasets_df = dataset_observation_index_df.join(
            observations_df, "observation_id"
        )
        raw_data_cols = [
            "biological_sample_group",
            "date_of_experiment",
            "external_sample_id",
            "specimen_sex",
            "body_weight",
            "data_point",
            "category",
            "time_point",
            "discrete_point",
            "window_weight",
        ]
        datasets_df = datasets_df.withColumn("raw_datum", struct(*raw_data_cols))
        datasets_df = datasets_df.groupBy("doc_id").agg(
            collect_set("raw_datum").alias("raw_data")
        )
        datasets_df = datasets_df.withColumn("raw_data", to_json("raw_data"))
        for idx, col_name in enumerate(raw_data_cols):
            datasets_df = datasets_df.withColumn(
                "raw_data",
                regexp_replace("raw_data", f'"{idx}":', f'"{col_name}":'),
            )
        compress_and_encode = udf(_compress_and_encode, StringType())

        datasets_df = datasets_df.withColumn(
            "raw_data", compress_and_encode("raw_data")
        )
        datasets_df.write.parquet(output_path)
