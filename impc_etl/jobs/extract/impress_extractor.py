"""
IMPRESS extractor task.
Crawls [IMPReSS](https://www.mousephenotype.org/impress/index) API to generate a full view of the current state of [IMPReSS](https://www.mousephenotype.org/impress/index) at a given time.
"""
import json
import os
import time
from typing import List, Dict

import luigi
import requests
from luigi.contrib.spark import PySparkTask
from pyspark import SparkContext
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import udf, explode_outer
from pyspark.sql.types import StructType, ArrayType

from impc_etl import logger
from impc_etl.shared.utils import convert_to_row
from impc_etl.workflow.config import ImpcConfig


class ImpressExtractor(PySparkTask):
    """
    PySpark Task class to extract [IMPReSS](https://www.mousephenotype.org/impress/index) information.
    It goes from the most general endpoint listing resources and down the hierarchy defined by IMPReSS.
    It starts by crawling Schedules, then Pipelines, Procedures, Paramemeters, Units, and Ontology Terms.

    The output is a Parquet file that contains a de-normalized view of the data,
    being each row a combination between Pipeline, Procedure, Parameter, Unit, Ontology Term information.
    """

    #: Name of the Spark task
    name: str = "IMPC_IMPReSS_Crawler"

    #: The URL used to access the IMPRESS API (e.g. https://api.mousephenotype.org/impress/)
    impress_api_url: luigi.Parameter = luigi.Parameter()

    #: The crawler can start from any given level of the IMPReSS hierarchy.
    #: Can be 'pipeline', 'procedure' or 'parameter'.
    impress_root_type: luigi.Parameter = luigi.Parameter(default="pipeline")

    #: URL to the HTTP proxy server to be used on the crawling process.
    http_proxy: luigi.Parameter = luigi.Parameter(default="")

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def output(self):
        """
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/impress_parquet)
        """
        return ImpcConfig().get_target(f"{self.output_path}impress_parquet")

    def app_options(self):
        """
        Generates the options pass to the PySpark job
        """
        return [
            self.impress_api_url,
            self.impress_root_type,
            self.http_proxy,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        """
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        """
        impress_api_url = args[0]
        impress_root_type = args[1]
        http_proxy = args[2]
        output_path = args[3]

        # Set HTTP proxy so the jobs has access to the Internet from EBI infrastructure.
        os.environ["HTTP_PROXY"] = http_proxy
        os.environ["HTTPS_PROXY"] = http_proxy

        spark = SparkSession(sc)
        proxy_map = {"http": http_proxy, "https": http_proxy}
        impress_df = extract_impress(
            spark, impress_api_url, impress_root_type, proxy_map
        )
        ontology_terms = get_ontology_terms(impress_api_url, spark, proxy_map)
        impress_df = impress_df.join(
            ontology_terms,
            impress_df["parammpterm.ontologyTermId"] == ontology_terms.termId,
            "left_outer",
        )
        impress_df.write.mode("overwrite").parquet(output_path)


def extract_impress(
    spark_session: SparkSession, impress_api_url: str, start_type: str, proxy_map: Dict
) -> DataFrame:
    """
    Starts the IMPReSS API crawling, by getting the list of entities corresponding to the stat_type (e.g. pipeline).

    Attributes
    __________
    proxy_map: Dict
            e.g. {"http": "0.0.0.0", "https": "0.0.0.0"}
    """
    impress_api_url = (
        impress_api_url[:-1] if impress_api_url.endswith("/") else impress_api_url
    )

    # Builds the URL to get the list of entities with type = start_type
    # e.g. https://api.mousephenotype.org/impress/pipeline/list
    root_index = requests.get(f"{impress_api_url}/{start_type}/list", proxies=proxy_map)
    try:
        entity = root_index.json()
    except json.decoder.JSONDecodeError:
        logger.info(f"{impress_api_url}/{start_type}/list")
        logger.info("         " + root_index.text)
        raise requests.exceptions.RequestException(response=root_index)
    root_ids = [key for key in entity.keys()]
    return get_entities_dataframe(
        spark_session, impress_api_url, start_type, root_ids, proxy_map
    )


def get_entities_dataframe(
    spark_session: SparkSession,
    impress_api_url,
    impress_type: str,
    impress_ids: List[str],
    proxy_map: Dict,
) -> DataFrame:
    """
    Using a list of entities identifiers (e.g. pipeline IDs),
    returns a DataFrame containing all the information about those entities.
    It also fires up the process to start crawling the collection belonging to each entity
    (e.g. the procedures contained on a pipeline).
    """

    # Gets the information for a list of entities with a specific identifier.
    entities = [
        get_impress_entity_by_id(impress_api_url, impress_type, impress_id, proxy_map)
        for impress_id in impress_ids
    ]
    entity_df = spark_session.createDataFrame(
        convert_to_row(entity) for entity in entities
    )

    # Extends the current entity with the data coming from its child collections
    current_type = ""
    current_schema = entity_df.schema
    entity_df = process_collection(
        spark_session,
        impress_api_url,
        current_schema,
        current_type,
        entity_df,
        proxy_map,
    )
    unit_df = get_impress_units(impress_api_url, spark_session, proxy_map)
    entity_df = entity_df.join(
        unit_df, entity_df["parameter.unit"] == unit_df["unitID"], "left_outer"
    )
    return entity_df


def process_collection(
    spark_session, impress_api_url, current_schema, current_type, entity_df, proxies
):
    """

    :param spark_session:
    :param impress_api_url:
    :param current_schema:
    :param current_type:
    :param entity_df:
    :return:
    """
    impress_subtype = ""
    collection_types = []
    for column_name in current_schema.names:
        if "Collection" in column_name:
            impress_subtype = column_name.replace("Collection", "")
            if current_type != "":
                column_name = current_type + "." + column_name
            sub_entity_schema = get_impress_entity_schema(
                spark_session, impress_api_url, impress_subtype, proxies
            )
            get_entities_udf = udf(
                lambda x: get_impress_entity_by_ids(
                    impress_api_url, impress_subtype, x, proxies
                ),
                ArrayType(StructType(sub_entity_schema)),
            )
            entity_df = entity_df.withColumn(
                impress_subtype, get_entities_udf(entity_df[column_name])
            )
            collection_types.append(
                dict(type=impress_subtype, schema=sub_entity_schema)
            )
            entity_df = entity_df.withColumn(
                impress_subtype, explode_outer(entity_df[impress_subtype])
            )

    for collection_type in collection_types:
        logger.info("Calling to process:" + collection_type["type"])
        entity_df = process_collection(
            spark_session,
            impress_api_url,
            collection_type["schema"],
            collection_type["type"],
            entity_df,
            proxies,
        )
    return entity_df


def get_impress_entity_by_ids(
    impress_api_url: str, impress_type: str, impress_ids: List[int], proxies, retries=0
):
    """

    :param impress_api_url:
    :param impress_type:
    :param impress_ids:
    :param retries:
    :return:
    """
    api_call_url = "{}/{}/multiple".format(impress_api_url, impress_type)
    logger.info("parsing :" + api_call_url)
    if impress_ids is None or len(impress_ids) == 0:
        return []
    try:
        response = requests.post(api_call_url, json=impress_ids, proxies=proxies)
        try:
            entity = response.json()
        except json.decoder.JSONDecodeError:
            logger.info("{}/{}/multiple".format(impress_api_url, impress_type))
            logger.info("         " + response.text)
            raise requests.exceptions.RequestException(response=response)
    except requests.exceptions.RequestException as e:
        if retries < 4:
            time.sleep(1)
            entity = get_impress_entity_by_ids(
                impress_api_url, impress_type, impress_ids, proxies, retries + 1
            )
        else:
            logger.info(
                "Max retries for "
                + "{}/{}/multiple".format(impress_api_url, impress_type)
            )
            raise Exception(
                "Max retries for "
                + "{}/{}/multiple with {} ".format(
                    impress_api_url,
                    impress_type,
                    ",".join([str(identifier) for identifier in impress_ids]),
                )
            )
    return entity


def get_impress_entity_by_id(
    impress_api_url: str, impress_type: str, impress_id: str, proxies, retries=0
):
    """

    :param impress_api_url:
    :param impress_type:
    :param impress_id:
    :param retries:
    :return:
    """
    api_call_url = "{}/{}/{}".format(impress_api_url, impress_type, impress_id)
    logger.info("parsing :" + api_call_url)
    if impress_id is None:
        return None
    try:
        response = requests.get(api_call_url, timeout=(5, 14), proxies=proxies)
        try:
            entity = response.json()
        except json.decoder.JSONDecodeError:
            logger.info("{}/{}/{}".format(impress_api_url, impress_type, impress_id))
            logger.info("         " + response.text)
            raise requests.exceptions.RequestException(response=response)
    except requests.exceptions.RequestException as e:
        if retries < 4:
            time.sleep(1)
            entity = get_impress_entity_by_id(
                impress_api_url, impress_type, impress_id, proxies, retries + 1
            )
        else:
            logger.info(
                "Max retries for "
                + "{}/{}/{}".format(impress_api_url, impress_type, impress_id)
            )
            raise Exception(
                "Max retries for "
                + "{}/{}/multiple".format(impress_api_url, impress_type)
            )
    return entity


def get_impress_entity_schema(
    spark_session: SparkSession, impress_api_url: str, impress_type: str, proxies
):
    """

    :param spark_session:
    :param impress_api_url:
    :param impress_type:
    :return:
    """
    schema_example = (
        1 if impress_type not in ["increment", "option", "parammpterm"] else 0
    )
    first_entity = requests.get(
        "{}/{}/{}".format(impress_api_url, impress_type, schema_example),
        proxies=proxies,
    ).text
    entity_rdd = spark_session.sparkContext.parallelize([first_entity])
    return spark_session.read.json(entity_rdd).schema


def get_impress_units(impress_api_url, spark_session, proxies):
    json_obj: Dict = json.loads(
        requests.get("{}/{}".format(impress_api_url, "unit/list"), proxies=proxies).text
    )
    unit_index = [{"unitID": key, "unitName": value} for key, value in json_obj.items()]
    entity_rdd = spark_session.sparkContext.parallelize(unit_index)
    return spark_session.read.json(entity_rdd)


def get_ontology_terms(impress_api_url, spark_session, proxies):
    json_obj: Dict = json.loads(
        requests.get(
            "{}/{}".format(impress_api_url, "ontologyterm/list"), proxies=proxies
        ).text
    )
    unit_index = [{"termId": key, "termAcc": value} for key, value in json_obj.items()]
    entity_rdd = spark_session.sparkContext.parallelize(unit_index)
    return spark_session.read.json(entity_rdd)
