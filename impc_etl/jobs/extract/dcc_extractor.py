"""
DCC Extractor module
    This module takes care of extracting data from XML DCC files to  Spark DataFrames.
    There are two kinds of DCC files: experiment and specimen files representing 4 different data
    entities, experiment and line in the experiment files; mouse and embryo in the specimen files.

    The files are expected to be organized by data source in the following directory structure:
    <dcc_xml_path>
        * 3i
        * impc
        * europhenome
    Each directory containing the raw XML from the DCC.
"""
import sys
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.types import StringType
from pyspark.sql.functions import input_file_name, udf, explode_outer, lit
from impc_etl.shared.exceptions import UnsupportedEntityError, UnsupportedFileTypeError
from typing import List
from impc_etl import logger
import py4j


def main(argv):
    """
    DCC Extractor job runner
    :param list argv: the list elements should be:
                    [1]: Input Path
                    [2]: Output Path
                    [3]: File type (experiment or specimen)
                    [4]: Entity type (experiment, line, mouse or embryo)
    """
    input_path = argv[1]
    output_path = argv[2]
    file_type = argv[3]
    entity_type = argv[4]
    spark = SparkSession.builder.getOrCreate()
    dcc_df = extract_dcc_xml_files(spark, input_path, file_type)
    spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")
    spark.sql("set spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY")

    entities_df = None

    if file_type == "experiment":
        entities_df = get_experiments_by_type(dcc_df, entity_type)
    if file_type == "specimen":
        entities_df = get_specimens_by_type(dcc_df, entity_type)

    entities_df.write.mode("overwrite").parquet(output_path)


def extract_dcc_xml_files(
    spark_session: SparkSession, dcc_xml_path: str, file_type: str
) -> DataFrame:
    """
    Extracts the DCC XML files into a Spark DataFrame

    :param SparkSession spark_session: the Apache Spark session object
    :param str dcc_xml_path: The directory containing the DCC XML files
                         in the right directory structure
                         (see :py:mod:dcc_extractor).
    :param str file_type: 'specimen' or 'experiment'
    :return: a Spark DataFrame with the XML files in it
    :rtype: DataFrame
    """
    if file_type not in ["experiment", "specimen"]:
        raise UnsupportedFileTypeError

    dcc_xml_path = (
        dcc_xml_path + "/" if not dcc_xml_path.endswith("/") else dcc_xml_path
    )
    path = f"{dcc_xml_path}**{file_type}*.xml"

    logger.info(f"loading DCC data source from path '{path}'")
    try:
        dcc_df = (
            spark_session.read.format("com.databricks.spark.xml")
            .options(rowTag="centre", samplingRatio="1", nullValue="", mode="FAILFAST")
            .load(path)
        )

        logger.info(f"adding _dataSource column")
        dcc_df = dcc_df.withColumn("_sourceFile", lit(input_file_name()))
        data_source_extract = udf(lambda x: x.split("/")[-2], StringType())
        dcc_df = dcc_df.withColumn("_dataSource", data_source_extract("_sourceFile"))

        logger.info(f"finished load of DCC data source from path '{path}'")
    except py4j.protocol.Py4JJavaError as e:
        if "InvalidInputException" in str(e):
            raise FileNotFoundError
        else:
            raise e
    return dcc_df


def get_experiments_by_type(dcc_df: DataFrame, entity_type: str) -> DataFrame:
    """
    Takes a DataFrame generated by extract_dcc_xml_files
    and gets the experiment data entities. It also expands the procedure struct column
    and adds all its fields to the top level.

    :param DataFrame dcc_df: a DataFrame generated by extract_dcc_xml_files
    :param str entity_type: 'experiment' or 'line'
    :return: a experiment DataFrame
    :rtype: DataFrame
    :raise: UnsupportedEntityError when the given entity_type is not supported
    """
    if entity_type not in ["experiment", "line"]:
        raise UnsupportedEntityError

    experiment_df = _get_entity_by_type(
        dcc_df,
        entity_type,
        ["_centreID", "_pipeline", "_project", "_sourceFile", "_dataSource"],
    )
    return experiment_df.select(
        ["procedure.*"]
        + [column for column in experiment_df.columns if column is not "procedure"]
    ).drop("procedure")


def get_specimens_by_type(dcc_df: DataFrame, entity_type: str) -> DataFrame:
    """
    Takes a DataFrame generated by extract_dcc_xml_files
    and gets the experiment data entities. It also expands the procedure struct column
    and adds all its fields to the top level.

    :param DataFrame dcc_df: a DataFrame generated by extract_dcc_xml_files
    :param str entity_type: 'mouse' or 'embryo'
    :return: a specimen DataFrame
    :rtype: DataFrame
    :raise: UnsupportedEntityError when the given entity_type is not supported
    """
    if entity_type not in ["mouse", "embryo"]:
        raise UnsupportedEntityError
    return _get_entity_by_type(
        dcc_df, entity_type, ["_centreID", "_sourceFile", "_dataSource"]
    )


def _get_entity_by_type(
    dcc_df: DataFrame, entity_type: str, centre_columns: List[str]
) -> DataFrame:
    """
    Takes a DCC DataFrame and obtains a given the entity type
    adding a '_type' column to the DataFrame
    :param DataFrame dcc_df: a DataFrame generated by extract_dcc_xml_files
    :param str entity_type: 'line', 'experiment', 'mouse', 'embryo'
    :param List[str] centre_columns: the Centre columns that the output DataFrame should maintain
    :return: A DataFrame containing only the specified entity type
    :rtype: DataFrame
    """
    centre_columns.append(dcc_df[entity_type])
    entity_df = dcc_df.where(dcc_df[entity_type].isNotNull()).select(
        centre_columns + [entity_type]
    )
    entity_df = (
        entity_df.withColumn("tmp", explode_outer(entity_df[entity_type]))
        .select(["tmp.*"] + centre_columns)
        .withColumn("_type", lit(entity_type))
        .drop(entity_type)
    )
    return entity_df


if __name__ == "__main__":
    sys.exit(main(sys.argv))
