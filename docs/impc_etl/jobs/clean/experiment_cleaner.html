<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>impc_etl.jobs.clean.experiment_cleaner API documentation</title>
<meta name="description" content="Luigi PySpark task that takes the Experiment data coming from the different
data sources
(e.g. IMPC, 3i, EuroPhenome, PWG)
and applies some data …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>impc_etl.jobs.clean.experiment_cleaner</code></h1>
</header>
<section id="section-intro">
<p>Luigi PySpark task that takes the Experiment data coming from the different
data sources
(e.g. IMPC, 3i, EuroPhenome, PWG)
and applies some data sanitizing functions.</p>
<p>The cleaning process includes:</p>
<ul>
<li>mapping identifiers</li>
<li>dropping entries with required values as NULL</li>
<li>drop a list of predefined skipped experiments</li>
<li>creating experiment IDs for line level experiments</li>
<li>sanitize identifiers with XML elements (e.g. gt;)</li>
<li>cleaning legacy identifiers</li>
<li>lastly generate a unique id</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
    Luigi PySpark task that takes the Experiment data coming from the different  data sources  (e.g. IMPC, 3i, EuroPhenome, PWG)
    and applies some data sanitizing functions.

    The cleaning process includes:

    - mapping identifiers
    - dropping entries with required values as NULL
    - drop a list of predefined skipped experiments
    - creating experiment IDs for line level experiments
    - sanitize identifiers with XML elements (e.g. gt;)
    - cleaning legacy identifiers
    - lastly generate a unique id
&#34;&#34;&#34;
from typing import Any

import luigi
from luigi.contrib.spark import PySparkTask
from pyspark import SparkContext
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import (
    udf,
    when,
    lit,
    md5,
    concat,
    col,
    regexp_replace,
    regexp_extract,
)
from pyspark.sql.types import StringType

from impc_etl.config.constants import Constants
from impc_etl.shared import utils
from impc_etl.workflow.config import ImpcConfig
from impc_etl.workflow.extraction import (
    SpecimenExperimentExtractor,
    LineExperimentExtractor,
)


class IMPCExperimentCleaner(PySparkTask):
    &#34;&#34;&#34;
    PySpark task  to clean the IMPC Experimental data.

    This task depends on `impc_etl.workflow.extraction.SpecimenExperimentExtractor` for cleaning Specimen Level experiments
    or `impc_etl.workflow.extraction.LineExperimentExtractor`
    for cleaning Line Level experiments.
    &#34;&#34;&#34;

    #: Name of the Spark task
    name: str = &#34;IMPC_Experiment_Cleaner&#34;

    #: Experimental type can be &#39;specimen_level&#39; or &#39;line_level&#39;.
    experiment_type: luigi.Parameter = luigi.Parameter()

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        &#34;&#34;&#34;
        Defines the luigi  task dependencies
        &#34;&#34;&#34;
        return (
            SpecimenExperimentExtractor()
            if self.experiment_type == &#34;specimen_level&#34;
            else LineExperimentExtractor()
        )

    def output(self):
        &#34;&#34;&#34;
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/specimen_level_experiment_clean_parquet)
        &#34;&#34;&#34;
        return ImpcConfig().get_target(
            f&#34;{self.output_path}{self.experiment_type}_experiment_clean_parquet&#34;
        )

    def app_options(self):
        &#34;&#34;&#34;
        Generates the options pass to the PySpark job
        &#34;&#34;&#34;
        return [
            self.input().path,
            self.experiment_type,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args: Any):
        &#34;&#34;&#34;
        Loads the given  input Experiment  parquet and applies some sanitizing functions to it.
        &#34;&#34;&#34;
        input_path = args[0]
        experiment_type = args[1]
        output_path = args[2]
        spark = SparkSession(sc)
        dcc_df = spark.read.parquet(input_path)

        if experiment_type == &#34;specimen_level&#34;:
            dcc_clean_df = self.clean_experiments(dcc_df)
        else:
            dcc_clean_df = self.clean_lines(dcc_df)

        dcc_clean_df.write.mode(&#34;overwrite&#34;).parquet(output_path)

    def clean_experiments(self, experiment_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Cleaning function  for specimen  level experiments.
        &#34;&#34;&#34;
        experiment_df = (
            experiment_df.transform(self.map_centre_ids)
            .transform(self.map_project_ids)
            .transform(self.truncate_europhenome_specimen_ids)
            .transform(self.drop_skipped_experiments)
            .transform(self.drop_skipped_procedures)
            .transform(self.map_3i_project_ids)
            .transform(self.prefix_3i_experiment_ids)
            .transform(self.drop_null_centre_id)
            .transform(self.drop_null_data_source)
            .transform(self.drop_null_date_of_experiment)
            .transform(self.drop_null_pipeline)
            .transform(self.drop_null_project)
            .transform(self.drop_null_specimen_id)
            .transform(self.generate_unique_id)
        )
        return experiment_df

    def clean_lines(self, line_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        DCC cleaner for line level experiments.
        &#34;&#34;&#34;
        line_df = (
            line_df.transform(self.generate_line_experiment_id)
            .transform(self.map_centre_ids)
            .transform(self.map_project_ids)
            .transform(self.drop_skipped_procedures)
            .transform(self.truncate_europhenome_colony_ids)
            .transform(self.parse_europhenome_colony_xml_entities)
            .transform(self.map_3i_project_ids)
            .transform(self.prefix_3i_experiment_ids)
            .transform(self.drop_null_centre_id)
            .transform(self.drop_null_data_source)
            .transform(self.drop_null_pipeline)
            .transform(self.drop_null_project)
            .transform(self.generate_unique_id)
        )
        return line_df

    def generate_line_experiment_id(self, line_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Takes in a DataFrame containing line level experiments and generates an Experiment  ID for each row.
        Line level experiments in the XML files don&#39;t have a experiment ID and that is needed for further processing so we generate an artificial one.
        &#34;&#34;&#34;
        line_df = line_df.withColumn(
            &#34;_experimentID&#34;, concat(col(&#34;_procedureID&#34;), lit(&#34;-&#34;), col(&#34;_colonyID&#34;))
        )
        return line_df

    def map_centre_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Maps the center ids  found in the XML files to a standard list of ids e.g:
            - gmc -&gt; HMGU
            - h -&gt; MRC Harwell
        The full list of mappings can be found at `impc_etl.config.Constants.CENTRE_ID_MAP`
        &#34;&#34;&#34;
        dcc_df = dcc_df.withColumn(
            &#34;_centreID&#34;, udf(utils.map_centre_id, StringType())(&#34;_centreID&#34;)
        )
        return dcc_df

    def map_project_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Maps the center ids  found in the XML files to a standard list of ids e.g:
            - dtcc -&gt; DTCC
            - riken brc -&gt; RBRC
        The full list of mappings can be found at `impc_etl.config.Constants.PROJECT_ID_MAP`
        &#34;&#34;&#34;
        dcc_df = dcc_df.withColumn(
            &#34;_project&#34;, udf(utils.map_project_id, StringType())(&#34;_project&#34;)
        )
        return dcc_df

    def truncate_europhenome_specimen_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Some EuroPhenome Specimen Ids have a suffix that should be truncated, the legacy identifying strategy for Specimen id included
        a suffix tno reference the production/phenotyping Centre (e.g. 232328312_HRW), that suffix needs to be removed.
        &#34;&#34;&#34;
        dcc_df = dcc_df.withColumn(
            &#34;specimenID&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
                udf(utils.truncate_specimen_id, StringType())(dcc_df[&#34;specimenID&#34;]),
            ).otherwise(dcc_df[&#34;specimenID&#34;]),
        )
        return dcc_df

    def truncate_europhenome_colony_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Some EuroPhenome Colony Ids have a suffix that should be truncated. Same as Specimen IDs some colony IDs used to have a suffix to identify
        the Centre and that has been  removed from the
        tracking system, so we need to truncate them so they  match the corresponding  entry on GenTar.
        &#34;&#34;&#34;
        dcc_df = dcc_df.withColumn(
            &#34;_colonyID&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
                udf(utils.truncate_colony_id, StringType())(dcc_df[&#34;_colonyID&#34;]),
            ).otherwise(dcc_df[&#34;_colonyID&#34;]),
        )
        return dcc_df

    def parse_europhenome_colony_xml_entities(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Some EuroPhenome Colony Ids have &amp;lt; &amp;gt; values that have to be replaced by the  corresponding characters &lt; and &gt;.
        &#34;&#34;&#34;
        dcc_df = dcc_df.withColumn(
            &#34;_colonyID&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
                regexp_replace(&#34;_colonyID&#34;, &#34;&amp;lt;&#34;, &#34;&lt;&#34;),
            ).otherwise(dcc_df[&#34;_colonyID&#34;]),
        )

        dcc_df = dcc_df.withColumn(
            &#34;_colonyID&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
                regexp_replace(&#34;_colonyID&#34;, &#34;&amp;gt;&#34;, &#34;&gt;&#34;),
            ).otherwise(dcc_df[&#34;_colonyID&#34;]),
        )
        return dcc_df

    def drop_skipped_experiments(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        We have a list of IMPC experiments that should be dropped from the loading process.
        &#34;&#34;&#34;
        return dcc_df.where(
            ~(
                (dcc_df[&#34;_centreID&#34;] == &#34;Ucd&#34;)
                &amp; (
                    dcc_df[&#34;_experimentID&#34;].isin(
                        [&#34;GRS_2013-10-09_4326&#34;, &#34;GRS_2014-07-16_8800&#34;]
                    )
                )
            )
        )

    def drop_skipped_procedures(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Drop the experiments corresponding with the list of dropped procedures found
        at `impc_etl.config.constants.Constants.SKIPPED_PROCEDURES`.
        &#34;&#34;&#34;
        return dcc_df.where(
            (
                ~(
                    regexp_extract(col(&#34;_procedureID&#34;), &#34;(.+_.+)_.+&#34;, 1).isin(
                        Constants.SKIPPED_PROCEDURES
                    )
                )
            )
            | (dcc_df[&#34;_dataSource&#34;] == &#34;3i&#34;)
        )

    def map_3i_project_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Some data from the  3i project contains invalid Project identifiers, those entries should be marked as MGP.
        &#34;&#34;&#34;
        return dcc_df.withColumn(
            &#34;_project&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;3i&#34;)
                &amp; (~dcc_df[&#34;_project&#34;].isin(Constants.VALID_PROJECT_IDS)),
                lit(&#34;MGP&#34;),
            ).otherwise(dcc_df[&#34;_project&#34;]),
        )

    def prefix_3i_experiment_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        In  order to avoid collusion  of experiment IDs coming from 3i witht he  ones we generate on the IMPC data,
        we add a prefix the  3i experiment IDs.
        &#34;&#34;&#34;
        return dcc_df.withColumn(
            &#34;_experimentID&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;3i&#34;),
                concat(lit(&#34;3i_&#34;), dcc_df[&#34;_experimentID&#34;]),
            ).otherwise(dcc_df[&#34;_experimentID&#34;]),
        )

    def drop_null_procedure_id(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Procedure ID. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_procedureID&#34;)

    def drop_null_centre_id(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Centre ID. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_centreID&#34;)

    def drop_null_data_source(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a DataSource. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_dataSource&#34;)

    def drop_null_date_of_experiment(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Date of Experiment. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_dateOfExperiment&#34;)

    def drop_null_pipeline(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Pipeline ID. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_pipeline&#34;)

    def drop_null_project(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Project ID. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_project&#34;)

    def drop_null_specimen_id(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Specimen ID. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;specimenID&#34;)

    def drop_if_null(self, dcc_df: DataFrame, column: str) -&gt; DataFrame:
        &#34;&#34;&#34;
        Function that takes in a DataFrame and a column_name and returns that DataFrame filtering out the rows that have
        a null value on the specified column.
        &#34;&#34;&#34;
        return dcc_df.where(dcc_df[column].isNotNull())

    def generate_unique_id(self, dcc_experiment_df: DataFrame):
        &#34;&#34;&#34;
        Generates a unique_id column using as an input every column
        except from those that have non-unique values and
        the ones that correspond to parameter values.
        Given that _sequenceID could be null, the function transforms it the to the string NA
        when is null to avoid the nullifying the concat.
        It concatenates the unique set of values and then applies
        an MD5 hash function to the resulting string.
        :param dcc_experiment_df:
        :return: DataFrame
        &#34;&#34;&#34;
        non_unique_columns = [
            &#34;_type&#34;,
            &#34;_sourceFile&#34;,
            &#34;_VALUE&#34;,
            &#34;procedureMetadata&#34;,
            &#34;statusCode&#34;,
            &#34;_sequenceID&#34;,
            &#34;_project&#34;,
        ]
        if &#34;_sequenceID&#34; in dcc_experiment_df.columns:
            dcc_experiment_df = dcc_experiment_df.withColumn(
                &#34;_sequenceIDStr&#34;,
                when(col(&#34;_sequenceID&#34;).isNull(), lit(&#34;NA&#34;)).otherwise(
                    col(&#34;_sequenceID&#34;)
                ),
            )
        unique_columns = [
            col_name
            for col_name in dcc_experiment_df.columns
            if col_name not in non_unique_columns and not col_name.endswith(&#34;Parameter&#34;)
        ]
        dcc_experiment_df = dcc_experiment_df.withColumn(
            &#34;unique_id&#34;, md5(concat(*unique_columns))
        )
        return dcc_experiment_df</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner"><code class="flex name class">
<span>class <span class="ident">IMPCExperimentCleaner</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>PySpark task
to clean the IMPC Experimental data.</p>
<p>This task depends on <code><a title="impc_etl.workflow.extraction.SpecimenExperimentExtractor" href="../../workflow/extraction.html#impc_etl.workflow.extraction.SpecimenExperimentExtractor">SpecimenExperimentExtractor</a></code> for cleaning Specimen Level experiments
or <code><a title="impc_etl.workflow.extraction.LineExperimentExtractor" href="../../workflow/extraction.html#impc_etl.workflow.extraction.LineExperimentExtractor">LineExperimentExtractor</a></code>
for cleaning Line Level experiments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IMPCExperimentCleaner(PySparkTask):
    &#34;&#34;&#34;
    PySpark task  to clean the IMPC Experimental data.

    This task depends on `impc_etl.workflow.extraction.SpecimenExperimentExtractor` for cleaning Specimen Level experiments
    or `impc_etl.workflow.extraction.LineExperimentExtractor`
    for cleaning Line Level experiments.
    &#34;&#34;&#34;

    #: Name of the Spark task
    name: str = &#34;IMPC_Experiment_Cleaner&#34;

    #: Experimental type can be &#39;specimen_level&#39; or &#39;line_level&#39;.
    experiment_type: luigi.Parameter = luigi.Parameter()

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def requires(self):
        &#34;&#34;&#34;
        Defines the luigi  task dependencies
        &#34;&#34;&#34;
        return (
            SpecimenExperimentExtractor()
            if self.experiment_type == &#34;specimen_level&#34;
            else LineExperimentExtractor()
        )

    def output(self):
        &#34;&#34;&#34;
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/specimen_level_experiment_clean_parquet)
        &#34;&#34;&#34;
        return ImpcConfig().get_target(
            f&#34;{self.output_path}{self.experiment_type}_experiment_clean_parquet&#34;
        )

    def app_options(self):
        &#34;&#34;&#34;
        Generates the options pass to the PySpark job
        &#34;&#34;&#34;
        return [
            self.input().path,
            self.experiment_type,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args: Any):
        &#34;&#34;&#34;
        Loads the given  input Experiment  parquet and applies some sanitizing functions to it.
        &#34;&#34;&#34;
        input_path = args[0]
        experiment_type = args[1]
        output_path = args[2]
        spark = SparkSession(sc)
        dcc_df = spark.read.parquet(input_path)

        if experiment_type == &#34;specimen_level&#34;:
            dcc_clean_df = self.clean_experiments(dcc_df)
        else:
            dcc_clean_df = self.clean_lines(dcc_df)

        dcc_clean_df.write.mode(&#34;overwrite&#34;).parquet(output_path)

    def clean_experiments(self, experiment_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Cleaning function  for specimen  level experiments.
        &#34;&#34;&#34;
        experiment_df = (
            experiment_df.transform(self.map_centre_ids)
            .transform(self.map_project_ids)
            .transform(self.truncate_europhenome_specimen_ids)
            .transform(self.drop_skipped_experiments)
            .transform(self.drop_skipped_procedures)
            .transform(self.map_3i_project_ids)
            .transform(self.prefix_3i_experiment_ids)
            .transform(self.drop_null_centre_id)
            .transform(self.drop_null_data_source)
            .transform(self.drop_null_date_of_experiment)
            .transform(self.drop_null_pipeline)
            .transform(self.drop_null_project)
            .transform(self.drop_null_specimen_id)
            .transform(self.generate_unique_id)
        )
        return experiment_df

    def clean_lines(self, line_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        DCC cleaner for line level experiments.
        &#34;&#34;&#34;
        line_df = (
            line_df.transform(self.generate_line_experiment_id)
            .transform(self.map_centre_ids)
            .transform(self.map_project_ids)
            .transform(self.drop_skipped_procedures)
            .transform(self.truncate_europhenome_colony_ids)
            .transform(self.parse_europhenome_colony_xml_entities)
            .transform(self.map_3i_project_ids)
            .transform(self.prefix_3i_experiment_ids)
            .transform(self.drop_null_centre_id)
            .transform(self.drop_null_data_source)
            .transform(self.drop_null_pipeline)
            .transform(self.drop_null_project)
            .transform(self.generate_unique_id)
        )
        return line_df

    def generate_line_experiment_id(self, line_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Takes in a DataFrame containing line level experiments and generates an Experiment  ID for each row.
        Line level experiments in the XML files don&#39;t have a experiment ID and that is needed for further processing so we generate an artificial one.
        &#34;&#34;&#34;
        line_df = line_df.withColumn(
            &#34;_experimentID&#34;, concat(col(&#34;_procedureID&#34;), lit(&#34;-&#34;), col(&#34;_colonyID&#34;))
        )
        return line_df

    def map_centre_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Maps the center ids  found in the XML files to a standard list of ids e.g:
            - gmc -&gt; HMGU
            - h -&gt; MRC Harwell
        The full list of mappings can be found at `impc_etl.config.Constants.CENTRE_ID_MAP`
        &#34;&#34;&#34;
        dcc_df = dcc_df.withColumn(
            &#34;_centreID&#34;, udf(utils.map_centre_id, StringType())(&#34;_centreID&#34;)
        )
        return dcc_df

    def map_project_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Maps the center ids  found in the XML files to a standard list of ids e.g:
            - dtcc -&gt; DTCC
            - riken brc -&gt; RBRC
        The full list of mappings can be found at `impc_etl.config.Constants.PROJECT_ID_MAP`
        &#34;&#34;&#34;
        dcc_df = dcc_df.withColumn(
            &#34;_project&#34;, udf(utils.map_project_id, StringType())(&#34;_project&#34;)
        )
        return dcc_df

    def truncate_europhenome_specimen_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Some EuroPhenome Specimen Ids have a suffix that should be truncated, the legacy identifying strategy for Specimen id included
        a suffix tno reference the production/phenotyping Centre (e.g. 232328312_HRW), that suffix needs to be removed.
        &#34;&#34;&#34;
        dcc_df = dcc_df.withColumn(
            &#34;specimenID&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
                udf(utils.truncate_specimen_id, StringType())(dcc_df[&#34;specimenID&#34;]),
            ).otherwise(dcc_df[&#34;specimenID&#34;]),
        )
        return dcc_df

    def truncate_europhenome_colony_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Some EuroPhenome Colony Ids have a suffix that should be truncated. Same as Specimen IDs some colony IDs used to have a suffix to identify
        the Centre and that has been  removed from the
        tracking system, so we need to truncate them so they  match the corresponding  entry on GenTar.
        &#34;&#34;&#34;
        dcc_df = dcc_df.withColumn(
            &#34;_colonyID&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
                udf(utils.truncate_colony_id, StringType())(dcc_df[&#34;_colonyID&#34;]),
            ).otherwise(dcc_df[&#34;_colonyID&#34;]),
        )
        return dcc_df

    def parse_europhenome_colony_xml_entities(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Some EuroPhenome Colony Ids have &amp;lt; &amp;gt; values that have to be replaced by the  corresponding characters &lt; and &gt;.
        &#34;&#34;&#34;
        dcc_df = dcc_df.withColumn(
            &#34;_colonyID&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
                regexp_replace(&#34;_colonyID&#34;, &#34;&amp;lt;&#34;, &#34;&lt;&#34;),
            ).otherwise(dcc_df[&#34;_colonyID&#34;]),
        )

        dcc_df = dcc_df.withColumn(
            &#34;_colonyID&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
                regexp_replace(&#34;_colonyID&#34;, &#34;&amp;gt;&#34;, &#34;&gt;&#34;),
            ).otherwise(dcc_df[&#34;_colonyID&#34;]),
        )
        return dcc_df

    def drop_skipped_experiments(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        We have a list of IMPC experiments that should be dropped from the loading process.
        &#34;&#34;&#34;
        return dcc_df.where(
            ~(
                (dcc_df[&#34;_centreID&#34;] == &#34;Ucd&#34;)
                &amp; (
                    dcc_df[&#34;_experimentID&#34;].isin(
                        [&#34;GRS_2013-10-09_4326&#34;, &#34;GRS_2014-07-16_8800&#34;]
                    )
                )
            )
        )

    def drop_skipped_procedures(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Drop the experiments corresponding with the list of dropped procedures found
        at `impc_etl.config.constants.Constants.SKIPPED_PROCEDURES`.
        &#34;&#34;&#34;
        return dcc_df.where(
            (
                ~(
                    regexp_extract(col(&#34;_procedureID&#34;), &#34;(.+_.+)_.+&#34;, 1).isin(
                        Constants.SKIPPED_PROCEDURES
                    )
                )
            )
            | (dcc_df[&#34;_dataSource&#34;] == &#34;3i&#34;)
        )

    def map_3i_project_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        Some data from the  3i project contains invalid Project identifiers, those entries should be marked as MGP.
        &#34;&#34;&#34;
        return dcc_df.withColumn(
            &#34;_project&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;3i&#34;)
                &amp; (~dcc_df[&#34;_project&#34;].isin(Constants.VALID_PROJECT_IDS)),
                lit(&#34;MGP&#34;),
            ).otherwise(dcc_df[&#34;_project&#34;]),
        )

    def prefix_3i_experiment_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
        &#34;&#34;&#34;
        In  order to avoid collusion  of experiment IDs coming from 3i witht he  ones we generate on the IMPC data,
        we add a prefix the  3i experiment IDs.
        &#34;&#34;&#34;
        return dcc_df.withColumn(
            &#34;_experimentID&#34;,
            when(
                (dcc_df[&#34;_dataSource&#34;] == &#34;3i&#34;),
                concat(lit(&#34;3i_&#34;), dcc_df[&#34;_experimentID&#34;]),
            ).otherwise(dcc_df[&#34;_experimentID&#34;]),
        )

    def drop_null_procedure_id(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Procedure ID. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_procedureID&#34;)

    def drop_null_centre_id(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Centre ID. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_centreID&#34;)

    def drop_null_data_source(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a DataSource. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_dataSource&#34;)

    def drop_null_date_of_experiment(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Date of Experiment. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_dateOfExperiment&#34;)

    def drop_null_pipeline(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Pipeline ID. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_pipeline&#34;)

    def drop_null_project(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Project ID. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;_project&#34;)

    def drop_null_specimen_id(self, dcc_df: DataFrame):
        &#34;&#34;&#34;
        Drops all the experiments without a Specimen ID. This case only appears on legacy  data.
        &#34;&#34;&#34;
        return self.drop_if_null(dcc_df, &#34;specimenID&#34;)

    def drop_if_null(self, dcc_df: DataFrame, column: str) -&gt; DataFrame:
        &#34;&#34;&#34;
        Function that takes in a DataFrame and a column_name and returns that DataFrame filtering out the rows that have
        a null value on the specified column.
        &#34;&#34;&#34;
        return dcc_df.where(dcc_df[column].isNotNull())

    def generate_unique_id(self, dcc_experiment_df: DataFrame):
        &#34;&#34;&#34;
        Generates a unique_id column using as an input every column
        except from those that have non-unique values and
        the ones that correspond to parameter values.
        Given that _sequenceID could be null, the function transforms it the to the string NA
        when is null to avoid the nullifying the concat.
        It concatenates the unique set of values and then applies
        an MD5 hash function to the resulting string.
        :param dcc_experiment_df:
        :return: DataFrame
        &#34;&#34;&#34;
        non_unique_columns = [
            &#34;_type&#34;,
            &#34;_sourceFile&#34;,
            &#34;_VALUE&#34;,
            &#34;procedureMetadata&#34;,
            &#34;statusCode&#34;,
            &#34;_sequenceID&#34;,
            &#34;_project&#34;,
        ]
        if &#34;_sequenceID&#34; in dcc_experiment_df.columns:
            dcc_experiment_df = dcc_experiment_df.withColumn(
                &#34;_sequenceIDStr&#34;,
                when(col(&#34;_sequenceID&#34;).isNull(), lit(&#34;NA&#34;)).otherwise(
                    col(&#34;_sequenceID&#34;)
                ),
            )
        unique_columns = [
            col_name
            for col_name in dcc_experiment_df.columns
            if col_name not in non_unique_columns and not col_name.endswith(&#34;Parameter&#34;)
        ]
        dcc_experiment_df = dcc_experiment_df.withColumn(
            &#34;unique_id&#34;, md5(concat(*unique_columns))
        )
        return dcc_experiment_df</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>luigi.contrib.spark.PySparkTask</li>
<li>luigi.contrib.spark.SparkSubmitTask</li>
<li>luigi.contrib.external_program.ExternalProgramTask</li>
<li>luigi.task.Task</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="impc_etl.workflow.cleaning.LineExperimentCleaner" href="../../workflow/cleaning.html#impc_etl.workflow.cleaning.LineExperimentCleaner">LineExperimentCleaner</a></li>
<li><a title="impc_etl.workflow.cleaning.SpecimenExperimentCleaner" href="../../workflow/cleaning.html#impc_etl.workflow.cleaning.SpecimenExperimentCleaner">SpecimenExperimentCleaner</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.experiment_type"><code class="name">var <span class="ident">experiment_type</span> : luigi.parameter.Parameter</code></dt>
<dd>
<div class="desc"><p>Experimental type can be 'specimen_level' or 'line_level'.</p></div>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"><p>Name of the Spark task</p></div>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.output_path"><code class="name">var <span class="ident">output_path</span> : luigi.parameter.Parameter</code></dt>
<dd>
<div class="desc"><p>Path of the output directory where the new parquet file will be generated.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.app_options"><code class="name flex">
<span>def <span class="ident">app_options</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates the options pass to the PySpark job</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def app_options(self):
    &#34;&#34;&#34;
    Generates the options pass to the PySpark job
    &#34;&#34;&#34;
    return [
        self.input().path,
        self.experiment_type,
        self.output().path,
    ]</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.clean_experiments"><code class="name flex">
<span>def <span class="ident">clean_experiments</span></span>(<span>self, experiment_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Cleaning function
for specimen
level experiments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean_experiments(self, experiment_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Cleaning function  for specimen  level experiments.
    &#34;&#34;&#34;
    experiment_df = (
        experiment_df.transform(self.map_centre_ids)
        .transform(self.map_project_ids)
        .transform(self.truncate_europhenome_specimen_ids)
        .transform(self.drop_skipped_experiments)
        .transform(self.drop_skipped_procedures)
        .transform(self.map_3i_project_ids)
        .transform(self.prefix_3i_experiment_ids)
        .transform(self.drop_null_centre_id)
        .transform(self.drop_null_data_source)
        .transform(self.drop_null_date_of_experiment)
        .transform(self.drop_null_pipeline)
        .transform(self.drop_null_project)
        .transform(self.drop_null_specimen_id)
        .transform(self.generate_unique_id)
    )
    return experiment_df</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.clean_lines"><code class="name flex">
<span>def <span class="ident">clean_lines</span></span>(<span>self, line_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>DCC cleaner for line level experiments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean_lines(self, line_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    DCC cleaner for line level experiments.
    &#34;&#34;&#34;
    line_df = (
        line_df.transform(self.generate_line_experiment_id)
        .transform(self.map_centre_ids)
        .transform(self.map_project_ids)
        .transform(self.drop_skipped_procedures)
        .transform(self.truncate_europhenome_colony_ids)
        .transform(self.parse_europhenome_colony_xml_entities)
        .transform(self.map_3i_project_ids)
        .transform(self.prefix_3i_experiment_ids)
        .transform(self.drop_null_centre_id)
        .transform(self.drop_null_data_source)
        .transform(self.drop_null_pipeline)
        .transform(self.drop_null_project)
        .transform(self.generate_unique_id)
    )
    return line_df</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_if_null"><code class="name flex">
<span>def <span class="ident">drop_if_null</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame, column: str) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Function that takes in a DataFrame and a column_name and returns that DataFrame filtering out the rows that have
a null value on the specified column.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_if_null(self, dcc_df: DataFrame, column: str) -&gt; DataFrame:
    &#34;&#34;&#34;
    Function that takes in a DataFrame and a column_name and returns that DataFrame filtering out the rows that have
    a null value on the specified column.
    &#34;&#34;&#34;
    return dcc_df.where(dcc_df[column].isNotNull())</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_centre_id"><code class="name flex">
<span>def <span class="ident">drop_null_centre_id</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Drops all the experiments without a Centre ID. This case only appears on legacy
data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_null_centre_id(self, dcc_df: DataFrame):
    &#34;&#34;&#34;
    Drops all the experiments without a Centre ID. This case only appears on legacy  data.
    &#34;&#34;&#34;
    return self.drop_if_null(dcc_df, &#34;_centreID&#34;)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_data_source"><code class="name flex">
<span>def <span class="ident">drop_null_data_source</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Drops all the experiments without a DataSource. This case only appears on legacy
data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_null_data_source(self, dcc_df: DataFrame):
    &#34;&#34;&#34;
    Drops all the experiments without a DataSource. This case only appears on legacy  data.
    &#34;&#34;&#34;
    return self.drop_if_null(dcc_df, &#34;_dataSource&#34;)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_date_of_experiment"><code class="name flex">
<span>def <span class="ident">drop_null_date_of_experiment</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Drops all the experiments without a Date of Experiment. This case only appears on legacy
data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_null_date_of_experiment(self, dcc_df: DataFrame):
    &#34;&#34;&#34;
    Drops all the experiments without a Date of Experiment. This case only appears on legacy  data.
    &#34;&#34;&#34;
    return self.drop_if_null(dcc_df, &#34;_dateOfExperiment&#34;)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_pipeline"><code class="name flex">
<span>def <span class="ident">drop_null_pipeline</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Drops all the experiments without a Pipeline ID. This case only appears on legacy
data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_null_pipeline(self, dcc_df: DataFrame):
    &#34;&#34;&#34;
    Drops all the experiments without a Pipeline ID. This case only appears on legacy  data.
    &#34;&#34;&#34;
    return self.drop_if_null(dcc_df, &#34;_pipeline&#34;)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_procedure_id"><code class="name flex">
<span>def <span class="ident">drop_null_procedure_id</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Drops all the experiments without a Procedure ID. This case only appears on legacy
data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_null_procedure_id(self, dcc_df: DataFrame):
    &#34;&#34;&#34;
    Drops all the experiments without a Procedure ID. This case only appears on legacy  data.
    &#34;&#34;&#34;
    return self.drop_if_null(dcc_df, &#34;_procedureID&#34;)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_project"><code class="name flex">
<span>def <span class="ident">drop_null_project</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Drops all the experiments without a Project ID. This case only appears on legacy
data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_null_project(self, dcc_df: DataFrame):
    &#34;&#34;&#34;
    Drops all the experiments without a Project ID. This case only appears on legacy  data.
    &#34;&#34;&#34;
    return self.drop_if_null(dcc_df, &#34;_project&#34;)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_specimen_id"><code class="name flex">
<span>def <span class="ident">drop_null_specimen_id</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Drops all the experiments without a Specimen ID. This case only appears on legacy
data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_null_specimen_id(self, dcc_df: DataFrame):
    &#34;&#34;&#34;
    Drops all the experiments without a Specimen ID. This case only appears on legacy  data.
    &#34;&#34;&#34;
    return self.drop_if_null(dcc_df, &#34;specimenID&#34;)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_skipped_experiments"><code class="name flex">
<span>def <span class="ident">drop_skipped_experiments</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>We have a list of IMPC experiments that should be dropped from the loading process.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_skipped_experiments(self, dcc_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    We have a list of IMPC experiments that should be dropped from the loading process.
    &#34;&#34;&#34;
    return dcc_df.where(
        ~(
            (dcc_df[&#34;_centreID&#34;] == &#34;Ucd&#34;)
            &amp; (
                dcc_df[&#34;_experimentID&#34;].isin(
                    [&#34;GRS_2013-10-09_4326&#34;, &#34;GRS_2014-07-16_8800&#34;]
                )
            )
        )
    )</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_skipped_procedures"><code class="name flex">
<span>def <span class="ident">drop_skipped_procedures</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Drop the experiments corresponding with the list of dropped procedures found
at <code><a title="impc_etl.config.constants.Constants.SKIPPED_PROCEDURES" href="../../config/constants.html#impc_etl.config.constants.Constants.SKIPPED_PROCEDURES">Constants.SKIPPED_PROCEDURES</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_skipped_procedures(self, dcc_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Drop the experiments corresponding with the list of dropped procedures found
    at `impc_etl.config.constants.Constants.SKIPPED_PROCEDURES`.
    &#34;&#34;&#34;
    return dcc_df.where(
        (
            ~(
                regexp_extract(col(&#34;_procedureID&#34;), &#34;(.+_.+)_.+&#34;, 1).isin(
                    Constants.SKIPPED_PROCEDURES
                )
            )
        )
        | (dcc_df[&#34;_dataSource&#34;] == &#34;3i&#34;)
    )</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.generate_line_experiment_id"><code class="name flex">
<span>def <span class="ident">generate_line_experiment_id</span></span>(<span>self, line_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Takes in a DataFrame containing line level experiments and generates an Experiment
ID for each row.
Line level experiments in the XML files don't have a experiment ID and that is needed for further processing so we generate an artificial one.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_line_experiment_id(self, line_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Takes in a DataFrame containing line level experiments and generates an Experiment  ID for each row.
    Line level experiments in the XML files don&#39;t have a experiment ID and that is needed for further processing so we generate an artificial one.
    &#34;&#34;&#34;
    line_df = line_df.withColumn(
        &#34;_experimentID&#34;, concat(col(&#34;_procedureID&#34;), lit(&#34;-&#34;), col(&#34;_colonyID&#34;))
    )
    return line_df</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.generate_unique_id"><code class="name flex">
<span>def <span class="ident">generate_unique_id</span></span>(<span>self, dcc_experiment_df: pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates a unique_id column using as an input every column
except from those that have non-unique values and
the ones that correspond to parameter values.
Given that _sequenceID could be null, the function transforms it the to the string NA
when is null to avoid the nullifying the concat.
It concatenates the unique set of values and then applies
an MD5 hash function to the resulting string.
:param dcc_experiment_df:
:return: DataFrame</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_unique_id(self, dcc_experiment_df: DataFrame):
    &#34;&#34;&#34;
    Generates a unique_id column using as an input every column
    except from those that have non-unique values and
    the ones that correspond to parameter values.
    Given that _sequenceID could be null, the function transforms it the to the string NA
    when is null to avoid the nullifying the concat.
    It concatenates the unique set of values and then applies
    an MD5 hash function to the resulting string.
    :param dcc_experiment_df:
    :return: DataFrame
    &#34;&#34;&#34;
    non_unique_columns = [
        &#34;_type&#34;,
        &#34;_sourceFile&#34;,
        &#34;_VALUE&#34;,
        &#34;procedureMetadata&#34;,
        &#34;statusCode&#34;,
        &#34;_sequenceID&#34;,
        &#34;_project&#34;,
    ]
    if &#34;_sequenceID&#34; in dcc_experiment_df.columns:
        dcc_experiment_df = dcc_experiment_df.withColumn(
            &#34;_sequenceIDStr&#34;,
            when(col(&#34;_sequenceID&#34;).isNull(), lit(&#34;NA&#34;)).otherwise(
                col(&#34;_sequenceID&#34;)
            ),
        )
    unique_columns = [
        col_name
        for col_name in dcc_experiment_df.columns
        if col_name not in non_unique_columns and not col_name.endswith(&#34;Parameter&#34;)
    ]
    dcc_experiment_df = dcc_experiment_df.withColumn(
        &#34;unique_id&#34;, md5(concat(*unique_columns))
    )
    return dcc_experiment_df</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>self, sc: pyspark.context.SparkContext, *args: Any)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the given
input Experiment
parquet and applies some sanitizing functions to it.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main(self, sc: SparkContext, *args: Any):
    &#34;&#34;&#34;
    Loads the given  input Experiment  parquet and applies some sanitizing functions to it.
    &#34;&#34;&#34;
    input_path = args[0]
    experiment_type = args[1]
    output_path = args[2]
    spark = SparkSession(sc)
    dcc_df = spark.read.parquet(input_path)

    if experiment_type == &#34;specimen_level&#34;:
        dcc_clean_df = self.clean_experiments(dcc_df)
    else:
        dcc_clean_df = self.clean_lines(dcc_df)

    dcc_clean_df.write.mode(&#34;overwrite&#34;).parquet(output_path)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.map_3i_project_ids"><code class="name flex">
<span>def <span class="ident">map_3i_project_ids</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Some data from the
3i project contains invalid Project identifiers, those entries should be marked as MGP.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_3i_project_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Some data from the  3i project contains invalid Project identifiers, those entries should be marked as MGP.
    &#34;&#34;&#34;
    return dcc_df.withColumn(
        &#34;_project&#34;,
        when(
            (dcc_df[&#34;_dataSource&#34;] == &#34;3i&#34;)
            &amp; (~dcc_df[&#34;_project&#34;].isin(Constants.VALID_PROJECT_IDS)),
            lit(&#34;MGP&#34;),
        ).otherwise(dcc_df[&#34;_project&#34;]),
    )</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.map_centre_ids"><code class="name flex">
<span>def <span class="ident">map_centre_ids</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Maps the center ids
found in the XML files to a standard list of ids e.g:
- gmc -&gt; HMGU
- h -&gt; MRC Harwell
The full list of mappings can be found at <code>impc_etl.config.Constants.CENTRE_ID_MAP</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_centre_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Maps the center ids  found in the XML files to a standard list of ids e.g:
        - gmc -&gt; HMGU
        - h -&gt; MRC Harwell
    The full list of mappings can be found at `impc_etl.config.Constants.CENTRE_ID_MAP`
    &#34;&#34;&#34;
    dcc_df = dcc_df.withColumn(
        &#34;_centreID&#34;, udf(utils.map_centre_id, StringType())(&#34;_centreID&#34;)
    )
    return dcc_df</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.map_project_ids"><code class="name flex">
<span>def <span class="ident">map_project_ids</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Maps the center ids
found in the XML files to a standard list of ids e.g:
- dtcc -&gt; DTCC
- riken brc -&gt; RBRC
The full list of mappings can be found at <code>impc_etl.config.Constants.PROJECT_ID_MAP</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_project_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Maps the center ids  found in the XML files to a standard list of ids e.g:
        - dtcc -&gt; DTCC
        - riken brc -&gt; RBRC
    The full list of mappings can be found at `impc_etl.config.Constants.PROJECT_ID_MAP`
    &#34;&#34;&#34;
    dcc_df = dcc_df.withColumn(
        &#34;_project&#34;, udf(utils.map_project_id, StringType())(&#34;_project&#34;)
    )
    return dcc_df</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.output"><code class="name flex">
<span>def <span class="ident">output</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the full parquet path as an output for the Luigi Task
(e.g. impc/dr15.2/parquet/specimen_level_experiment_clean_parquet)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def output(self):
    &#34;&#34;&#34;
    Returns the full parquet path as an output for the Luigi Task
    (e.g. impc/dr15.2/parquet/specimen_level_experiment_clean_parquet)
    &#34;&#34;&#34;
    return ImpcConfig().get_target(
        f&#34;{self.output_path}{self.experiment_type}_experiment_clean_parquet&#34;
    )</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.parse_europhenome_colony_xml_entities"><code class="name flex">
<span>def <span class="ident">parse_europhenome_colony_xml_entities</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Some EuroPhenome Colony Ids have &lt; &gt; values that have to be replaced by the
corresponding characters &lt; and &gt;.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_europhenome_colony_xml_entities(self, dcc_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Some EuroPhenome Colony Ids have &amp;lt; &amp;gt; values that have to be replaced by the  corresponding characters &lt; and &gt;.
    &#34;&#34;&#34;
    dcc_df = dcc_df.withColumn(
        &#34;_colonyID&#34;,
        when(
            (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
            regexp_replace(&#34;_colonyID&#34;, &#34;&amp;lt;&#34;, &#34;&lt;&#34;),
        ).otherwise(dcc_df[&#34;_colonyID&#34;]),
    )

    dcc_df = dcc_df.withColumn(
        &#34;_colonyID&#34;,
        when(
            (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
            regexp_replace(&#34;_colonyID&#34;, &#34;&amp;gt;&#34;, &#34;&gt;&#34;),
        ).otherwise(dcc_df[&#34;_colonyID&#34;]),
    )
    return dcc_df</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.prefix_3i_experiment_ids"><code class="name flex">
<span>def <span class="ident">prefix_3i_experiment_ids</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>In
order to avoid collusion
of experiment IDs coming from 3i witht he
ones we generate on the IMPC data,
we add a prefix the
3i experiment IDs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prefix_3i_experiment_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    In  order to avoid collusion  of experiment IDs coming from 3i witht he  ones we generate on the IMPC data,
    we add a prefix the  3i experiment IDs.
    &#34;&#34;&#34;
    return dcc_df.withColumn(
        &#34;_experimentID&#34;,
        when(
            (dcc_df[&#34;_dataSource&#34;] == &#34;3i&#34;),
            concat(lit(&#34;3i_&#34;), dcc_df[&#34;_experimentID&#34;]),
        ).otherwise(dcc_df[&#34;_experimentID&#34;]),
    )</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.requires"><code class="name flex">
<span>def <span class="ident">requires</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the luigi
task dependencies</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def requires(self):
    &#34;&#34;&#34;
    Defines the luigi  task dependencies
    &#34;&#34;&#34;
    return (
        SpecimenExperimentExtractor()
        if self.experiment_type == &#34;specimen_level&#34;
        else LineExperimentExtractor()
    )</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.truncate_europhenome_colony_ids"><code class="name flex">
<span>def <span class="ident">truncate_europhenome_colony_ids</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Some EuroPhenome Colony Ids have a suffix that should be truncated. Same as Specimen IDs some colony IDs used to have a suffix to identify
the Centre and that has been
removed from the
tracking system, so we need to truncate them so they
match the corresponding
entry on GenTar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def truncate_europhenome_colony_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Some EuroPhenome Colony Ids have a suffix that should be truncated. Same as Specimen IDs some colony IDs used to have a suffix to identify
    the Centre and that has been  removed from the
    tracking system, so we need to truncate them so they  match the corresponding  entry on GenTar.
    &#34;&#34;&#34;
    dcc_df = dcc_df.withColumn(
        &#34;_colonyID&#34;,
        when(
            (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
            udf(utils.truncate_colony_id, StringType())(dcc_df[&#34;_colonyID&#34;]),
        ).otherwise(dcc_df[&#34;_colonyID&#34;]),
    )
    return dcc_df</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.truncate_europhenome_specimen_ids"><code class="name flex">
<span>def <span class="ident">truncate_europhenome_specimen_ids</span></span>(<span>self, dcc_df: pyspark.sql.dataframe.DataFrame) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Some EuroPhenome Specimen Ids have a suffix that should be truncated, the legacy identifying strategy for Specimen id included
a suffix tno reference the production/phenotyping Centre (e.g. 232328312_HRW), that suffix needs to be removed.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def truncate_europhenome_specimen_ids(self, dcc_df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    Some EuroPhenome Specimen Ids have a suffix that should be truncated, the legacy identifying strategy for Specimen id included
    a suffix tno reference the production/phenotyping Centre (e.g. 232328312_HRW), that suffix needs to be removed.
    &#34;&#34;&#34;
    dcc_df = dcc_df.withColumn(
        &#34;specimenID&#34;,
        when(
            (dcc_df[&#34;_dataSource&#34;] == &#34;europhenome&#34;),
            udf(utils.truncate_specimen_id, StringType())(dcc_df[&#34;specimenID&#34;]),
        ).otherwise(dcc_df[&#34;specimenID&#34;]),
    )
    return dcc_df</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<div style="max-width: 300px; text-align: center">
<img src="https://www.mousephenotype.org/wp-content/themes/impc/images/IMPC_10_YEAR_Logo.svg" alt="IMPC Logo">
</div>
<h1 style="text-align: center; max-width: 300px;">IMPC ETL</h1>
<h2 style="text-align: center; max-width: 300px;">Reference Documentation</h2>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="impc_etl.jobs.clean" href="index.html">impc_etl.jobs.clean</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner">IMPCExperimentCleaner</a></code></h4>
<ul class="">
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.app_options" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.app_options">app_options</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.clean_experiments" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.clean_experiments">clean_experiments</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.clean_lines" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.clean_lines">clean_lines</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_if_null" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_if_null">drop_if_null</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_centre_id" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_centre_id">drop_null_centre_id</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_data_source" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_data_source">drop_null_data_source</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_date_of_experiment" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_date_of_experiment">drop_null_date_of_experiment</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_pipeline" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_pipeline">drop_null_pipeline</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_procedure_id" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_procedure_id">drop_null_procedure_id</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_project" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_project">drop_null_project</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_specimen_id" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_null_specimen_id">drop_null_specimen_id</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_skipped_experiments" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_skipped_experiments">drop_skipped_experiments</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_skipped_procedures" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.drop_skipped_procedures">drop_skipped_procedures</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.experiment_type" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.experiment_type">experiment_type</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.generate_line_experiment_id" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.generate_line_experiment_id">generate_line_experiment_id</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.generate_unique_id" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.generate_unique_id">generate_unique_id</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.main" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.main">main</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.map_3i_project_ids" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.map_3i_project_ids">map_3i_project_ids</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.map_centre_ids" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.map_centre_ids">map_centre_ids</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.map_project_ids" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.map_project_ids">map_project_ids</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.name" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.name">name</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.output" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.output">output</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.output_path" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.output_path">output_path</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.parse_europhenome_colony_xml_entities" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.parse_europhenome_colony_xml_entities">parse_europhenome_colony_xml_entities</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.prefix_3i_experiment_ids" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.prefix_3i_experiment_ids">prefix_3i_experiment_ids</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.requires" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.requires">requires</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.truncate_europhenome_colony_ids" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.truncate_europhenome_colony_ids">truncate_europhenome_colony_ids</a></code></li>
<li><code><a title="impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.truncate_europhenome_specimen_ids" href="#impc_etl.jobs.clean.experiment_cleaner.IMPCExperimentCleaner.truncate_europhenome_specimen_ids">truncate_europhenome_specimen_ids</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p><span></span></p>
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>