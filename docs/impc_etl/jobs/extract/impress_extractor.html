<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>impc_etl.jobs.extract.impress_extractor API documentation</title>
<meta name="description" content="IMPRESS extractor task.
Crawls [IMPReSS](https://www.mousephenotype.org/impress/index) API to generate a full view of the current state of …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>impc_etl.jobs.extract.impress_extractor</code></h1>
</header>
<section id="section-intro">
<p>IMPRESS extractor task.
Crawls <a href="https://www.mousephenotype.org/impress/index">IMPReSS</a> API to generate a full view of the current state of <a href="https://www.mousephenotype.org/impress/index">IMPReSS</a> at a given time.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
IMPRESS extractor task.
Crawls [IMPReSS](https://www.mousephenotype.org/impress/index) API to generate a full view of the current state of [IMPReSS](https://www.mousephenotype.org/impress/index) at a given time.
&#34;&#34;&#34;
import json
import os
import time
from typing import List, Dict

import luigi
import requests
from luigi.contrib.spark import PySparkTask
from pyspark import SparkContext
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import udf, explode_outer
from pyspark.sql.types import StructType, ArrayType

from impc_etl import logger
from impc_etl.shared.utils import convert_to_row
from impc_etl.workflow.config import ImpcConfig


class ImpressExtractor(PySparkTask):
    &#34;&#34;&#34;
    PySpark Task class to extract [IMPReSS](https://www.mousephenotype.org/impress/index) information.
    It goes from the most general endpoint listing resources and down the hierarchy defined by IMPReSS.
    It starts by crawling Schedules, then Pipelines, Procedures, Paramemeters, Units, and Ontology Terms.

    The output is a Parquet file that contains a de-normalized view of the data,
    being each row a combination between Pipeline, Procedure, Parameter, Unit, Ontology Term information.
    &#34;&#34;&#34;

    #: Name of the Spark task
    name: str = &#34;IMPC_IMPReSS_Crawler&#34;

    #: The URL used to access the IMPRESS API (e.g. https://api.mousephenotype.org/impress/)
    impress_api_url: luigi.Parameter = luigi.Parameter()

    #: The crawler can start from any given level of the IMPReSS hierarchy.
    #: Can be &#39;pipeline&#39;, &#39;procedure&#39; or &#39;parameter&#39;.
    impress_root_type: luigi.Parameter = luigi.Parameter(default=&#34;pipeline&#34;)

    #: URL to the HTTP proxy server to be used on the crawling process.
    http_proxy: luigi.Parameter = luigi.Parameter(default=&#34;&#34;)

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def output(self):
        &#34;&#34;&#34;
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/impress_parquet)
        &#34;&#34;&#34;
        return ImpcConfig().get_target(f&#34;{self.output_path}impress_parquet&#34;)

    def app_options(self):
        &#34;&#34;&#34;
        Generates the options pass to the PySpark job
        &#34;&#34;&#34;
        return [
            self.impress_api_url,
            self.impress_root_type,
            self.http_proxy,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        &#34;&#34;&#34;
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        &#34;&#34;&#34;
        impress_api_url = args[0]
        impress_root_type = args[1]
        http_proxy = args[2]
        output_path = args[3]

        # Set HTTP proxy so the jobs has access to the Internet from EBI infrastructure.
        os.environ[&#34;HTTP_PROXY&#34;] = http_proxy
        os.environ[&#34;HTTPS_PROXY&#34;] = http_proxy

        spark = SparkSession(sc)
        proxy_map = {&#34;http&#34;: http_proxy, &#34;https&#34;: http_proxy}
        impress_df = extract_impress(
            spark, impress_api_url, impress_root_type, proxy_map
        )
        ontology_terms = get_ontology_terms(impress_api_url, spark, proxy_map)
        impress_df = impress_df.join(
            ontology_terms,
            impress_df[&#34;parammpterm.ontologyTermId&#34;] == ontology_terms.termId,
            &#34;left_outer&#34;,
        )
        impress_df.write.mode(&#34;overwrite&#34;).parquet(output_path)


def extract_impress(
    spark_session: SparkSession, impress_api_url: str, start_type: str, proxy_map: Dict
) -&gt; DataFrame:
    &#34;&#34;&#34;
    Starts the IMPReSS API crawling, by getting the list of entities corresponding to the stat_type (e.g. pipeline).

    Attributes
    __________
    proxy_map: Dict
            e.g. {&#34;http&#34;: &#34;0.0.0.0&#34;, &#34;https&#34;: &#34;0.0.0.0&#34;}
    &#34;&#34;&#34;
    impress_api_url = (
        impress_api_url[:-1] if impress_api_url.endswith(&#34;/&#34;) else impress_api_url
    )

    # Builds the URL to get the list of entities with type = start_type
    # e.g. https://api.mousephenotype.org/impress/pipeline/list
    root_index = requests.get(f&#34;{impress_api_url}/{start_type}/list&#34;, proxies=proxy_map)
    try:
        entity = root_index.json()
    except json.decoder.JSONDecodeError:
        logger.info(f&#34;{impress_api_url}/{start_type}/list&#34;)
        logger.info(&#34;         &#34; + root_index.text)
        raise requests.exceptions.RequestException(response=root_index)
    root_ids = [key for key in entity.keys()]
    return get_entities_dataframe(
        spark_session, impress_api_url, start_type, root_ids, proxy_map
    )


def get_entities_dataframe(
    spark_session: SparkSession,
    impress_api_url,
    impress_type: str,
    impress_ids: List[str],
    proxy_map: Dict,
) -&gt; DataFrame:
    &#34;&#34;&#34;
    Using a list of entities identifiers (e.g. pipeline IDs),
    returns a DataFrame containing all the information about those entities.
    It also fires up the process to start crawling the collection belonging to each entity
    (e.g. the procedures contained on a pipeline).
    &#34;&#34;&#34;

    # Gets the information for a list of entities with a specific identifier.
    entities = [
        get_impress_entity_by_id(impress_api_url, impress_type, impress_id, proxy_map)
        for impress_id in impress_ids
    ]
    entity_df = spark_session.createDataFrame(
        convert_to_row(entity) for entity in entities
    )

    # Extends the current entity with the data coming from its child collections
    current_type = &#34;&#34;
    current_schema = entity_df.schema
    entity_df = process_collection(
        spark_session,
        impress_api_url,
        current_schema,
        current_type,
        entity_df,
        proxy_map,
    )
    unit_df = get_impress_units(impress_api_url, spark_session, proxy_map)
    entity_df = entity_df.join(
        unit_df, entity_df[&#34;parameter.unit&#34;] == unit_df[&#34;unitID&#34;], &#34;left_outer&#34;
    )
    return entity_df


def process_collection(
    spark_session, impress_api_url, current_schema, current_type, entity_df, proxies
):
    &#34;&#34;&#34;

    :param spark_session:
    :param impress_api_url:
    :param current_schema:
    :param current_type:
    :param entity_df:
    :return:
    &#34;&#34;&#34;
    impress_subtype = &#34;&#34;
    collection_types = []
    for column_name in current_schema.names:
        if &#34;Collection&#34; in column_name:
            impress_subtype = column_name.replace(&#34;Collection&#34;, &#34;&#34;)
            if current_type != &#34;&#34;:
                column_name = current_type + &#34;.&#34; + column_name
            sub_entity_schema = get_impress_entity_schema(
                spark_session, impress_api_url, impress_subtype, proxies
            )
            get_entities_udf = udf(
                lambda x: get_impress_entity_by_ids(
                    impress_api_url, impress_subtype, x, proxies
                ),
                ArrayType(StructType(sub_entity_schema)),
            )
            entity_df = entity_df.withColumn(
                impress_subtype, get_entities_udf(entity_df[column_name])
            )
            collection_types.append(
                dict(type=impress_subtype, schema=sub_entity_schema)
            )
            entity_df = entity_df.withColumn(
                impress_subtype, explode_outer(entity_df[impress_subtype])
            )

    for collection_type in collection_types:
        logger.info(&#34;Calling to process:&#34; + collection_type[&#34;type&#34;])
        entity_df = process_collection(
            spark_session,
            impress_api_url,
            collection_type[&#34;schema&#34;],
            collection_type[&#34;type&#34;],
            entity_df,
            proxies,
        )
    return entity_df


def get_impress_entity_by_ids(
    impress_api_url: str, impress_type: str, impress_ids: List[int], proxies, retries=0
):
    &#34;&#34;&#34;

    :param impress_api_url:
    :param impress_type:
    :param impress_ids:
    :param retries:
    :return:
    &#34;&#34;&#34;
    api_call_url = &#34;{}/{}/multiple&#34;.format(impress_api_url, impress_type)
    logger.info(&#34;parsing :&#34; + api_call_url)
    if impress_ids is None or len(impress_ids) == 0:
        return []
    try:
        response = requests.post(api_call_url, json=impress_ids, proxies=proxies)
        try:
            entity = response.json()
        except json.decoder.JSONDecodeError:
            logger.info(&#34;{}/{}/multiple&#34;.format(impress_api_url, impress_type))
            logger.info(&#34;         &#34; + response.text)
            raise requests.exceptions.RequestException(response=response)
    except requests.exceptions.RequestException as e:
        if retries &lt; 4:
            time.sleep(1)
            entity = get_impress_entity_by_ids(
                impress_api_url, impress_type, impress_ids, proxies, retries + 1
            )
        else:
            logger.info(
                &#34;Max retries for &#34;
                + &#34;{}/{}/multiple&#34;.format(impress_api_url, impress_type)
            )
            raise Exception(
                &#34;Max retries for &#34;
                + &#34;{}/{}/multiple with {} &#34;.format(
                    impress_api_url,
                    impress_type,
                    &#34;,&#34;.join([str(identifier) for identifier in impress_ids]),
                )
            )
    return entity


def get_impress_entity_by_id(
    impress_api_url: str, impress_type: str, impress_id: str, proxies, retries=0
):
    &#34;&#34;&#34;

    :param impress_api_url:
    :param impress_type:
    :param impress_id:
    :param retries:
    :return:
    &#34;&#34;&#34;
    api_call_url = &#34;{}/{}/{}&#34;.format(impress_api_url, impress_type, impress_id)
    logger.info(&#34;parsing :&#34; + api_call_url)
    if impress_id is None:
        return None
    try:
        response = requests.get(api_call_url, timeout=(5, 14), proxies=proxies)
        try:
            entity = response.json()
        except json.decoder.JSONDecodeError:
            logger.info(&#34;{}/{}/{}&#34;.format(impress_api_url, impress_type, impress_id))
            logger.info(&#34;         &#34; + response.text)
            raise requests.exceptions.RequestException(response=response)
    except requests.exceptions.RequestException as e:
        if retries &lt; 4:
            time.sleep(1)
            entity = get_impress_entity_by_id(
                impress_api_url, impress_type, impress_id, proxies, retries + 1
            )
        else:
            logger.info(
                &#34;Max retries for &#34;
                + &#34;{}/{}/{}&#34;.format(impress_api_url, impress_type, impress_id)
            )
            raise Exception(
                &#34;Max retries for &#34;
                + &#34;{}/{}/multiple&#34;.format(impress_api_url, impress_type)
            )
    return entity


def get_impress_entity_schema(
    spark_session: SparkSession, impress_api_url: str, impress_type: str, proxies
):
    &#34;&#34;&#34;

    :param spark_session:
    :param impress_api_url:
    :param impress_type:
    :return:
    &#34;&#34;&#34;
    schema_example = (
        1 if impress_type not in [&#34;increment&#34;, &#34;option&#34;, &#34;parammpterm&#34;] else 0
    )
    first_entity = requests.get(
        &#34;{}/{}/{}&#34;.format(impress_api_url, impress_type, schema_example),
        proxies=proxies,
    ).text
    entity_rdd = spark_session.sparkContext.parallelize([first_entity])
    return spark_session.read.json(entity_rdd).schema


def get_impress_units(impress_api_url, spark_session, proxies):
    json_obj: Dict = json.loads(
        requests.get(&#34;{}/{}&#34;.format(impress_api_url, &#34;unit/list&#34;), proxies=proxies).text
    )
    unit_index = [{&#34;unitID&#34;: key, &#34;unitName&#34;: value} for key, value in json_obj.items()]
    entity_rdd = spark_session.sparkContext.parallelize(unit_index)
    return spark_session.read.json(entity_rdd)


def get_ontology_terms(impress_api_url, spark_session, proxies):
    json_obj: Dict = json.loads(
        requests.get(
            &#34;{}/{}&#34;.format(impress_api_url, &#34;ontologyterm/list&#34;), proxies=proxies
        ).text
    )
    unit_index = [{&#34;termId&#34;: key, &#34;termAcc&#34;: value} for key, value in json_obj.items()]
    entity_rdd = spark_session.sparkContext.parallelize(unit_index)
    return spark_session.read.json(entity_rdd)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="impc_etl.jobs.extract.impress_extractor.extract_impress"><code class="name flex">
<span>def <span class="ident">extract_impress</span></span>(<span>spark_session: pyspark.sql.session.SparkSession, impress_api_url: str, start_type: str, proxy_map: Dict[~KT, ~VT]) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Starts the IMPReSS API crawling, by getting the list of entities corresponding to the stat_type (e.g. pipeline).</p>
<p>Attributes</p>
<hr>
<p>proxy_map: Dict
e.g. {"http": "0.0.0.0", "https": "0.0.0.0"}</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_impress(
    spark_session: SparkSession, impress_api_url: str, start_type: str, proxy_map: Dict
) -&gt; DataFrame:
    &#34;&#34;&#34;
    Starts the IMPReSS API crawling, by getting the list of entities corresponding to the stat_type (e.g. pipeline).

    Attributes
    __________
    proxy_map: Dict
            e.g. {&#34;http&#34;: &#34;0.0.0.0&#34;, &#34;https&#34;: &#34;0.0.0.0&#34;}
    &#34;&#34;&#34;
    impress_api_url = (
        impress_api_url[:-1] if impress_api_url.endswith(&#34;/&#34;) else impress_api_url
    )

    # Builds the URL to get the list of entities with type = start_type
    # e.g. https://api.mousephenotype.org/impress/pipeline/list
    root_index = requests.get(f&#34;{impress_api_url}/{start_type}/list&#34;, proxies=proxy_map)
    try:
        entity = root_index.json()
    except json.decoder.JSONDecodeError:
        logger.info(f&#34;{impress_api_url}/{start_type}/list&#34;)
        logger.info(&#34;         &#34; + root_index.text)
        raise requests.exceptions.RequestException(response=root_index)
    root_ids = [key for key in entity.keys()]
    return get_entities_dataframe(
        spark_session, impress_api_url, start_type, root_ids, proxy_map
    )</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.get_entities_dataframe"><code class="name flex">
<span>def <span class="ident">get_entities_dataframe</span></span>(<span>spark_session: pyspark.sql.session.SparkSession, impress_api_url, impress_type: str, impress_ids: List[str], proxy_map: Dict[~KT, ~VT]) ‑> pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Using a list of entities identifiers (e.g. pipeline IDs),
returns a DataFrame containing all the information about those entities.
It also fires up the process to start crawling the collection belonging to each entity
(e.g. the procedures contained on a pipeline).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_entities_dataframe(
    spark_session: SparkSession,
    impress_api_url,
    impress_type: str,
    impress_ids: List[str],
    proxy_map: Dict,
) -&gt; DataFrame:
    &#34;&#34;&#34;
    Using a list of entities identifiers (e.g. pipeline IDs),
    returns a DataFrame containing all the information about those entities.
    It also fires up the process to start crawling the collection belonging to each entity
    (e.g. the procedures contained on a pipeline).
    &#34;&#34;&#34;

    # Gets the information for a list of entities with a specific identifier.
    entities = [
        get_impress_entity_by_id(impress_api_url, impress_type, impress_id, proxy_map)
        for impress_id in impress_ids
    ]
    entity_df = spark_session.createDataFrame(
        convert_to_row(entity) for entity in entities
    )

    # Extends the current entity with the data coming from its child collections
    current_type = &#34;&#34;
    current_schema = entity_df.schema
    entity_df = process_collection(
        spark_session,
        impress_api_url,
        current_schema,
        current_type,
        entity_df,
        proxy_map,
    )
    unit_df = get_impress_units(impress_api_url, spark_session, proxy_map)
    entity_df = entity_df.join(
        unit_df, entity_df[&#34;parameter.unit&#34;] == unit_df[&#34;unitID&#34;], &#34;left_outer&#34;
    )
    return entity_df</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.get_impress_entity_by_id"><code class="name flex">
<span>def <span class="ident">get_impress_entity_by_id</span></span>(<span>impress_api_url: str, impress_type: str, impress_id: str, proxies, retries=0)</span>
</code></dt>
<dd>
<div class="desc"><p>:param impress_api_url:
:param impress_type:
:param impress_id:
:param retries:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_impress_entity_by_id(
    impress_api_url: str, impress_type: str, impress_id: str, proxies, retries=0
):
    &#34;&#34;&#34;

    :param impress_api_url:
    :param impress_type:
    :param impress_id:
    :param retries:
    :return:
    &#34;&#34;&#34;
    api_call_url = &#34;{}/{}/{}&#34;.format(impress_api_url, impress_type, impress_id)
    logger.info(&#34;parsing :&#34; + api_call_url)
    if impress_id is None:
        return None
    try:
        response = requests.get(api_call_url, timeout=(5, 14), proxies=proxies)
        try:
            entity = response.json()
        except json.decoder.JSONDecodeError:
            logger.info(&#34;{}/{}/{}&#34;.format(impress_api_url, impress_type, impress_id))
            logger.info(&#34;         &#34; + response.text)
            raise requests.exceptions.RequestException(response=response)
    except requests.exceptions.RequestException as e:
        if retries &lt; 4:
            time.sleep(1)
            entity = get_impress_entity_by_id(
                impress_api_url, impress_type, impress_id, proxies, retries + 1
            )
        else:
            logger.info(
                &#34;Max retries for &#34;
                + &#34;{}/{}/{}&#34;.format(impress_api_url, impress_type, impress_id)
            )
            raise Exception(
                &#34;Max retries for &#34;
                + &#34;{}/{}/multiple&#34;.format(impress_api_url, impress_type)
            )
    return entity</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.get_impress_entity_by_ids"><code class="name flex">
<span>def <span class="ident">get_impress_entity_by_ids</span></span>(<span>impress_api_url: str, impress_type: str, impress_ids: List[int], proxies, retries=0)</span>
</code></dt>
<dd>
<div class="desc"><p>:param impress_api_url:
:param impress_type:
:param impress_ids:
:param retries:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_impress_entity_by_ids(
    impress_api_url: str, impress_type: str, impress_ids: List[int], proxies, retries=0
):
    &#34;&#34;&#34;

    :param impress_api_url:
    :param impress_type:
    :param impress_ids:
    :param retries:
    :return:
    &#34;&#34;&#34;
    api_call_url = &#34;{}/{}/multiple&#34;.format(impress_api_url, impress_type)
    logger.info(&#34;parsing :&#34; + api_call_url)
    if impress_ids is None or len(impress_ids) == 0:
        return []
    try:
        response = requests.post(api_call_url, json=impress_ids, proxies=proxies)
        try:
            entity = response.json()
        except json.decoder.JSONDecodeError:
            logger.info(&#34;{}/{}/multiple&#34;.format(impress_api_url, impress_type))
            logger.info(&#34;         &#34; + response.text)
            raise requests.exceptions.RequestException(response=response)
    except requests.exceptions.RequestException as e:
        if retries &lt; 4:
            time.sleep(1)
            entity = get_impress_entity_by_ids(
                impress_api_url, impress_type, impress_ids, proxies, retries + 1
            )
        else:
            logger.info(
                &#34;Max retries for &#34;
                + &#34;{}/{}/multiple&#34;.format(impress_api_url, impress_type)
            )
            raise Exception(
                &#34;Max retries for &#34;
                + &#34;{}/{}/multiple with {} &#34;.format(
                    impress_api_url,
                    impress_type,
                    &#34;,&#34;.join([str(identifier) for identifier in impress_ids]),
                )
            )
    return entity</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.get_impress_entity_schema"><code class="name flex">
<span>def <span class="ident">get_impress_entity_schema</span></span>(<span>spark_session: pyspark.sql.session.SparkSession, impress_api_url: str, impress_type: str, proxies)</span>
</code></dt>
<dd>
<div class="desc"><p>:param spark_session:
:param impress_api_url:
:param impress_type:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_impress_entity_schema(
    spark_session: SparkSession, impress_api_url: str, impress_type: str, proxies
):
    &#34;&#34;&#34;

    :param spark_session:
    :param impress_api_url:
    :param impress_type:
    :return:
    &#34;&#34;&#34;
    schema_example = (
        1 if impress_type not in [&#34;increment&#34;, &#34;option&#34;, &#34;parammpterm&#34;] else 0
    )
    first_entity = requests.get(
        &#34;{}/{}/{}&#34;.format(impress_api_url, impress_type, schema_example),
        proxies=proxies,
    ).text
    entity_rdd = spark_session.sparkContext.parallelize([first_entity])
    return spark_session.read.json(entity_rdd).schema</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.get_impress_units"><code class="name flex">
<span>def <span class="ident">get_impress_units</span></span>(<span>impress_api_url, spark_session, proxies)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_impress_units(impress_api_url, spark_session, proxies):
    json_obj: Dict = json.loads(
        requests.get(&#34;{}/{}&#34;.format(impress_api_url, &#34;unit/list&#34;), proxies=proxies).text
    )
    unit_index = [{&#34;unitID&#34;: key, &#34;unitName&#34;: value} for key, value in json_obj.items()]
    entity_rdd = spark_session.sparkContext.parallelize(unit_index)
    return spark_session.read.json(entity_rdd)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.get_ontology_terms"><code class="name flex">
<span>def <span class="ident">get_ontology_terms</span></span>(<span>impress_api_url, spark_session, proxies)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ontology_terms(impress_api_url, spark_session, proxies):
    json_obj: Dict = json.loads(
        requests.get(
            &#34;{}/{}&#34;.format(impress_api_url, &#34;ontologyterm/list&#34;), proxies=proxies
        ).text
    )
    unit_index = [{&#34;termId&#34;: key, &#34;termAcc&#34;: value} for key, value in json_obj.items()]
    entity_rdd = spark_session.sparkContext.parallelize(unit_index)
    return spark_session.read.json(entity_rdd)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.process_collection"><code class="name flex">
<span>def <span class="ident">process_collection</span></span>(<span>spark_session, impress_api_url, current_schema, current_type, entity_df, proxies)</span>
</code></dt>
<dd>
<div class="desc"><p>:param spark_session:
:param impress_api_url:
:param current_schema:
:param current_type:
:param entity_df:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_collection(
    spark_session, impress_api_url, current_schema, current_type, entity_df, proxies
):
    &#34;&#34;&#34;

    :param spark_session:
    :param impress_api_url:
    :param current_schema:
    :param current_type:
    :param entity_df:
    :return:
    &#34;&#34;&#34;
    impress_subtype = &#34;&#34;
    collection_types = []
    for column_name in current_schema.names:
        if &#34;Collection&#34; in column_name:
            impress_subtype = column_name.replace(&#34;Collection&#34;, &#34;&#34;)
            if current_type != &#34;&#34;:
                column_name = current_type + &#34;.&#34; + column_name
            sub_entity_schema = get_impress_entity_schema(
                spark_session, impress_api_url, impress_subtype, proxies
            )
            get_entities_udf = udf(
                lambda x: get_impress_entity_by_ids(
                    impress_api_url, impress_subtype, x, proxies
                ),
                ArrayType(StructType(sub_entity_schema)),
            )
            entity_df = entity_df.withColumn(
                impress_subtype, get_entities_udf(entity_df[column_name])
            )
            collection_types.append(
                dict(type=impress_subtype, schema=sub_entity_schema)
            )
            entity_df = entity_df.withColumn(
                impress_subtype, explode_outer(entity_df[impress_subtype])
            )

    for collection_type in collection_types:
        logger.info(&#34;Calling to process:&#34; + collection_type[&#34;type&#34;])
        entity_df = process_collection(
            spark_session,
            impress_api_url,
            collection_type[&#34;schema&#34;],
            collection_type[&#34;type&#34;],
            entity_df,
            proxies,
        )
    return entity_df</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="impc_etl.jobs.extract.impress_extractor.ImpressExtractor"><code class="flex name class">
<span>class <span class="ident">ImpressExtractor</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>PySpark Task class to extract <a href="https://www.mousephenotype.org/impress/index">IMPReSS</a> information.
It goes from the most general endpoint listing resources and down the hierarchy defined by IMPReSS.
It starts by crawling Schedules, then Pipelines, Procedures, Paramemeters, Units, and Ontology Terms.</p>
<p>The output is a Parquet file that contains a de-normalized view of the data,
being each row a combination between Pipeline, Procedure, Parameter, Unit, Ontology Term information.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ImpressExtractor(PySparkTask):
    &#34;&#34;&#34;
    PySpark Task class to extract [IMPReSS](https://www.mousephenotype.org/impress/index) information.
    It goes from the most general endpoint listing resources and down the hierarchy defined by IMPReSS.
    It starts by crawling Schedules, then Pipelines, Procedures, Paramemeters, Units, and Ontology Terms.

    The output is a Parquet file that contains a de-normalized view of the data,
    being each row a combination between Pipeline, Procedure, Parameter, Unit, Ontology Term information.
    &#34;&#34;&#34;

    #: Name of the Spark task
    name: str = &#34;IMPC_IMPReSS_Crawler&#34;

    #: The URL used to access the IMPRESS API (e.g. https://api.mousephenotype.org/impress/)
    impress_api_url: luigi.Parameter = luigi.Parameter()

    #: The crawler can start from any given level of the IMPReSS hierarchy.
    #: Can be &#39;pipeline&#39;, &#39;procedure&#39; or &#39;parameter&#39;.
    impress_root_type: luigi.Parameter = luigi.Parameter(default=&#34;pipeline&#34;)

    #: URL to the HTTP proxy server to be used on the crawling process.
    http_proxy: luigi.Parameter = luigi.Parameter(default=&#34;&#34;)

    #: Path of the output directory where the new parquet file will be generated.
    output_path: luigi.Parameter = luigi.Parameter()

    def output(self):
        &#34;&#34;&#34;
        Returns the full parquet path as an output for the Luigi Task
        (e.g. impc/dr15.2/parquet/impress_parquet)
        &#34;&#34;&#34;
        return ImpcConfig().get_target(f&#34;{self.output_path}impress_parquet&#34;)

    def app_options(self):
        &#34;&#34;&#34;
        Generates the options pass to the PySpark job
        &#34;&#34;&#34;
        return [
            self.impress_api_url,
            self.impress_root_type,
            self.http_proxy,
            self.output().path,
        ]

    def main(self, sc: SparkContext, *args):
        &#34;&#34;&#34;
        Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
        &#34;&#34;&#34;
        impress_api_url = args[0]
        impress_root_type = args[1]
        http_proxy = args[2]
        output_path = args[3]

        # Set HTTP proxy so the jobs has access to the Internet from EBI infrastructure.
        os.environ[&#34;HTTP_PROXY&#34;] = http_proxy
        os.environ[&#34;HTTPS_PROXY&#34;] = http_proxy

        spark = SparkSession(sc)
        proxy_map = {&#34;http&#34;: http_proxy, &#34;https&#34;: http_proxy}
        impress_df = extract_impress(
            spark, impress_api_url, impress_root_type, proxy_map
        )
        ontology_terms = get_ontology_terms(impress_api_url, spark, proxy_map)
        impress_df = impress_df.join(
            ontology_terms,
            impress_df[&#34;parammpterm.ontologyTermId&#34;] == ontology_terms.termId,
            &#34;left_outer&#34;,
        )
        impress_df.write.mode(&#34;overwrite&#34;).parquet(output_path)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>luigi.contrib.spark.PySparkTask</li>
<li>luigi.contrib.spark.SparkSubmitTask</li>
<li>luigi.contrib.external_program.ExternalProgramTask</li>
<li>luigi.task.Task</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.http_proxy"><code class="name">var <span class="ident">http_proxy</span> : luigi.parameter.Parameter</code></dt>
<dd>
<div class="desc"><p>URL to the HTTP proxy server to be used on the crawling process.</p></div>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.impress_api_url"><code class="name">var <span class="ident">impress_api_url</span> : luigi.parameter.Parameter</code></dt>
<dd>
<div class="desc"><p>The URL used to access the IMPRESS API (e.g. <a href="https://api.mousephenotype.org/impress/">https://api.mousephenotype.org/impress/</a>)</p></div>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.impress_root_type"><code class="name">var <span class="ident">impress_root_type</span> : luigi.parameter.Parameter</code></dt>
<dd>
<div class="desc"><p>The crawler can start from any given level of the IMPReSS hierarchy.
Can be 'pipeline', 'procedure' or 'parameter'.</p></div>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"><p>Name of the Spark task</p></div>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.output_path"><code class="name">var <span class="ident">output_path</span> : luigi.parameter.Parameter</code></dt>
<dd>
<div class="desc"><p>Path of the output directory where the new parquet file will be generated.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.app_options"><code class="name flex">
<span>def <span class="ident">app_options</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates the options pass to the PySpark job</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def app_options(self):
    &#34;&#34;&#34;
    Generates the options pass to the PySpark job
    &#34;&#34;&#34;
    return [
        self.impress_api_url,
        self.impress_root_type,
        self.http_proxy,
        self.output().path,
    ]</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>self, sc: pyspark.context.SparkContext, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Takes in a SparkContext and the list of arguments generated by <code>app_options</code> and executes the PySpark job.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main(self, sc: SparkContext, *args):
    &#34;&#34;&#34;
    Takes in a SparkContext and the list of arguments generated by `app_options` and executes the PySpark job.
    &#34;&#34;&#34;
    impress_api_url = args[0]
    impress_root_type = args[1]
    http_proxy = args[2]
    output_path = args[3]

    # Set HTTP proxy so the jobs has access to the Internet from EBI infrastructure.
    os.environ[&#34;HTTP_PROXY&#34;] = http_proxy
    os.environ[&#34;HTTPS_PROXY&#34;] = http_proxy

    spark = SparkSession(sc)
    proxy_map = {&#34;http&#34;: http_proxy, &#34;https&#34;: http_proxy}
    impress_df = extract_impress(
        spark, impress_api_url, impress_root_type, proxy_map
    )
    ontology_terms = get_ontology_terms(impress_api_url, spark, proxy_map)
    impress_df = impress_df.join(
        ontology_terms,
        impress_df[&#34;parammpterm.ontologyTermId&#34;] == ontology_terms.termId,
        &#34;left_outer&#34;,
    )
    impress_df.write.mode(&#34;overwrite&#34;).parquet(output_path)</code></pre>
</details>
</dd>
<dt id="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.output"><code class="name flex">
<span>def <span class="ident">output</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the full parquet path as an output for the Luigi Task
(e.g. impc/dr15.2/parquet/impress_parquet)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def output(self):
    &#34;&#34;&#34;
    Returns the full parquet path as an output for the Luigi Task
    (e.g. impc/dr15.2/parquet/impress_parquet)
    &#34;&#34;&#34;
    return ImpcConfig().get_target(f&#34;{self.output_path}impress_parquet&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<div style="max-width: 300px; text-align: center">
<img src="https://www.mousephenotype.org/wp-content/themes/impc/images/IMPC_10_YEAR_Logo.svg" alt="IMPC Logo">
</div>
<h1 style="text-align: center; max-width: 300px;">IMPC ETL</h1>
<h2 style="text-align: center; max-width: 300px;">Reference Documentation</h2>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="impc_etl.jobs.extract" href="index.html">impc_etl.jobs.extract</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="impc_etl.jobs.extract.impress_extractor.extract_impress" href="#impc_etl.jobs.extract.impress_extractor.extract_impress">extract_impress</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.get_entities_dataframe" href="#impc_etl.jobs.extract.impress_extractor.get_entities_dataframe">get_entities_dataframe</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.get_impress_entity_by_id" href="#impc_etl.jobs.extract.impress_extractor.get_impress_entity_by_id">get_impress_entity_by_id</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.get_impress_entity_by_ids" href="#impc_etl.jobs.extract.impress_extractor.get_impress_entity_by_ids">get_impress_entity_by_ids</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.get_impress_entity_schema" href="#impc_etl.jobs.extract.impress_extractor.get_impress_entity_schema">get_impress_entity_schema</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.get_impress_units" href="#impc_etl.jobs.extract.impress_extractor.get_impress_units">get_impress_units</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.get_ontology_terms" href="#impc_etl.jobs.extract.impress_extractor.get_ontology_terms">get_ontology_terms</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.process_collection" href="#impc_etl.jobs.extract.impress_extractor.process_collection">process_collection</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="impc_etl.jobs.extract.impress_extractor.ImpressExtractor" href="#impc_etl.jobs.extract.impress_extractor.ImpressExtractor">ImpressExtractor</a></code></h4>
<ul class="two-column">
<li><code><a title="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.app_options" href="#impc_etl.jobs.extract.impress_extractor.ImpressExtractor.app_options">app_options</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.http_proxy" href="#impc_etl.jobs.extract.impress_extractor.ImpressExtractor.http_proxy">http_proxy</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.impress_api_url" href="#impc_etl.jobs.extract.impress_extractor.ImpressExtractor.impress_api_url">impress_api_url</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.impress_root_type" href="#impc_etl.jobs.extract.impress_extractor.ImpressExtractor.impress_root_type">impress_root_type</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.main" href="#impc_etl.jobs.extract.impress_extractor.ImpressExtractor.main">main</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.name" href="#impc_etl.jobs.extract.impress_extractor.ImpressExtractor.name">name</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.output" href="#impc_etl.jobs.extract.impress_extractor.ImpressExtractor.output">output</a></code></li>
<li><code><a title="impc_etl.jobs.extract.impress_extractor.ImpressExtractor.output_path" href="#impc_etl.jobs.extract.impress_extractor.ImpressExtractor.output_path">output_path</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p><span></span></p>
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>